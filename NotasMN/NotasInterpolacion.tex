
%<<==>><<==>><<==>><<==>><<==>><<==>>
\chapter{Interpolaci\'on Num\'erica}
%<<==>><<==>><<==>><<==>><<==>><<==>>

%<<==>><<==>><<==>><<==>><<==>><<==>>
\section{Conceptos Fundamentales}
%<<==>><<==>><<==>><<==>><<==>><<==>>
\begin{Def}
La interpolaci\'on es una t\'ecnica num\'erica utilizada para estimar valores desconocidos de una funci\'on a partir de un conjunto de puntos conocidos.  A diferencia del ajuste de curvas, la interpolaci\'on busca una funci\'on que pase exactamente por los puntos dados.   La interpolaci\'on consiste en estimar el valor de una funci\'on dentro del intervalo cubierto por un conjunto de datos conocidos.   
\begin{eqnarray}
(x_0, y_0), \; (x_1, y_1), \; \ldots, \; (x_n, y_n)
\end{eqnarray}
donde se asume que $y_i = f(x_i)$ para $i = 0, 1, \ldots, n$.  
El objetivo es construir una funci\'on $P(x)$, generalmente un polinomio, tal que:
\begin{eqnarray}
P(x_i) = f(x_i) = y_i, \quad i=0,1,\ldots,n.
\end{eqnarray}
Dado que existen $n+1$ puntos distintos $(x_i, y_i)$, se puede demostrar que existe un \'unico polinomio de grado $n$ que los interpola:
\begin{eqnarray*}
P_n(x) = a_0 + a_1x + a_2x^2 + \cdots + a_nx^n.
\end{eqnarray*}
La determinaci\'on de los coeficientes $a_i$ puede hacerse de diversas maneras, dando origen a los distintos m\'etodos de interpolaci\'on.
\end{Def}



\begin{Note}
En la interpolaci\'on, la funci\'on construida pasa exactamente por los puntos dados;  en el ajuste de curvas, se busca una funci\'on que aproxime los datos minimizando un error global. Si el polinomio interpolante se usa para estimar valores de $f(x)$ fuera del intervalo cubierto por los puntos dados, el proceso se denomina \textbf{extrapolaci\'on}.  La extrapolaci\'on extiende esta idea a valores fuera del intervalo, con menor confiabilidad, tiende a generar errores grandes, por lo que debe usarse con precauci\'on.
\end{Note}

\begin{center}
\textbf{Comparaci\'on entre interpolaci\'on y ajuste de curvas:}
\begin{tabular}{|p{5cm}|p{5cm}|}
\hline
\textbf{Interpolaci\'on} & \textbf{Ajuste de Curvas} \\ \hline
La curva pasa exactamente por todos los puntos. & La curva se aproxima a los puntos, pero no necesariamente pasa por todos. \\ \hline
Se usa cuando los datos son exactos o tabulados sin error experimental. & Se usa cuando los datos contienen ruido o errores de medici\'on. \\ \hline
Ejemplo: tablas de logaritmos o funciones trigonom\'etricas. & Ejemplo: datos experimentales en laboratorio. \\ \hline
\end{tabular}
\end{center}

\begin{Note}
El error de interpolaci\'on mide la diferencia entre el valor real de la funci\'on y el valor estimado por el polinomio interpolante.  Se expresa mediante la f\'ormula:
\begin{eqnarray}
R_n(x) = f(x) - P_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^{n}(x - x_i),
\end{eqnarray}
para alg\'un $\xi$ en el intervalo de interpolaci\'on.

El error cometido al aproximar $f(x)$ mediante el polinomio interpolante $P_n(x)$ se expresa como:
\begin{eqnarray*}
R_n(x) = f(x) - P_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^{n}(x - x_i),
\end{eqnarray*}
donde $\xi$ es alg\'un valor dentro del intervalo $(x_0, x_n)$.  
El error depende de la derivada de orden $(n+1)$ de la funci\'on $f(x)$ y de la distribuci\'on de los nodos $x_i$.
\end{Note}

\begin{Note}
\textbf{Problemas mal condicionados (efecto Runge):}  El uso de polinomios de alto grado con nodos equiespaciados puede producir oscilaciones extremas cerca de los extremos del intervalo.
Cuando los nodos $x_i$ est\'an equiespaciados y se incrementa el n\'umero de puntos, el polinomio interpolante puede oscilar significativamente, especialmente cerca de los extremos del intervalo.  
Este fen\'omeno se conoce como \textbf{efecto Runge}.  
Una forma de mitigarlo es usar nodos distribuidos seg\'un los \textbf{polinomios de Chebyshev} o recurrir a m\'etodos por tramos como los 
\end{Note}

%<<==>><<==>><<==>><<==>><<==>><<==>>
\section{Forma General del Polinomio Interpolante}
%<<==>><<==>><<==>><<==>><<==>><<==>>

Se define el polinomio de grado $n$ que pasa por $n+1$ puntos $(x_0, y_0), (x_1, y_1), \ldots, (x_n, y_n)$.
\begin{eqnarray*}
P_n(x_i) = y_i, \quad i = 0, 1, 2, \ldots, n
\end{eqnarray*}
donde se conoce el valor de la funci\'on $ f(x) $ en cada punto, es decir, $ y_i = f(x_i) $. El \textbf{problema de interpolaci\'on polin\'omica} consiste en determinar un polinomio $ P_n(x) $ de grado menor o igual a $ n $,  la forma general del polinomio interpolante es:
\begin{eqnarray*}
P_n(x) = a_0 + a_1x + a_2x^2 + \cdots + a_nx^n,
\end{eqnarray*}


donde los coeficientes $ a_0, a_1, \ldots, a_n $ se determinan imponiendo las condiciones de interpolaci\'on:
\begin{eqnarray*}
\begin{cases}
a_0 + a_1x_0 + a_2x_0^2 + \cdots + a_nx_0^n = y_0 \\
a_0 + a_1x_1 + a_2x_1^2 + \cdots + a_nx_1^n = y_1 \\
\quad \vdots \\
a_0 + a_1x_n + a_2x_n^2 + \cdots + a_nx_n^n = y_n
\end{cases}
\end{eqnarray*}

En forma matricial:
\begin{eqnarray*}
\underbrace{
\begin{bmatrix}
1 & x_0 & x_0^2 & \cdots & x_0^n \\
1 & x_1 & x_1^2 & \cdots & x_1^n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^n
\end{bmatrix}}_{V}
\underbrace{
\begin{bmatrix}
a_0 \\ a_1 \\ \vdots \\ a_n
\end{bmatrix}}_{\vec{a}}
=
\underbrace{
\begin{bmatrix}
y_0 \\ y_1 \\ \vdots \\ y_n
\end{bmatrix}}_{\vec{y}}
\end{eqnarray*}
donde $ V $ es la \textbf{matriz de Vandermonde}.  
La existencia de una soluci\'on \'unica depende de que $ V $ sea invertible, lo cual se cumple siempre que todos los $ x_i $ sean distintos. Para demostrar que el polinomio interpolante es \'unico, consideremos dos polinomios $ P_n(x) $ y $ Q_n(x) $ que ambos interpola los mismos puntos:
\begin{eqnarray*}
P_n(x_i) = Q_n(x_i) = y_i, \quad i=0,1,\ldots,n.
\end{eqnarray*}
se define la funci\'on:
\begin{eqnarray*}
R(x) = P_n(x) - Q_n(x).
\end{eqnarray*}
Entonces $ R(x) $ es tambi\'en un polinomio de grado a lo sumo $ n $, y satisface:
\begin{eqnarray*}
R(x_i) = P_n(x_i) - Q_n(x_i) = 0, \quad \text{para todos } i=0,1,\ldots,n.
\end{eqnarray*}
Por lo tanto, $ R(x) $ tiene $ n+1 $ ra\'ices distintas $ x_0, x_1, \ldots, x_n $.  
Sin embargo, un polinomio de grado $ n $ no puede tener m\'as de $ n $ ra\'ices distintas, a menos que todos sus coeficientes sean cero.  
As\'i,
\begin{eqnarray*}
R(x) \equiv 0 \quad \Longrightarrow \quad P_n(x) = Q_n(x),
\end{eqnarray*}
lo que demuestra que el polinomio interpolante es \textbf{\'unico}.

\begin{Note}
\begin{itemize}
    \item Para $ n+1 $ puntos distintos, siempre existe un polinomio de grado $ n $ que pasa exactamente por ellos.
    \item El c\'alculo directo mediante la matriz de Vandermonde es exacto te\'oricamente, pero num\'ericamente inestable para grandes $ n $ debido a la mala condici\'on de la matriz.
    \item Por esta raz\'on, en la pr\'actica se prefieren formas m\'as estables del polinomio, como las f\'ormulas de \textbf{Lagrange} o \textbf{Newton}.
\end{itemize}
\end{Note}

\begin{Ejem}
Sup\'ongase que se tienen los puntos:
\begin{eqnarray*}
(1, 2), \quad (2, 3), \quad (4, 7).
\end{eqnarray*}
El polinomio de grado 2 tendr\'a la forma:
\begin{eqnarray*}
P_2(x) = a_0 + a_1x + a_2x^2.
\end{eqnarray*}
Sustituyendo los puntos:
\begin{eqnarray*}
\begin{cases}
a_0 + a_1(1) + a_2(1)^2 = 2 \\
a_0 + a_1(2) + a_2(2)^2 = 3 \\
a_0 + a_1(4) + a_2(4)^2 = 7
\end{cases}
\end{eqnarray*}
Al resolver el sistema, se obtiene:
\begin{eqnarray*}
a_0 = 1, \quad a_1 = 0.5, \quad a_2 = 0.25.
\end{eqnarray*}
Por tanto,
\begin{eqnarray*}
P_2(x) = 1 + 0.5x + 0.25x^2.
\end{eqnarray*}
Verificando:
\begin{eqnarray*}
P_2(1)=2, \quad P_2(2)=3, \quad P_2(4)=7,
\end{eqnarray*}
lo que confirma que el polinomio interpola correctamente los puntos.
\end{Ejem}

\begin{Ejem}
Interpolar los puntos:
\begin{eqnarray*}
(0, 1), \quad (1, 0), \quad (2, -1), \quad (3, 2).
\end{eqnarray*}

Buscamos un polinomio c\'ubico:
\begin{eqnarray*}
P_3(x) = a_0 + a_1x + a_2x^2 + a_3x^3.
\end{eqnarray*}

Sustituyendo:
\begin{eqnarray*}
\begin{cases}
a_0 = 1, \\
a_0 + a_1 + a_2 + a_3 = 0, \\
a_0 + 2a_1 + 4a_2 + 8a_3 = -1, \\
a_0 + 3a_1 + 9a_2 + 27a_3 = 2.
\end{cases}
\end{eqnarray*}

De la primera ecuaci\'on, $ a_0 = 1 $. Sustituyendo en las dem\'as:
\begin{eqnarray*}
\begin{cases}
a_1 + a_2 + a_3 = -1, \\
2a_1 + 4a_2 + 8a_3 = -2, \\
3a_1 + 9a_2 + 27a_3 = 1.
\end{cases}
\end{eqnarray*}

Resolviendo el sistema:
\begin{eqnarray*}
a_1 = -\tfrac{5}{2}, \quad a_2 = \tfrac{9}{4}, \quad a_3 = -\tfrac{3}{4}.
\end{eqnarray*}

El polinomio interpolante es:
\begin{eqnarray*}
P_3(x) = 1 - \frac{5}{2}x + \frac{9}{4}x^2 - \frac{3}{4}x^3.
\end{eqnarray*}

Verificando:
\begin{eqnarray*}
P_3(0)=1, \quad P_3(1)=0, \quad P_3(2)=-1, \quad P_3(3)=2.
\end{eqnarray*}

\textbf{Interpretaci\'on:} este polinomio de grado tres conecta los cuatro puntos de manera exacta.  
Si se grafican los datos y la curva, se observa un comportamiento ondulado t\'ipico de polinomios de orden alto, mostrando c\'omo la interpolaci\'on exacta puede oscilar entre los nodos.
\end{Ejem}

\begin{Note}
\begin{itemize}
    \item Para $ n+1 $ puntos distintos, existe un \'unico polinomio de grado $ n $ que interpola los datos.
    \item El c\'alculo directo de los coeficientes mediante la matriz de Vandermonde es te\'oricamente exacto, pero num\'ericamente inestable para valores grandes de $ n $.
    \item Por eficiencia y estabilidad se prefieren m\'etodos como los de \textbf{Lagrange} y \textbf{Newton}, que se estudiar\'an a continuaci\'on.
\end{itemize}
\end{Note}

%<<==>><<==>><<==>><<==>><<==>><<==>>
\section{Interpolaci\'on de Lagrange}
%<<==>><<==>><<==>><<==>><<==>><<==>>
Dado un conjunto de $n+1$ puntos con abscisas distintas
\begin{eqnarray*}
(x_0,y_0),\ (x_1,y_1),\ \ldots,\ (x_n,y_n),\qquad y_i=f(x_i),
\end{eqnarray*}
el polinomio interpolante de Lagrange $P_n(x)$ se escribe como
\begin{eqnarray*}
P_n(x)=\sum_{i=0}^{n} y_i\,L_i(x),\qquad
L_i(x)=\prod_{\substack{j=0\\ j\neq i}}^{n}\frac{x-x_j}{\,x_i-x_j\,}.
\end{eqnarray*}
Cada base $L_i$ satisface $L_i(x_k)=\delta_{ik}$, por lo que $P_n(x_i)=y_i$ para todo $i$.

Si $f$ es $(n+1)$ veces derivable en un intervalo que contiene a los nodos $x_i$ y al punto $x$, entonces
\begin{eqnarray*}
f(x)-P_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}\,\omega_{n+1}(x),\textrm{ donde }
\omega_{n+1}(x)=\prod_{i=0}^{n}(x-x_i),
\end{eqnarray*}
para alg\'un $\xi$ en el intervalo convexo de $\{x_0,\ldots,x_n,x\}$.

\begin{Note}
\begin{itemize}
    \item Construir $P_n(x)$ y evaluarlo directamente con la f\'ormula de Lagrange cuesta $O(n^2)$ por punto de evaluaci\'on.
    \item Para muchos nodos o evaluaciones repetidas, se prefiere la forma \emph{baric\'entrica} por estabilidad y costo $O(n)$ por evaluaci\'on (se comenta al final).
    \item Con nodos equiespaciados y $n$ grande puede aparecer el \emph{efecto Runge}. Una mitigaci\'on es usar nodos de Chebyshev o splines.
\end{itemize}
\end{Note}


\begin{Ejem}
Interpolar $(0,0)$, $(1,1)$, $(2,4)$ (muestran $f(x)=x^2$). Se obtiene
\begin{eqnarray*}
\begin{aligned}
L_0(x)&=\frac{(x-1)(x-2)}{(0-1)(0-2)}=\frac{(x-1)(x-2)}{2},\\
L_1(x)&=\frac{(x-0)(x-2)}{(1-0)(1-2)}=-(x)(x-2),\\
L_2(x)&=\frac{(x-0)(x-1)}{(2-0)(2-1)}=\frac{x(x-1)}{2}.
\end{aligned}
\end{eqnarray*}
Entonces
\begin{eqnarray*}
P_2(x)=0\cdot L_0(x)+1\cdot L_1(x)+4\cdot L_2(x) = x^2,
\end{eqnarray*}
que recupera exactamente la funci\'on cuadr\'atica.
\end{Ejem}

%<<==>><<==>><<==>><<==>><<==>><<==>>
\section{Interpolaci\'on de Newton--Diferencias Finitas}
%<<==>><<==>><<==>><<==>><<==>><<==>>

El m\'etodo de Newton construye el polinomio interpolante de manera incremental,  usando el concepto de \textbf{diferencias divididas}.   Esto permite agregar nuevos puntos sin recalcular todo el polinomio.

Sea un conjunto de $n+1$ puntos distintos:
\begin{eqnarray*}
(x_0, y_0),\ (x_1, y_1),\ \ldots,\ (x_n, y_n).
\end{eqnarray*}
El polinomio de Newton tiene la forma:
\begin{eqnarray*}
P_n(x)& = &f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1) + \cdots \\
&+& f[x_0,x_1,\ldots,x_n]\prod_{j=0}^{n-1}(x-x_j),
\end{eqnarray*}
donde los coeficientes $f[x_i,\ldots,x_j]$ son las \emph{diferencias divididas}.

\begin{itemize}
    \item De primer orden:
    \begin{eqnarray*}
    f[x_i,x_{i+1}] = \frac{f(x_{i+1}) - f(x_i)}{x_{i+1}-x_i}.
    \end{eqnarray*}
    \item De segundo orden:
    \begin{eqnarray*}
    f[x_i,x_{i+1},x_{i+2}] = \frac{f[x_{i+1},x_{i+2}] - f[x_i,x_{i+1}]}{x_{i+2}-x_i}.
    \end{eqnarray*}
    \item En general:
    \begin{eqnarray*}
    f[x_i, x_{i+1}, \ldots, x_{i+k}] = 
    \frac{f[x_{i+1},\ldots,x_{i+k}] - f[x_i,\ldots,x_{i+k-1}]}{x_{i+k}-x_i}.
    \end{eqnarray*}
\end{itemize}

Estas diferencias se organizan en una tabla triangular.




\begin{Ejem}
Interpolar los datos:
\begin{eqnarray*}
(1, 2),\ (2, 3),\ (4, 7).
\end{eqnarray*}
\textbf{Paso 1:} construir la tabla de diferencias divididas:

\begin{center}
\begin{tabular}{c|c|c|c}
$x_i$ & $f[x_i]$ & $f[x_i,x_{i+1}]$ & $f[x_i,x_{i+1},x_{i+2}]$ \\ \hline
1 & 2 & $\frac{3-2}{2-1}=1$ & $\frac{(7-3)/(4-2) - 1}{4-1}=\frac{1-1}{3}=0$\\
2 & 3 & $\frac{7-3}{4-2}=2$ &  \\
4 & 7 &  &  \\ 
\end{tabular}
\end{center}

\textbf{Paso 2:} polinomio de Newton:
\begin{eqnarray*}
P_2(x) = f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1),
\end{eqnarray*}
sustituyendo:
\begin{eqnarray*}
P_2(x) = 2 + 1(x-1) + 0(x-1)(x-2) = 2 + (x-1) = x + 1.
\end{eqnarray*}

En este caso el t\'ermino cuadr\'atico se anul\'o (el polinomio resultante es lineal porque los puntos est\'an casi alineados).
\end{Ejem}

\begin{Ejem}
Sea:
\begin{eqnarray*}
(0, 1),\ (1, 0),\ (2, -1),\ (3, 2).
\end{eqnarray*}
\textbf{Tabla de diferencias divididas:}

\begin{center}
\begin{tabular}{c|c|c|c|c}
$x_i$ & $f[x_i]$ & $f[x_i,x_{i+1}]$ & $f[x_i,x_{i+1},x_{i+2}]$ & $f[x_i,x_{i+1},x_{i+2},x_{i+3}]$ \\ \hline
0 & 1 & $\frac{0-1}{1-0}=-1$ & $\frac{(-1)-(-1)}{2-0}=0$ & $\frac{(1.5)-0}{3-0}=0.5$ \\
1 & 0 & $\frac{-1-0}{2-1}=-1$ & $\frac{2-(-1)}{3-1}=1.5$ &  \\
2 & -1 & $\frac{2-(-1)}{3-2}=3$ &  &  \\
3 & 2 &  &  &  \\ 
\end{tabular}
\end{center}

\textbf{Polinomio:}
\begin{eqnarray*}
P_3(x) &=& f[x_0] 
+ f[x_0,x_1](x-x_0)
+ f[x_0,x_1,x_2](x-x_0)(x-x_1)\\
&+& f[x_0,x_1,x_2,x_3](x-x_0)(x-x_1)(x-x_2) \\
&=& 1 -1(x-0) + 0(x)(x-1) + 0.5(x)(x-1)(x-2).
\end{eqnarray*}
Simplificando:
\begin{eqnarray*}
P_3(x) = 1 - x + 0.5x(x-1)(x-2) = 1 - x + 0.5x^3 - 1.5x^2 + x = 1 - 1.5x^2 + 0.5x^3.
\end{eqnarray*}
Por tanto:
\begin{eqnarray*}
P_3(x) = 1 - \frac{3}{2}x^2 + \frac{1}{2}x^3.
\end{eqnarray*}

Verificando:
\begin{eqnarray*}
P_3(0)=1,\quad P_3(1)=0,\quad P_3(2)=-1,\quad P_3(3)=2.
\end{eqnarray*}
\end{Ejem}


\begin{Note}
\begin{itemize}
    \item Las diferencias divididas se pueden calcular una sola vez y reutilizar para m\'ultiples evaluaciones.
    \item Si se a\~nade un nuevo punto $(x_{n+1}, y_{n+1})$, basta con agregar una nueva columna a la tabla sin recomputar todo.
    \item El polinomio de Newton es m\'as estable que el de Lagrange para grandes $n$ y se adapta mejor a la incorporaci\'on incremental de datos.
\end{itemize}
\end{Note}

\begin{Ejem}
Dados los puntos
\begin{eqnarray*}
(1,2),\quad (2,3),\quad (4,7),
\end{eqnarray*}
construyamos la tabla de diferencias divididas y el polinomio de Newton. 

\paragraph{Paso 1: Primera columna (valores de la funcion).}
\begin{eqnarray*}
f[x_0]=2\ (\text{en }x_0=1),\qquad
f[x_1]=3\ (\text{en }x_1=2),\qquad
f[x_2]=7\ (\text{en }x_2=4).
\end{eqnarray*}

\paragraph{Paso 2: Diferencias divididas de primer orden.}
\begin{eqnarray*}
\begin{aligned}
f[x_0,x_1] &= \frac{f[x_1]-f[x_0]}{x_1-x_0}
= \frac{3-2}{2-1} = 1,\\
f[x_1,x_2] &= \frac{f[x_2]-f[x_1]}{x_2-x_1}
= \frac{7-3}{4-2} = 2.
\end{aligned}
\end{eqnarray*}

\paragraph{Paso 3: Diferencia dividida de segundo orden.}
\begin{eqnarray*}
f[x_0,x_1,x_2] = 
\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}
= \frac{2-1}{4-1} = \frac{1}{3}.
\end{eqnarray*}
\textbf{Atencion:} En el ejemplo del texto principal se obtuvo cero por un redondeo al mostrar pasos; aqu\'i dejamos el valor exacto $\tfrac{1}{3}$. Para verificar coherencia, construyamos el polinomio y validemos en los nodos.

\paragraph{Paso 4: Polinomio de Newton.}
\begin{eqnarray*}
\begin{aligned}
P_2(x) &= f[x_0] 
+ f[x_0,x_1](x-x_0)
+ f[x_0,x_1,x_2](x-x_0)(x-x_1)\\
&= 2 + 1(x-1) + \frac{1}{3}(x-1)(x-2).
\end{aligned}
\end{eqnarray*}

\paragraph{Paso 5: Verificacion en los nodos.}
\begin{eqnarray*}
\begin{aligned}
P_2(1) &= 2 + 1(0) + \tfrac{1}{3}(0)(-1) = 2,\\
P_2(2) &= 2 + 1(1) + \tfrac{1}{3}(1)(0) = 3,\\
P_2(4) &= 2 + 1(3) + \tfrac{1}{3}(3)(2) = 2+3+2 = 7.
\end{aligned}
\end{eqnarray*}

\paragraph{Paso 6: Evaluacion en un punto intermedio (por ejemplo, $x=3$).}
\begin{eqnarray*}
P_2(3) = 2 + 1(2) + \tfrac{1}{3}(2)(1) = 2 + 2 + \tfrac{2}{3} = \tfrac{14}{3}.
\end{eqnarray*}

\paragraph{Tabla triangular resumida.}
\begin{eqnarray*}
\begin{array}{c|c|c|c}
x_i & f[x_i] & f[x_i,x_{i+1}] & f[x_i,x_{i+1},x_{i+2}]\\ \hline
1 & 2 & 1 & \tfrac{1}{3}\\
2 & 3 & 2 & \\
4 & 7 &  & 
\end{array}
\end{eqnarray*}
\end{Ejem}

%-----------------------------------------------------------
\section{Polinomio de Newton}
%-----------------------------------------------------------

Sea una funci\'on $f$ conocida en los puntos
\begin{eqnarray*}
(x_0,f_0), (x_1,f_1),\dots,(x_n,f_n),
\qquad f_k = f(x_k).
\end{eqnarray*}
El polinomio interpolante de Newton en t\'erminos de diferencias divididas
se escribe como
\begin{equation}
P_n(x) = f[x_0]
+ (x - x_0) f[x_0,x_1]
+ (x - x_0)(x - x_1) f[x_0,x_1,x_2]
+ \cdots
+ (x-x_0)\cdots(x-x_{n-1}) f[x_0,\dots,x_n].
\label{eq:newton-general}
\end{equation}

Recordemos que las \emph{diferencias divididas} se definen recursivamente:
\begin{eqnarray*}
f[x_k] &=& f(x_k),\\
f[x_k,x_{k+1}] &=& \frac{f(x_{k+1}) - f(x_k)}{x_{k+1}-x_k},\\
f[x_k,x_{k+1},x_{k+2}] &=& \frac{f[x_{k+1},x_{k+2}] - f[x_k,x_{k+1}]}
                               {x_{k+2}-x_k},\\
&\ \ \vdots
\end{eqnarray*}

En esta secci\'on supondremos que los nodos est\'an \emph{equispaciados}, es
decir:
\begin{eqnarray*}
x_k = x_0 + kh, \qquad k=0,1,\dots,n,
\end{eqnarray*}
para cierto paso $h>0$ constante. Definimos primero las \emph{diferencias progresivas} de $f$ en los nodos
$x_k$:

\begin{eqnarray*}
\Delta f_k
&=& f_{k+1} - f_k,\\
\Delta^2 f_k
&=& \Delta f_{k+1} - \Delta f_k,\\
\Delta^3 f_k
&=& \Delta^2 f_{k+1} - \Delta^2 f_k,\\
&\ \ \vdots
\end{eqnarray*}
es decir: 
\begin{eqnarray*}
\Delta f_0 &=& f_1 - f_0,\\
\Delta^2 f_0 &=& (f_2 - f_1) - (f_1 - f_0) = f_2 - 2f_1 + f_0,\\
\Delta^3 f_0 &=& (f_3 - 2f_2 + f_1) - (f_2 - 2f_1 + f_0)\\
&=& f_3 - 3f_2 + 3f_1 - f_0,\\
&\ \ \vdots
\end{eqnarray*}

Para el primer orden tenemos:
\begin{eqnarray*}
f[x_0,x_1]
= \frac{f_1 - f_0}{x_1 - x_0}.
\end{eqnarray*}
Como $x_1 - x_0 = h$ y $f_1 - f_0 = \Delta f_0$, se sigue que
\begin{equation}
f[x_0,x_1] = \frac{\Delta f_0}{h}.
\label{eq:dividida1}
\end{equation}

Por definici\'on de diferencias divididas de segundo orden:
\begin{eqnarray*}
f[x_0,x_1,x_2]
= \frac{f[x_1,x_2] - f[x_0,x_1]}{x_2 - x_0}.
\end{eqnarray*}
Usamos el caso de primer orden para $f[x_1,x_2]$ y $f[x_0,x_1]$:
\begin{eqnarray*}
f[x_1,x_2] = \frac{f_2 - f_1}{h}, \qquad
f[x_0,x_1] = \frac{f_1 - f_0}{h}.
\end{eqnarray*}
Entonces:
\begin{eqnarray*}
f[x_0,x_1,x_2]
&=& \frac{\frac{f_2 - f_1}{h} - \frac{f_1 - f_0}{h}}{x_2 - x_0}\\
&=& \frac{\frac{f_2 - 2f_1 + f_0}{h}}{2h}
= \frac{f_2 - 2f_1 + f_0}{2h^2}.
\end{eqnarray*}
Observamos que $f_2 - 2f_1 + f_0 = \Delta^2 f_0$, por lo que
\begin{equation}
f[x_0,x_1,x_2]
= \frac{\Delta^2 f_0}{2!\,h^2}.
\label{eq:dividida2}
\end{equation}

Procediendo de manera an\'aloga llegamos a la expresi\'on general:
\begin{equation}
f[x_0,x_1,\dots,x_k]
= \frac{\Delta^k f_0}{k!\,h^k}, \qquad k=0,1,\dots,n.
\label{eq:dividida-general-prog}
\end{equation}

Para $k=0$ se tiene $f[x_0] = f_0$, y como $\Delta^0 f_0 = f_0$,
la igualdad se cumple.  Para $k=1$ y $k=2$ la f\'ormula se verific\'o expl\'icitamente en 
\ref{eq:dividida1} y \ref{eq:dividida2}.

Supongamos que
\begin{eqnarray*}
f[x_0,\dots,x_k] = \frac{\Delta^k f_0}{k!\,h^k},
\qquad
f[x_1,\dots,x_{k+1}] = \frac{\Delta^k f_1}{k!\,h^k}.
\end{eqnarray*}

entonces consideremos
\begin{eqnarray*}
f[x_0,x_1,\dots,x_{k+1}]
= \frac{f[x_1,\dots,x_{k+1}] - f[x_0,\dots,x_k]}{x_{k+1} - x_0}.
\end{eqnarray*}
Como $x_{k+1} - x_0 = (k+1)h$, usando la hip\'otesis:
\begin{eqnarray*}
f[x_1,\dots,x_{k+1}] = \frac{\Delta^k f_1}{k!\,h^k},
\qquad
f[x_0,\dots,x_k]   = \frac{\Delta^k f_0}{k!\,h^k},
\end{eqnarray*}
por tanto se obtiene
\begin{eqnarray*}
f[x_0,\dots,x_{k+1}]
&=& \frac{\frac{\Delta^k f_1}{k!\,h^k} - \frac{\Delta^k f_0}{k!\,h^k}}
         {(k+1)h}=\frac{\Delta^k f_1 - \Delta^k f_0}{k!\,h^k\,(k+1)h}.
\end{eqnarray*}
Por definici\'on de diferencia progresiva de orden $k+1$:
\begin{eqnarray*}
\Delta^{k+1} f_0 = \Delta^k f_1 - \Delta^k f_0.
\end{eqnarray*}
es decir
\begin{eqnarray*}
f[x_0,\dots,x_{k+1}]
= \frac{\Delta^{k+1} f_0}{(k+1)!\,h^{k+1}},
\end{eqnarray*}
que es precisamente la f\'ormula \eqref{eq:dividida-general-prog} para $k+1$, y por lo tanto queda demostrada la relaci\'on
\begin{eqnarray*}
f[x_0,\dots,x_k]
= \frac{\Delta^k f_0}{k!\,h^k}, \quad k=0,1,\dots,n,
\end{eqnarray*}
cuando los nodos son equiespaciados. Ahora, a partir de la expresi\'on general \ref{eq:newton-general} y sustituimos
las diferencias divididas usando \ref{eq:dividida-general-prog}:
\begin{eqnarray*}
P_n(x)
&=& f_0
+ (x-x_0) \frac{\Delta f_0}{1!\,h}
+ (x-x_0)(x-x_1) \frac{\Delta^2 f_0}{2!\,h^2}
+ \cdots \\
&+& (x-x_0)(x-x_1)\cdots(x-x_{n-1}) \frac{\Delta^n f_0}{n!\,h^n}.
\end{eqnarray*}

donde
\begin{eqnarray*}
x_k = x_0 + kh, \textrm{ por lo tanto }x - x_k = x - x_0 - kh.
\end{eqnarray*}
haciendo 
\begin{eqnarray*}
s = \frac{x - x_0}{h}\textrm{ se tiene que }x - x_0 = sh,
\end{eqnarray*}
y entonces
\begin{eqnarray*}
x - x_1 &=& x - (x_0 + h)   = (x - x_0) - h   = (s - 1) h,\\
x - x_2 &=& x - (x_0 + 2h) = (x - x_0) - 2h  = (s - 2) h,\\
&\vdots& \\
x - x_{k-1} &=& (s - (k-1))h.
\end{eqnarray*}
Por lo tanto, el producto
\begin{eqnarray*}
(x-x_0)(x-x_1)\cdots(x-x_{k-1})
= h^k\, s(s-1)\cdots(s-k+1).
\end{eqnarray*}

Sustituyendo en $P_n(x)$:
\begin{eqnarray*}
P_n(x)
&=& f_0
+ \frac{h s}{h} \Delta f_0
+ \frac{h^2 s(s-1)}{2!\,h^2}\Delta^2 f_0
+ \cdots
+ \frac{h^n s(s-1)\cdots(s-n+1)}{n!\,h^n}\Delta^n f_0\\
&=& f_0
+ s\Delta f_0
+ \frac{s(s-1)}{2!}\,\Delta^2 f_0
+ \cdots
+ \frac{s(s-1)\cdots(s-n+1)}{n!}\,\Delta^n f_0.
\end{eqnarray*}

Es decir, se tiene la f\'ormula de Newton con diferencias progresivas:

\begin{equation}
P_n(x) = f_0
+ s\,\Delta f_0
+ \frac{s(s-1)}{2!}\,\Delta^2 f_0
+ \frac{s(s-1)(s-2)}{3!}\,\Delta^3 f_0
+ \cdots
+ \frac{s(s-1)\cdots(s-n+1)}{n!}\,\Delta^n f_0,
\label{eq:newton-progresivo}
\end{equation}
donde
\begin{eqnarray*}
s = \frac{x - x_0}{h}.
\end{eqnarray*}

De forma an\'aloga definimos las \emph{diferencias regresivas}:
\begin{eqnarray*}
\nabla f_k &=& f_k - f_{k-1},\\
\nabla^2 f_k &=& \nabla f_k - \nabla f_{k-1},\\
\nabla^3 f_k &=& \nabla^2 f_k - \nabla^2 f_{k-1},\\
&\ \ \vdots
\end{eqnarray*}

En particular, las primeras diferencias regresivas en el nodo final $x_n$
son:
\begin{eqnarray*}
\nabla f_n &=& f_n - f_{n-1},\textrm{ luego }\\
\nabla^2 f_n &=& (f_n - f_{n-1}) - (f_{n-1} - f_{n-2})
= f_n - 2f_{n-1} + f_{n-2},\\
&\vdots&
\end{eqnarray*}

Ahora consideramos las diferencias divididas de atr\'as hacia adelante:
\begin{eqnarray*}
f[x_n],\ f[x_n,x_{n-1}],\ f[x_n,x_{n-1},x_{n-2}],\dots
\end{eqnarray*}

Se tiene que
\begin{eqnarray*}
f[x_n,x_{n-1}]
= \frac{f_n - f_{n-1}}{x_n - x_{n-1}}
= \frac{\nabla f_n}{h}.
\end{eqnarray*}

luego,
\begin{eqnarray*}
f[x_n,x_{n-1},x_{n-2}]
= \frac{f[x_{n-1},x_{n-2}] - f[x_n,x_{n-1}]}{x_{n-2} - x_n}.
\end{eqnarray*}
donde
\begin{eqnarray*}
f[x_{n-1},x_{n-2}] = \frac{f_{n-1} - f_{n-2}}{h}, \textrm{ y }
f[x_n,x_{n-1}]     = \frac{f_n     - f_{n-1}}{h},
\end{eqnarray*}
y $x_{n-2} - x_n = -2h$. Entonces:
\begin{eqnarray*}
f[x_n,x_{n-1},x_{n-2}]
&=& \frac{\frac{f_{n-1} - f_{n-2}}{h} - \frac{f_n - f_{n-1}}{h}}{-2h}=\frac{(f_{n-1} - f_{n-2}) - (f_n - f_{n-1})}{-2h^2}\\
&=& \frac{-f_n + 2f_{n-1} - f_{n-2}}{-2h^2}
= \frac{f_n - 2f_{n-1} + f_{n-2}}{2h^2}=\frac{\nabla^2 f_n}{2!\,h^2}.
\end{eqnarray*}

De forma an\'aloga al caso progresivo, se puede probar por inducci\'on que:
\begin{equation}
f[x_n,x_{n-1},\dots,x_{n-k}]
= \frac{\nabla^k f_n}{k!\,h^k},\qquad k=0,1,\dots,n.
\label{eq:dividida-general-reg}
\end{equation}

Procediendo de manera an\'aloga escribimos ahora el polinomio de Newton de atr\'as hacia adelante:
\begin{eqnarray*}
P_n(x)
&=& f[x_n]
+ (x - x_n) f[x_n,x_{n-1}]
+ (x - x_n)(x - x_{n-1}) f[x_n,x_{n-1},x_{n-2}]\\
&+& \cdots
+ (x-x_n)(x-x_{n-1})\cdots(x-x_{n-k+1}) f[x_n,\dots,x_{n-k}]+\cdots.
\end{eqnarray*}

Sustituimos la ecuaci\'on \eqref{eq:dividida-general-reg}:
\begin{eqnarray*}
P_n(x)
&=& f_n
+ (x - x_n)\frac{\nabla f_n}{1!\,h}
+ (x - x_n)(x - x_{n-1})\frac{\nabla^2 f_n}{2!\,h^2}
+ \cdots\\
&+& (x-x_n)(x-x_{n-1})\cdots(x-x_{n-k+1}) \frac{\nabla^k f_n}{k!\,h^k}
+ \cdots.
\end{eqnarray*}

Definimos ahora
\begin{eqnarray*}
s = \frac{x - x_n}{h}\textrm{ entonces }x - x_n = sh.
\end{eqnarray*}
Adem\'as,
\begin{eqnarray*}
x - x_{n-1}
&=& x - (x_n - h)
 = (x - x_n) + h
 = (s+1)h,\\
x - x_{n-2}
&=& x - (x_n - 2h)
 = (x - x_n) + 2h
 = (s+2)h,\\
&\ \ \vdots\\
x - x_{n-k+1}
&=& (s + k - 1) h.
\end{eqnarray*}
Por lo tanto
\begin{eqnarray*}
(x-x_n)(x-x_{n-1})\cdots(x-x_{n-k+1})
= h^k\, s(s+1)\cdots(s + k - 1).
\end{eqnarray*}

Sustituyendo en $P_n(x)$:
\begin{eqnarray*}
P_n(x)
&=& f_n
+ \frac{h s}{h}\nabla f_n
+ \frac{h^2 s(s+1)}{2!\,h^2}\nabla^2 f_n
+ \cdots
+ \frac{h^k s(s+1)\cdots(s+k-1)}{k!\,h^k}\nabla^k f_n
+ \cdots\\
&=& f_n
+ s\,\nabla f_n
+ \frac{s(s+1)}{2!}\,\nabla^2 f_n
+ \cdots
+ \frac{s(s+1)\cdots(s+k-1)}{k!}\,\nabla^k f_n
+ \cdots.
\end{eqnarray*}

con lo cual se obtiene la f\'ormula de Newton con diferencias regresivas:
\begin{equation}
P_n(x)
= f_n
+ s\,\nabla f_n
+ \frac{s(s+1)}{2!}\,\nabla^2 f_n
+ \frac{s(s+1)(s+2)}{3!}\,\nabla^3 f_n
+ \cdots
+ \frac{s(s+1)\cdots(s+n-1)}{n!}\,\nabla^n f_n,
\label{eq:newton-regresivo}
\end{equation}
donde
\begin{eqnarray*}
s = \frac{x - x_n}{h}.
\end{eqnarray*}

\subsection{Ejemplos del m\'etodo de Newton con diferencias finitas}

\begin{Ejem} Se quiere  aproximar $f(x)=x^2$ en un punto cercano a $x_0=0$ usando diferencias progresivas de Newton.

Consid\'erense los puntos equiespaciados (con $h=1$):
\begin{eqnarray*}
x_0=0,\quad x_1=1,x_2=2,
\end{eqnarray*}
y
\begin{eqnarray*}
f_0=f(0)=0, f_1=f(1)=1, f_2=f(2)=4.
\end{eqnarray*}

Entonces se tienen la siguiente tabla de diferencias divididas:

\begin{eqnarray*}
\begin{array}{c|c|c|c}
k & x_k & f_k & \text{Diferencias} \\ \hline
0 & 0 & 0 & \\
1 & 1 & 1 & \Delta f_0 = f_1 - f_0 = 1 - 0 = 1 \\
2 & 2 & 4 & \Delta f_1 = f_2 - f_1 = 4 - 1 = 3 \\
\hline
  &   &   & \Delta^2 f_0 = \Delta f_1 - \Delta f_0 = 3 - 1 = 2
\end{array}
\end{eqnarray*}

entonces el Polinomio de Newton progresivo se calcula a partir de la f\'ormula de Newton progresiva:
\begin{eqnarray*}
P_2(x) &=& f_0 + s\,\Delta f_0 + \frac{s(s-1)}{2!}\,\Delta^2 f_0,\\
&s& = \frac{x - x_0}{h}.
\end{eqnarray*}
Con $x_0=0$ y $h=1$, resulta $s=x$, sustituyendo:
\begin{eqnarray*}
P_2(x) 
= 0 + s(1) + \frac{s(s-1)}{2}(2)
= s + s(s-1)
= s + s^2 - s
= s^2.
\end{eqnarray*}
Como $s=x$, se tiene $P_2(x) = x^2$, es decir, recuperamos la funci\'on exacta (como era de esperar al interpolar un polinomio de grado 2 con tres puntos). Evaluando en $x=1.5$: $s = \frac{x-x_0}{h} = \frac{1.5-0}{1} = 1.5$, $P_2(1.5) = (1.5)^2 = 2.25$.  El valor exacto de $f(1.5)$ es tambi\'en $2.25$, de modo que no hay  error de interpolaci\'on en este caso.
\end{Ejem}

\begin{Ejem} Aproximar $f(x)=\ln(1+x)$ cerca de $x_0=0$ usando diferencias progresivas con $h=0.5$, y luego se evaluar\'a en $x=0.75$.

Consid\'erense los puntos equiespaciados:
\begin{eqnarray*}
x_0 = 0,\quad x_1 = 0.5,\quad x_2 = 1.0,\qquad h=0.5.
\end{eqnarray*}
Evaluando la funci\'on:
\begin{eqnarray*}
f_0 &=& \ln(1+0) = 0,\\
f_1 &=& \ln(1+0.5) = \ln(1.5) \approx 0.405465,\\
f_2 &=& \ln(1+1.0) = \ln(2) \approx 0.693147.
\end{eqnarray*}

entonces la Tabla de diferencias progresivas quedar\'ia como:

\begin{eqnarray*}
\begin{array}{c|c|c|c}
k & x_k & f_k & \text{Diferencias} \\ \hline
0 & 0.0 & 0.000000 & \\
1 & 0.5 & 0.405465 & \Delta f_0 = 0.405465 - 0.000000 = 0.405465 \\
2 & 1.0 & 0.693147 & \Delta f_1 = 0.693147 - 0.405465 = 0.287682 \\
\hline
  &     &         & \Delta^2 f_0 = \Delta f_1 - \Delta f_0 = 0.287682 - 0.405465 = -0.117783
\end{array}
\end{eqnarray*}

Entonces el polinomio de Newton progresivo se obtiene con la f\'ormula:
\begin{eqnarray*}
P_2(x) &=& f_0 + s\,\Delta f_0 + \frac{s(s-1)}{2!}\,\Delta^2 f_0,\\
s &=& \frac{x-x_0}{h} = \frac{x}{0.5} = 2x.
\end{eqnarray*}
Sustituimos $f_0=0$, $\Delta f_0=0.405465$ y $\Delta^2 f_0=-0.117783$:
\begin{eqnarray*}
P_2(x)= 0 + s(0.405465) + \frac{s(s-1)}{2}(-0.117783).
\end{eqnarray*}

Evaluando  en $x=0.75$
\begin{eqnarray*}
s = \frac{0.75 - 0}{0.5} = 1.5.
\end{eqnarray*}
Entonces
\begin{eqnarray*}
P_2(0.75)= 1.5(0.405465) + \frac{1.5(1.5-1)}{2}(-0.117783),
\end{eqnarray*}
donde
$1.5(0.405465) \approx 0.6081975$, $\frac{1.5(0.5)}{2} = \frac{0.75}{2} = 0.375$, y $0.375(-0.117783) \approx -0.044168$,
por lo tanto, $P_2(0.75) \approx 0.6081975 - 0.044168 \approx 0.564029$, donde el valor real es $f(0.75) = \ln(1.75) \approx 0.559616$.  El error de interpolaci\'on es peque\~no: $\lvert f(0.75) - P_2(0.75) \rvert \approx 0.0044$.
\end{Ejem}

\begin{Ejem} Ahora usaremos la forma \emph{regresiva} para aproximar $f(x)=\sqrt{x}$ cerca del nodo final $x_n=3$, con $h=1$, y se estimar\'a $f(2.6)$. Consid\'erense los puntos $x_0=1,\quad x_1=2,\quad x_2=3,\qquad h=1$.

Entonces
\begin{eqnarray*}
f_0 &=& \sqrt{1} = 1.000000,\\
f_1 &=& \sqrt{2} \approx 1.414214,\\
f_2 &=& \sqrt{3} \approx 1.732051.
\end{eqnarray*}

la correspondiente tabla de diferencias regresivas (centradas en $x_2$)

\begin{eqnarray*}
\begin{array}{c|c|c|c}
k & x_k & f_k & \text{Diferencias} \\ \hline
0 & 1 & 1.000000 & \\
1 & 2 & 1.414214 & \nabla f_1 = f_1 - f_0 = 1.414214 - 1.000000 = 0.414214 \\
2 & 3 & 1.732051 & \nabla f_2 = f_2 - f_1 = 1.732051 - 1.414214 = 0.317837 \\
\hline
  &   &         & \nabla^2 f_2 = \nabla f_2 - \nabla f_1 = 0.317837 - 0.414214 = -0.096376
\end{array}
\end{eqnarray*}

Nos interesan especialmente $\nabla f_2$ y $\nabla^2 f_2$ para el polinomio
regresivo. La f\'ormula de Newton regresivo de grado 2 es
\begin{eqnarray*}
P_2(x) = f_n + s\,\nabla f_n + \frac{s(s+1)}{2!}\,\nabla^2 f_n,
\qquad
s = \frac{x-x_n}{h}.
\end{eqnarray*}
donde $n=2$, $x_n=x_2=3$, $h=1$, por lo que $s = x-3$.  Sustituyendo se tiene
\begin{eqnarray*}
P_2(x) = f_2 + s\,\nabla f_2 + \frac{s(s+1)}{2}\,\nabla^2 f_2.
\end{eqnarray*}

Evaluando en $x=2.6$: $s = \frac{2.6-3}{1} = -0.4$. Entonces $P_2(2.6)= 1.732051 + (-0.4)(0.317837) 
+ \frac{(-0.4)(-0.4+1)}{2}(-0.096376)$.
Calculamos cada t\'ermino:
\begin{eqnarray*}
(-0.4)(0.317837) \approx -0.127135,\\
-0.4+1 = 0.6,\quad (-0.4)(0.6) = -0.24,\quad \frac{-0.24}{2} = -0.12,\\
-0.12(-0.096376) \approx 0.011570.
\end{eqnarray*}
Por lo tanto $P_2(2.6)\approx 1.732051 - 0.127135 + 0.011570 \approx 1.616486$.

El valor real es $f(2.6) = \sqrt{2.6} \approx 1.612452$. El error es $\lvert f(2.6) - P_2(2.6) \rvert \approx 0.0040$,
lo cual muestra una buena aproximaci\'on usando la forma regresivacerca del extremo derecho.
\end{Ejem}

\begin{Ejem}
Ahora ilustraremos un caso de grado 3 usando $f(x)=\sin(x)$ con paso
$h=0.5$, cerca de $x_0=0$, y evaluaremos en $x=0.7$. Consid\'erese
\begin{eqnarray*}
x_0 = 0.0,\quad x_1=0.5,\quad x_2=1.0,\quad x_3=1.5,\qquad h=0.5.
\end{eqnarray*}
Evaluamos la funci\'on en los puntos dados
\begin{eqnarray*}
f_0 = \sin(0.0) = 0.000000,\\
f_1 = \sin(0.5) \approx 0.479426,\\
f_2 = \sin(1.0) \approx 0.841471,\\
f_3 = \sin(1.5) \approx 0.997495
\end{eqnarray*}

con lo que la tabla de diferencias progresivas

\begin{eqnarray*}
\begin{array}{c|c|c|c}
k & x_k & f_k & \text{Diferencias} \\ \hline
0 & 0.0 & 0.000000 & \\
1 & 0.5 & 0.479426 & \Delta f_0 = 0.479426 - 0.000000 = 0.479426 \\
2 & 1.0 & 0.841471 & \Delta f_1 = 0.841471 - 0.479426 = 0.362045 \\
3 & 1.5 & 0.997495 & \Delta f_2 = 0.997495 - 0.841471 = 0.156024 \\
\hline
  &     &         & \Delta^2 f_0 = \Delta f_1 - \Delta f_0 = 0.362045 - 0.479426 = -0.117380 \\
  &     &         & \Delta^2 f_1 = \Delta f_2 - \Delta f_1 = 0.156024 - 0.362045 = -0.206021 \\
\hline
  &     &         & \Delta^3 f_0 = \Delta^2 f_1 - \Delta^2 f_0 = -0.206021 - (-0.117380) = -0.088641
\end{array}
\end{eqnarray*}

y por tanto el polinomio de Newton progresivo de grado 3 tiene por ecuaci\'on
\begin{eqnarray*}
P_3(x) = f_0 + s\,\Delta f_0 + \frac{s(s-1)}{2!}\,\Delta^2 f_0 + \frac{s(s-1)(s-2)}{3!}\,\Delta^3 f_0,
\qquad
s = \frac{x-x_0}{h} = \frac{x}{0.5} = 2x.
\end{eqnarray*}
Sustituimos $f_0=0$, $\Delta f_0=0.479426$,  $\Delta^2 f_0=-0.117380$, $\Delta^3 f_0=-0.088641$:
\begin{eqnarray*}
P_3(x)= s(0.479426) + \frac{s(s-1)}{2}(-0.117380)+ \frac{s(s-1)(s-2)}{6}(-0.088641).
\end{eqnarray*}

Evaluando en $x=0.7$: $s = \frac{0.7-0}{0.5} = 1.4$, entonces.
\begin{eqnarray*}
P_3(0.7)= 1.4(0.479426)+ \frac{1.4(1.4-1)}{2}(-0.117380)+ \frac{1.4(1.4-1)(1.4-2)}{6}(-0.088641).
\end{eqnarray*}

Evaluamos cada t\'ermino: $1.4(0.479426) \approx 0.671196$, $1.4-1 = 0.4$, $\frac{1.4(0.4)}{2} = \frac{0.56}{2} = 0.28$,
$0.28(-0.117380) \approx -0.032866$, luego $1.4-2 = -0.6$, $1.4(0.4)(-0.6) = 1.4\cdot 0.4\cdot (-0.6) = -0.336$,  $\frac{-0.336}{6} \approx -0.056$, $-0.056(-0.088641) \approx 0.004963$. Por tanto $P_3(0.7)\approx 0.671196 - 0.032866 + 0.004963\approx 0.643293$. Donde el valor real es $\sin(0.7) \approx 0.644218$. El error de interpolaci\'on es $\lvert \sin(0.7) - P_3(0.7) \rvert \approx 0.0009$, lo cual muestra que un polinomio de grado 3 con cuatro nodos cercanos puede aproximar muy bien a $\sin(x)$ en el intervalo considerado.
\end{Ejem}

\begin{Ejem}
Dados los puntos
\begin{eqnarray*}
(0,1),\quad (1,0),\quad (2,-1),\quad (3,2),
\end{eqnarray*}
construyamos la tabla y el polinomio $P_3(x)$.

\begin{eqnarray*}
f[x_0]=1,\quad f[x_1]=0,\quad f[x_2]=-1,\quad f[x_3]=2.
\end{eqnarray*}

\begin{eqnarray*}
\begin{aligned}
f[x_0,x_1] &= \frac{0-1}{1-0} = -1,\\
f[x_1,x_2] &= \frac{-1-0}{2-1} = -1,\\
f[x_2,x_3] &= \frac{2-(-1)}{3-2} = 3.
\end{aligned}
\end{eqnarray*}

\begin{eqnarray*}
\begin{aligned}
f[x_0,x_1,x_2] &= \frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}
= \frac{(-1)-(-1)}{2-0} = 0,\\
f[x_1,x_2,x_3] &= \frac{f[x_2,x_3]-f[x_1,x_2]}{x_3-x_1}
= \frac{3-(-1)}{3-1} = \frac{4}{2}=2.
\end{aligned}
\end{eqnarray*}

\begin{eqnarray*}
f[x_0,x_1,x_2,x_3] =
\frac{f[x_1,x_2,x_3]-f[x_0,x_1,x_2]}{x_3-x_0}
= \frac{2-0}{3-0} = \frac{2}{3}.
\end{eqnarray*}

Entonces el Polinomio de Newton.
\begin{eqnarray*}
P_3(x) &=& f[x_0]+ f[x_0,x_1](x-x_0)+ f[x_0,x_1,x_2](x-x_0)(x-x_1)\\
& +& f[x_0,x_1,x_2,x_3](x-x_0)(x-x_1)(x-x_2)\\
&=& 1 + (-1)(x-0) + 0\cdot(x)(x-1) + \frac{2}{3}(x)(x-1)(x-2).
\end{eqnarray*}

Verificando: 
\begin{eqnarray*}
\begin{aligned}
P_3(0) &= 1 - 0 + 0 + 0 = 1,\\
P_3(1) &= 1 - 1 + 0 + 0 = 0,\\
P_3(2) &= 1 - 2 + 0 + 0 = -1,\\
P_3(3) &= 1 - 3 + 0 + \tfrac{2}{3}(3)(2)(1) = -2 + 4 = 2.
\end{aligned}
\end{eqnarray*}

La tabla triangular completa quedar\'ia en la forma
\begin{eqnarray*}
\begin{array}{c|c|c|c|c}
x_i & f[x_i] & f[x_i,x_{i+1}] & f[x_i,x_{i+1},x_{i+2}] & f[x_i,x_{i+1},x_{i+2},x_{i+3}]\\ \hline
0 & 1 & -1 & 0 & \tfrac{2}{3}\\
1 & 0 & -1 & 2 & \\
2 & -1 & 3 &  & \\
3 & 2 &  &  & 
\end{array}
\end{eqnarray*}

Evaluando en un punto interno (por ejemplo, $x=1.5$).
\begin{eqnarray*}
\begin{aligned}
P_3(1.5) 
&= 1 + (-1)(1.5) + 0\cdot(1.5)(0.5) + \tfrac{2}{3}(1.5)(0.5)(-0.5)\\
&= 1 - 1.5 + \tfrac{2}{3}\cdot(1.5\cdot 0.5\cdot -0.5)\\
&= -0.5 + \tfrac{2}{3}\cdot(-0.375)\\
&= -0.5 - 0.25 = -0.75.
\end{aligned}
\end{eqnarray*}
\end{Ejem}



\begin{Ejem}
$f(x)=x^2$ en $x_0=0$, $h=1$, estimar $f(0.5)$
Datos en $x=0,1,2,3$:
\begin{eqnarray*}
\begin{array}{c|cccc}
x & 0 & 1 & 2 & 3\\ \hline
y=f(x)=x^2 & 0 & 1 & 4 & 9
\end{array}
\end{eqnarray*}
Tabla de diferencias progresivas (en la primera fila):
\begin{eqnarray*}
\begin{array}{c|ccc}
\Delta y_0=1-0=1 & \Delta^2 y_0=3-1=2 & \Delta^3 y_0=2-2=0
\end{array}
\end{eqnarray*}
Aqu\'i $\Delta y_1=4-1=3$, $\Delta^2 y_1=5-3=2$ (mostradas s\'olo para c\'alculo de $\Delta^2 y_0$).

Par\'ametro $p=\dfrac{x-x_0}{h}=\dfrac{0.5-0}{1}=0.5$.
\begin{eqnarray*}
\begin{aligned}
P(0.5)
&= y_0 + p\,\Delta y_0 + \frac{p(p-1)}{2}\,\Delta^2 y_0 \\
&= 0 + (0.5)(1) + \frac{0.5(-0.5)}{2}\,(2) \\
&= 0.5 + \left(-\frac{0.25}{2}\right)(2)
= 0.5 - 0.25 = 0.25.
\end{aligned}
\end{eqnarray*}
Valor exacto: $f(0.5)=0.25$. Coincide (el t\'ermino de tercer orden es nulo).
\end{Ejem}


\begin{Ejem}
 $f(x)=x^3$ en $x_0=1$, $h=1$, estimar $f(1.5)$
Datos en $x=1,2,3,4$:
\begin{eqnarray*}
\begin{array}{c|cccc}
x & 1 & 2 & 3 & 4\\ \hline
y=f(x)=x^3 & 1 & 8 & 27 & 64
\end{array}
\end{eqnarray*}
Diferencias progresivas (primera fila en $x_0=1$):
\begin{eqnarray*}
\Delta y_0=8-1=7,\quad \Delta y_1=27-8=19,\quad \Delta y_2=64-27=37.
\end{eqnarray*}
\begin{eqnarray*}
\Delta^2 y_0=19-7=12,\quad \Delta^2 y_1=37-19=18,\qquad
\Delta^3 y_0=18-12=6.
\end{eqnarray*}
Par\'ametro $p=\dfrac{x-x_0}{h}=\dfrac{1.5-1}{1}=0.5$.
\begin{eqnarray*}
\begin{aligned}
P(1.5)
&= y_0 + p\,\Delta y_0 + \frac{p(p-1)}{2}\,\Delta^2 y_0
 + \frac{p(p-1)(p-2)}{6}\,\Delta^3 y_0\\
&= 1 + (0.5)(7) + \frac{0.5(-0.5)}{2}(12)
 + \frac{0.5(-0.5)(-1.5)}{6}(6).
\end{aligned}
\end{eqnarray*}
C\'alculo t\'ermino a t\'ermino:
\begin{eqnarray*}
1 + 3.5 + \left(\frac{-0.25}{2}\right)12 + \left(\frac{0.375}{6}\right)6
= 1 + 3.5 - 1.5 + 0.375 = 3.375.
\end{eqnarray*}
Exacto: $f(1.5)=(1.5)^3=3.375$.
\end{Ejem}


\begin{Ejem}  $f(x)=x^2$ en $x_n=3$, $h=1$, estimar $f(2.6)$
Datos en $x=0,1,2,3$ (como en el Ejemplo 1).
Diferencias regresivas en el extremo $x_3=3$:
\begin{eqnarray*}
\nabla y_3 = y_3-y_2 = 9-4=5,\quad
\nabla^2 y_3 = \nabla y_3 - \nabla y_2 = 5 - 3 = 2,\quad
\nabla^3 y_3 = 2 - 2 = 0.
\end{eqnarray*}
Aqu\'i $\nabla y_2 = y_2-y_1=3$, $\nabla^2 y_2 = \nabla y_2 - \nabla y_1 = 3-2=1$ (solo de apoyo).

Par\'ametro $q=\dfrac{x-x_n}{h}=\dfrac{2.6-3}{1}=-0.4$.
\begin{eqnarray*}
\begin{aligned}
P(2.6)
&= y_3 + q\,\nabla y_3 + \frac{q(q+1)}{2}\,\nabla^2 y_3 \\
&= 9 + (-0.4)(5) + \frac{(-0.4)(0.6)}{2}(2) \\
&= 9 - 2 - 0.24 = 6.76.
\end{aligned}
\end{eqnarray*}
Exacto: $f(2.6)=(2.6)^2=6.76$.
\end{Ejem}


\begin{Ejem}
 $f(x)=x^3$ en $x_n=4$, $h=1$, estimar $f(3.2)$
Datos en $x=1,2,3,4$ .
Diferencias regresivas en el extremo $x_4=4$:
\begin{eqnarray*}
\begin{aligned}
\nabla y_4 &= y_4-y_3 = 64-27=37,\\
\nabla^2 y_4 &= \nabla y_4 - \nabla y_3 = 37 - 19 = 18,\\
\nabla^3 y_4 &= \nabla^2 y_4 - \nabla^2 y_3 = 18 - 12 = 6.
\end{aligned}
\end{eqnarray*}
Par\'ametro $q=\dfrac{x-x_n}{h}=\dfrac{3.2-4}{1}=-0.8$.
\begin{eqnarray*}
\begin{aligned}
P(3.2)
&= y_4 + q\,\nabla y_4 + \frac{q(q+1)}{2}\,\nabla^2 y_4
 + \frac{q(q+1)(q+2)}{6}\,\nabla^3 y_4\\
&= 64 + (-0.8)(37) + \frac{(-0.8)(0.2)}{2}(18)
 + \frac{(-0.8)(0.2)(1.2)}{6}(6).
\end{aligned}
\end{eqnarray*}
C\'alculo:
\begin{eqnarray*}
64 - 29.6 + (-0.08)(18) + \left(\frac{-0.192}{6}\right)6
= 64 - 29.6 - 1.44 - 0.192 = 32.768.
\end{eqnarray*}
Exacto: $f(3.2)=(3.2)^3=32.768$.
\end{Ejem}


\begin{Note}
\begin{itemize}
    \item Use Newton hacia adelante si $x$ est\'a pr\'oximo a $x_0$; use Newton hacia atr\'as si $x$ est\'a pr\'oximo a $x_n$.
    \item Para funciones polin\'omicas de grado $m$, las diferencias de orden $>m$ son cero y la f\'ormula se vuelve exacta con $m+1$ nodos.
    \item En datos reales con ruido, truncar la serie a pocas diferencias suele mejorar estabilidad.
\end{itemize}
\end{Note}

\begin{Ejem}

\begin{eqnarray*}
\begin{array}{c|cccc}
x & 0 & 1 & 2 & 3\\ \hline
f(x) & 0 & 1 & 4 & 9
\end{array}
\end{eqnarray*}

\begin{eqnarray*}
\begin{array}{c|cccc}
x & y & \Delta y & \Delta^2 y & \Delta^3 y\\ \hline
0 & 0 & 1 & 2 & 0\\
1 & 1 & 3 & 2 &  \\
2 & 4 & 5 &   &  \\
3 & 9 &   &   &  
\end{array}
\end{eqnarray*}

Se quiere estimar:$f(0.5)$, con $x_0=0$, $h=1$ y $p=0.5$.

\textbf{Sustituci\'on:}
\begin{eqnarray*}
\begin{aligned}
P(0.5) &= y_0 + p\,\Delta y_0 + \frac{p(p-1)}{2!}\Delta^2 y_0 \\
&= 0 + (0.5)(1) + \frac{(0.5)(-0.5)}{2}(2) \\
&= 0.5 - 0.25 = 0.25.
\end{aligned}
\end{eqnarray*}

Verificando: $f(0.5) = (0.5)^2 = 0.25.$ $\Rightarrow$ Coincide exactamente.
\end{Ejem}

\begin{Ejem}

\begin{eqnarray*}
\begin{array}{c|cccc}
x & 1 & 2 & 3 & 4\\ \hline
f(x) & 1 & 8 & 27 & 64
\end{array}
\end{eqnarray*}

La tabla de diferencias queda:
\begin{eqnarray*}
\begin{array}{c|cccc}
x & y & \Delta y & \Delta^2 y & \Delta^3 y\\ \hline
1 & 1 & 7 & 12 & 6\\
2 & 8 & 19 & 18 &  \\
3 & 27 & 37 &   &  \\
4 & 64 &   &   &  
\end{array}
\end{eqnarray*}

Estimemos $f(1.5)$.  
$x_0=1,\; h=1,\; p=0.5.$

\begin{eqnarray*}
\begin{aligned}
P(1.5)
&= y_0 + p\,\Delta y_0 + \frac{p(p-1)}{2!}\Delta^2 y_0
+ \frac{p(p-1)(p-2)}{3!}\Delta^3 y_0\\
&= 1 + (0.5)(7) + \frac{(0.5)(-0.5)}{2}(12)
+ \frac{(0.5)(-0.5)(-1.5)}{6}(6)\\
&= 1 + 3.5 - 1.5 + 0.375 = 3.375
\end{aligned}
\end{eqnarray*}

verificando $f(1.5) = (1.5)^3 = 3.375$, es decir el m\'etodo reproduce perfectamente el valor porque $f$ es un polinomio de grado 3 y se usaron 4 nodos.
\end{Ejem}


\begin{Ejem}

\begin{eqnarray*}
\begin{array}{c|cccc}
x & 0 & 1 & 2 & 3\\ \hline
f(x) & 0 & 1 & 4 & 9
\end{array}
\end{eqnarray*}

Las Diferencias regresivas (desde el final):
\begin{eqnarray*}
\begin{array}{c|cccc}
x & y & \nabla y & \nabla^2 y & \nabla^3 y\\ \hline
0 & 0 &   &   &  \\
1 & 1 & 1 &   &  \\
2 & 4 & 3 & 2 &  \\
3 & 9 & 5 & 2 & 0
\end{array}
\end{eqnarray*}

Estimando $f(2.6)$: $x_n=3,\; h=1,\; q = \dfrac{2.6 - 3}{1} = -0.4.$

\begin{eqnarray*}
\begin{aligned}
P(2.6)
&= y_3 + q\,\nabla y_3 + \frac{q(q+1)}{2!}\nabla^2 y_3\\
&= 9 + (-0.4)(5) + \frac{(-0.4)(0.6)}{2}(2)\\
&= 9 - 2 - 0.24 = 6.76.
\end{aligned}
\end{eqnarray*}

donde $f(2.6) = (2.6)^2 = 6.76.$

\end{Ejem}

\begin{Ejem}
\begin{eqnarray*}
\begin{array}{c|cccc}
x & 1 & 2 & 3 & 4\\ \hline
f(x) & 1 & 8 & 27 & 64
\end{array}
\end{eqnarray*}

donde las Diferencias regresivas:
\begin{eqnarray*}
\begin{array}{c|cccc}
x & y & \nabla y & \nabla^2 y & \nabla^3 y\\ \hline
1 & 1 &   &   &  \\
2 & 8 & 7 &   &  \\
3 & 27 & 19 & 12 &  \\
4 & 64 & 37 & 18 & 6
\end{array}
\end{eqnarray*}

Estimando a $f(3.2)$:   $x_n=4,\; h=1,\; q = \dfrac{3.2 - 4}{1} = -0.8.$

\begin{eqnarray*}
\begin{aligned}
P(3.2)
&= y_4 + q\,\nabla y_4 + \frac{q(q+1)}{2!}\nabla^2 y_4
+ \frac{q(q+1)(q+2)}{3!}\nabla^3 y_4\\
&= 64 + (-0.8)(37) + \frac{(-0.8)(0.2)}{2}(18)
+ \frac{(-0.8)(0.2)(1.2)}{6}(6).
\end{aligned}
\end{eqnarray*}

donde $64 - 29.6 - 1.44 - 0.192 = 32.768$, vefrificando $f(3.2) = (3.2)^3 = 32.768.$
\end{Ejem}


\begin{Ejem}
Implementaci\'on num\'erica:
\begin{verbatim}
is_equispaced <- function(x, tol = 1e-9) {
  if (length(x) < 2) return(TRUE)
  h <- diff(x)
  max(abs(h - h[1])) < tol
}

falling_prod <- function(p, k) {
  if (k == 0) return(1)
  prod(p - 0:(k-1))
}

rising_prod <- function(q, k) {
  if (k == 0) return(1)
  prod(q + 0:(k-1))
}

# =========================================================
# Tablas de diferencias
# =========================================================
forward_diff_table <- function(x, y, check_equispaced = TRUE) {
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  if (check_equispaced && !is_equispaced(x)) stop("Los nodos no son equiespaciados.")
  T <- matrix(0, nrow = n, ncol = n)
  T[,1] <- y
  if (n >= 2) {
    for (j in 2:n) {
      for (i in 1:(n - j + 1)) {
        T[i,j] <- T[i+1,j-1] - T[i,j-1]   # Delta^j y_i
      }
    }
  }
  T
}

backward_diff_table <- function(x, y, check_equispaced = TRUE) {
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  if (check_equispaced && !is_equispaced(x)) stop("Los nodos no son equiespaciados.")
  T <- matrix(0, nrow = n, ncol = n)
  T[,1] <- y
  if (n >= 2) {
    for (j in 2:n) {
      for (i in n:j) {
        T[i,j] <- T[i,j-1] - T[i-1,j-1]   # Nabla^j y_i
      }
    }
  }
  T
}

# =========================================================
# Evaluadores Newton
# - max_order: orden maximo a usar (por defecto, n-1). Truncar ayuda con ruido.
# =========================================================
newton_forward_eval <- function(x, T, xq, max_order = NULL) {
  n <- length(x)
  h <- x[2] - x[1]
  p <- (xq - x[1]) / h
  if (is.null(max_order)) max_order <- n - 1
  max_order <- max(0, min(max_order, n - 1))
  P <- T[1,1]
  for (k in 1:max_order) {
    coef <- falling_prod(p, k) / factorial(k)
    P <- P + coef * T[1, k + 1]
  }
  # Estimacion de error de truncamiento (termino siguiente) si existe:
  err <- NA_real_
  next_k <- max_order + 1
  if (next_k <= n - 1) {
    coef_next <- falling_prod(p, next_k) / factorial(next_k)
    err <- abs(coef_next * T[1, next_k + 1])
  }
  list(value = P, est_trunc_error = err, p = p)
}

newton_backward_eval <- function(x, T, xq, max_order = NULL) {
  n <- length(x)
  h <- x[2] - x[1]
  q <- (xq - x[n]) / h
  if (is.null(max_order)) max_order <- n - 1
  max_order <- max(0, min(max_order, n - 1))
  P <- T[n,1]
  for (k in 1:max_order) {
    coef <- rising_prod(q, k) / factorial(k)
    P <- P + coef * T[n, k + 1]
  }
  # Estimacion de error (termino siguiente) si existe:
  err <- NA_real_
  next_k <- max_order + 1
  if (next_k <= n - 1) {
    coef_next <- rising_prod(q, next_k) / factorial(next_k)
    err <- abs(coef_next * T[n, next_k + 1])
  }
  list(value = P, est_trunc_error = err, q = q)
}

# =========================================================
# Seleccion automatica de metodo y evaluacion vectorizada
# - method = "auto" | "forward" | "backward"
# - max_order: truncamiento del orden
# Devuelve data.frame con resultados por cada xq
# =========================================================
newton_interp_equispaced <- function(x, y, xq, method = "auto", max_order = NULL) {
  if (!is_equispaced(x)) stop("Los nodos x deben ser equiespaciados.")
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  # Precomputo de tablas
  Tforw <- forward_diff_table(x, y, check_equispaced = FALSE)
  Tback <- backward_diff_table(x, y, check_equispaced = FALSE)
  # Funcion auxiliar para elegir metodo segun cercania
  choose_method <- function(xq_single) {
    if (method == "forward") return("forward")
    if (method == "backward") return("backward")
    # auto: decide por cercania
    d0 <- abs(xq_single - x[1])
    dn <- abs(xq_single - x[n])
    if (d0 <= dn) "forward" else "backward"
  }
  # Evaluacion (vectorizada)
  vals <- numeric(length(xq))
  errs <- rep(NA_real_, length(xq))
  meth <- character(length(xq))
  par1 <- numeric(length(xq))  # p o q usado
  for (i in seq_along(xq)) {
    m <- choose_method(xq[i])
    meth[i] <- m
    if (m == "forward") {
      res <- newton_forward_eval(x, Tforw, xq[i], max_order = max_order)
      vals[i] <- res$value
      errs[i] <- res$est_trunc_error
      par1[i] <- res$p
    } else {
      res <- newton_backward_eval(x, Tback, xq[i], max_order = max_order)
      vals[i] <- res$value
      errs[i] <- res$est_trunc_error
      par1[i] <- res$q
    }
  }\newtheorem{Ses}{Sesi\'on}[section]
  data.frame(
    xq = xq,
    estimate = vals,
    est_trunc_error = errs,
    method_used = meth,
    p_or_q = par1
  )
}

# =========================================================
# Ejemplos rapidos
# =========================================================
# Ejemplo 1 (adelante, auto): f(x)=x^2 en x=0:3, evaluar en xq=seq(0,2,by=0.5)
# x <- 0:3; y <- x^2
# newton_interp_equispaced(x, y, seq(0, 2, by = 0.5))

# Ejemplo 2 (atras, auto): f(x)=x^3 en x=1:4, evaluar en xq=c(3.2, 3.6)
# x <- 1:4; y <- x^3
# newton_interp_equispaced(x, y, c(3.2, 3.6))
\end{verbatim}
\end{Ejem}


%<<==>><<==>><<==>><<==>><<==>><<==>>
\section{Splines}
%<<==>><<==>><<==>><<==>><<==>><<==>>

Los \textbf{splines} son funciones polin\'omicas por tramos utilizadas para interpolar un conjunto de puntos de forma suave y estable.  
A diferencia de un \'unico polinomio de grado alto (que puede oscilar violentamente entre nodos, efecto de Runge), los splines utilizan polinomios de bajo grado en cada intervalo, garantizando continuidad en los nodos y sus derivadas. Le idea es que en cada subintervalo $[x_i, x_{i+1}]$ se construye un polinomio c\'ubico $S_i(x)$, y se imponen condiciones para que toda la funci\'on $S(x)$ sea continua, con derivadas suaves.


\begin{Def}

Sea un conjunto de nodos: $x_0 < x_1 < \cdots < x_n$, y valores conocidos: $y_i = f(x_i), \quad i = 0, 1, \ldots, n.$

El \textbf{spline c\'ubico} $S(x)$ se define por tramos:
\begin{eqnarray}
S(x) =
\begin{cases}
S_0(x), & x_0 \le x < x_1,\\
S_1(x), & x_1 \le x < x_2,\\
\vdots & \\
S_{n-1}(x), & x_{n-1} \le x \le x_n,
\end{cases}
\end{eqnarray}
donde cada tramo es un polinomio c\'ubico:
\begin{eqnarray}
S_i(x) = a_i + b_i(x - x_i) + c_i(x - x_i)^2 + d_i(x - x_i)^3.
\end{eqnarray}
\end{Def}

El spline c\'ubico debe cumplir:
\begin{enumerate}
    \item Interpolaci\'on: $S_i(x_i) = y_i$, \quad $S_i(x_{i+1}) = y_{i+1}$.
    \item Continuidad de la primera derivada: $S_i'(x_{i+1}) = S_{i+1}'(x_{i+1})$.
    \item Continuidad de la segunda derivada: $S_i''(x_{i+1}) = S_{i+1}''(x_{i+1})$.
\end{enumerate}

Estas condiciones producen $4(n-1)$ ecuaciones, que junto con dos condiciones adicionales en los extremos definen completamente los $4n$ coeficientes.


\begin{itemize}
    \item \textbf{Spline natural:}  
    Se impone $S''(x_0)=S''(x_n)=0$.  
    Esto genera una forma “suave” en los extremos.
    
    \item \textbf{Spline completo (o clamped):}  
    Se imponen las derivadas primeras conocidas:  
    $S'(x_0)=f'(x_0)$, $S'(x_n)=f'(x_n)$.
    
    \item \textbf{Spline “not-a-knot”:}  
    Obliga continuidad de la tercera derivada en los nodos interiores $x_1$ y $x_{n-1}$.
\end{itemize}

\subsubsection*{Sistema tridiagonal de las segundas derivadas}

Definimos:
\begin{eqnarray}
h_i = x_{i+1}-x_i, \qquad m_i = \frac{y_{i+1}-y_i}{h_i}, \quad i=0,\ldots,n-1.
\end{eqnarray}

El sistema para las segundas derivadas $M_i=S''(x_i)$ es:
\begin{eqnarray}
h_{i-1}M_{i-1}+2(h_{i-1}+h_i)M_i+h_iM_{i+1}=6(m_i-m_{i-1}),\quad i=1,\ldots,n-1,
\end{eqnarray}
con las condiciones de frontera:
\begin{eqnarray}
\text{Spline natural: } M_0=M_n=0.
\end{eqnarray}

\textbf{Una vez calculadas las $M_i$,} cada tramo $S_i(x)$ se obtiene como:
\begin{eqnarray}
S_i(x)=
\frac{M_{i+1}(x-x_i)^3}{6h_i}
-\frac{M_i(x_{i+1}-x)^3}{6h_i}
+\left( \frac{y_{i+1}}{h_i}-\frac{M_{i+1}h_i}{6} \right)(x-x_i)
+\left( \frac{y_i}{h_i}-\frac{M_i h_i}{6} \right)(x_{i+1}-x).
\end{eqnarray}


\begin{Algthm}
El sistema tridiagonal se resuelve eficientemente con el \textbf{m\'etodo de Thomas} (algoritmo especializado para matrices tridiagonales).  
El proceso computacional sigue estos pasos:
\begin{enumerate}
    \item Calcular los $h_i$ y $m_i$.
    \item Formar el sistema tridiagonal en t\'erminos de $M_i$.
    \item Resolver para obtener $M_1,\ldots,M_{n-1}$.
    \item Construir los polinomios $S_i(x)$ en cada subintervalo.
\end{enumerate}
\end{Algthm}

\begin{Ejem}
Interpolar los puntos:
\begin{eqnarray*}
(0,0), \quad (1,1), \quad (2,0).
\end{eqnarray*}

Se tiene  $n=2$, $h_0=h_1=1$, 
$m_0=1$, $m_1=-1$.

Ecuaci\'on central:
\begin{eqnarray*}
h_0 M_0 + 2(h_0 + h_1) M_1 + h_1 M_2 = 6(m_1 - m_0),
\end{eqnarray*}

\begin{eqnarray*}
1(0) + 4M_1 + 1(0) = 6(-1 - 1) = -12 \quad \Rightarrow \quad M_1 = -3.
\end{eqnarray*}
Y $M_0=M_2=0$.

Sustituyendo en la f\'ormula del tramo $[0,1]$:
\begin{eqnarray*}
S_0(x) = -\frac{M_1}{6}(x-1)^3 + x.
\end{eqnarray*}
En $[1,2]$:
\begin{eqnarray*}
S_1(x) = -\frac{M_1}{6}(2-x)^3 + (2-x).
\end{eqnarray*}

Graficar ambos tramos muestra una curva suave en forma de “loma”.
\end{Ejem}


\begin{Ejem}
Puntos:
\begin{eqnarray*}
(0,0), \quad (1,1), \quad (2,0), \quad (3,1).
\end{eqnarray*}
Datos: $h_i=1$, 
$m_0=1$, $m_1=-1$, $m_2=1$.

Sistema:
\begin{eqnarray*}
\begin{cases}
4M_1 + M_2 = 6(-1 - 1) = -12,\\
M_1 + 4M_2 = 6(1 - (-1)) = 12.
\end{cases}
\end{eqnarray*}
Resolviendo:
\begin{eqnarray*}
M_1 = -4, \quad M_2 = 4, \quad M_0 = M_3 = 0.
\end{eqnarray*}

Cada tramo se obtiene sustituyendo los valores de $M_i$ en la f\'ormula general.
\end{Ejem}


\begin{Ejem}
Puntos:
\begin{eqnarray*}
(0,1), \quad (1,2), \quad (2,0),
\end{eqnarray*}
con derivadas conocidas $S'(0)=0$ y $S'(2)=-1$.

Se aplican las condiciones en los extremos:
\begin{eqnarray*}
2h_0 M_0 + h_0 M_1 = 6\left(\frac{y_1 - y_0}{h_0} - S'(0)\right),
\end{eqnarray*}

\begin{eqnarray*}
h_0 M_0 + 2h_0 M_1 = 6\left(S'(2) - \frac{y_2 - y_1}{h_0}\right).
\end{eqnarray*}
Resolviendo se obtienen $M_0$ y $M_1$, y luego los coeficientes $a_i,b_i,c_i,d_i$ de cada tramo.
\end{Ejem}


\begin{Ejem}
Mediciones de temperatura ($^\circ$C) a distintas horas:
\begin{eqnarray*}
\begin{array}{c|cccc}
\text{Hora (h)} & 0 & 3 & 6 & 9\\ \hline
\text{Temperatura} & 15 & 18 & 14 & 10
\end{array}
\end{eqnarray*}
Interpolar con un spline c\'ubico natural y estimar $T(4.5)$.

\begin{Algthm}
\begin{enumerate}
    \item Calcular $h_i=3$, $m_i=(y_{i+1}-y_i)/h_i$.
    \item Formar el sistema para $M_1, M_2$.
    \item Resolverlo con el m\'etodo de Thomas.
    \item Evaluar $S_1(4.5)$ usando la f\'ormula de $S_i(x)$ en el intervalo $[3,6]$.
\end{enumerate}
\end{Algthm}

\textbf{Resultado esperado:} $T(4.5)\approx 15.5^\circ$C.  
La curva obtenida describe de forma suave la variaci\'on de temperatura entre mediciones.
\end{Ejem}


\begin{Note}
\begin{itemize}
    \item Los splines c\'ubicos son continuos en $S$, $S'$, y $S''$, lo que garantiza suavidad.
    \item Reducen el error sin introducir oscilaciones (efecto Runge).
    \item El error m\'aximo en un spline c\'ubico natural es proporcional a $O(h^4)$.
    \item Son ampliamente usados en gr\'aficos por computadora, CAD, animaci\'on y an\'alisis de datos experimentales.
\end{itemize}
\end{Note}


%<<==>><<==>><<==>><<==>><<==>><<==>>
\section{Error de Interpolaci\'on}
%<<==>><<==>><<==>><<==>><<==>><<==>>

La interpolaci\'on polin\'omica busca aproximar una funci\'on $f(x)$ mediante un polinomio $P_n(x)$ que coincide con $f(x)$ en $n{+}1$ puntos conocidos.  
Sin embargo, entre los nodos de interpolaci\'on puede existir una diferencia o \textbf{error de interpolaci\'on}, denotado por $R_n(x)$.

\subsubsection*{F\'ormula general del error}

Sea $f\in C^{(n+1)}[a,b]$ y $P_n(x)$ el polinomio interpolante de grado $n$ para los puntos $(x_0,y_0),\ldots,(x_n,y_n)$ con $y_i=f(x_i)$.  
Entonces, para todo $x\in[a,b]$, existe un n\'umero $\xi\in(a,b)$ tal que:

\begin{eqnarray*}
R_n(x) = f(x) - P_n(x)
= \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^{n}(x - x_i).
\end{eqnarray*}

\textbf{Interpretaci\'on:}  
El error depende de dos factores:
\begin{itemize}
    \item El t\'ermino $\dfrac{f^{(n+1)}(\xi)}{(n+1)!}$, relacionado con la suavidad de $f(x)$.
    \item El producto $\displaystyle\prod_{i=0}^{n}(x - x_i)$, que refleja la posici\'on de los nodos de interpolaci\'on.
\end{itemize}

\subsubsection*{Cota del error}

Si se conoce una cota $|f^{(n+1)}(x)| \le M$ para $x \in [a,b]$, entonces:

\begin{eqnarray*}
|R_n(x)| \le \frac{M}{(n+1)!}\,\max_{x\in[a,b]}\left|\prod_{i=0}^{n}(x - x_i)\right|.
\end{eqnarray*}

Esta expresi\'on indica que el error aumenta:
\begin{itemize}
    \item Si $f^{(n+1)}(x)$ es grande (funci\'on muy curvada o con derivadas altas).
    \item Si los nodos $x_i$ est\'an mal distribuidos (especialmente equiespaciados para $n$ grande).
\end{itemize}

\subsubsection*{Comportamiento del error y efecto de Runge}

Cuando los nodos son equiespaciados, el t\'ermino
\begin{eqnarray*}
\prod_{i=0}^{n}(x - x_i)
\end{eqnarray*}
crece r\'apidamente en los extremos del intervalo, provocando oscilaciones en el polinomio interpolante — fen\'omeno conocido como \textbf{efecto de Runge}.  
Este efecto se agrava con $n$ grande y funciones con alta curvatura, como $f(x) = \dfrac{1}{1+x^2}$.

\textit{Conclusi\'on: aumentar el grado $n$ no garantiza una mejor aproximaci\'on.}


\subsubsection*{Nodos de Chebyshev (minimizan el error m\'aximo)}

Para reducir las oscilaciones, se eligen nodos no equiespaciados, conocidos como \textbf{nodos de Chebyshev}.  
En el intervalo $[-1,1]$ se definen como:

\begin{eqnarray*}
x_i = \cos\left(\frac{2i+1}{2(n+1)}\pi\right), \quad i=0,1,\ldots,n.
\end{eqnarray*}

Estos nodos distribuyen m\'as puntos cerca de los extremos del intervalo, donde el error tiende a crecer m\'as.

\textbf{Propiedad fundamental:}
Los nodos de Chebyshev minimizan la cantidad
\begin{eqnarray*}
\max_{x\in[-1,1]}\left|\prod_{i=0}^{n}(x - x_i)\right|,
\end{eqnarray*}
y, por tanto, minimizan el error m\'aximo de interpolaci\'on en el sentido de la norma infinita.

\begin{Ejem}: comparaci\'on cualitativa

Sea $f(x)=\dfrac{1}{1+x^2}$ en $[-5,5]$.

\begin{itemize}
    \item Con $n=10$ y nodos \textbf{equiespaciados}: el polinomio presenta grandes oscilaciones en los extremos (efecto de Runge).
    \item Con $n=10$ y nodos \textbf{de Chebyshev}: la interpolaci\'on es estable y el error m\'aximo es mucho menor.
\end{itemize}
\end{Ejem}

\begin{Ejem} Estimaci\'on del error
Sea $f(x)=e^x$ en $[0,1]$, con nodos equiespaciados $x_0=0$, $x_1=0.5$, $x_2=1$.
Entonces $f^{(3)}(x)=e^x \le e$.

Usando la f\'ormula del error para $x=0.25$:
\begin{eqnarray*}
|R_2(0.25)| \le \frac{e}{3!}|(0.25-0)(0.25-0.5)(0.25-1)| = \frac{e}{6}(0.25\cdot 0.25\cdot 0.75) \approx 0.0085.
\end{eqnarray*}
La cota muestra que el error es pequeño, y decrece proporcionalmente a $h^{n+1}$.
\end{Ejem}

\begin{Note}
\begin{itemize}
    \item La magnitud de $R_n(x)$ depende tanto de la funci\'on como de la elecci\'on de nodos.
    \item Nodos de Chebyshev distribuyen mejor el error, especialmente para grados altos.
    \item En la pr\'actica, cuando los datos son experimentales o $f(x)$ es desconocida, se prefiere reducir $n$ y usar \textbf{splines}, que mantienen continuidad y baja oscilaci\'on.
\end{itemize}
\end{Note}


%<<==>><<==>><<==>><<==>><<==>><<==>>
\section{Ejercicios }
%<<==>><<==>><<==>><<==>><<==>><<==>>
Esta secci\'on presenta ejercicios orientados a consolidar la comprensi\'on de los m\'etodos 
de interpolaci\'on estudiados: Lagrange, Newton (diferencias divididas) 
y Newton hacia adelante/atr\'as (diferencias finitas).  

Se recomienda resolver los primeros ejercicios de forma manual 
y posteriormente verificar los resultados con el c\'odigo en R proporcionado.

\begin{Ejer}
\begin{enumerate}
    \item Dado el conjunto de puntos
    \begin{eqnarray*}
    (0,1),\ (1,3),\ (2,11),
    \end{eqnarray*}
    \begin{enumerate}
        \item Determine el polinomio interpolante mediante el \textbf{m\'etodo de Lagrange}.  
        \item Verifique el resultado construyendo la \textbf{tabla de diferencias divididas} 
              y aplicando el \textbf{m\'etodo de Newton}.  
        \item Eval\'ue el polinomio en $x=1.5$ y compare con la funci\'on cuadr\'atica
              ajustada visualmente a los puntos.
    \end{enumerate}
    \vspace{1em}

    \item Con los puntos
    \begin{eqnarray*}
    (1,2),\ (2,5),\ (3,10),\ (4,17),
    \end{eqnarray*}
    \begin{enumerate}
        \item Obtenga el polinomio interpolante de grado 3 usando el \textbf{m\'etodo de Newton}.  
        \item Muestre la tabla completa de diferencias divididas.  
        \item Eval\'ue el polinomio en $x=2.5$.  
        \item Verifique el resultado calculando directamente $f(x)=x^2+1$ y compare.
    \end{enumerate}
    \vspace{1em}

    \item Dados los puntos
    \begin{eqnarray*}
    (-1,4),\ (0,1),\ (2,3),
    \end{eqnarray*}
    \begin{enumerate}
        \item Construya el polinomio de Lagrange.  
        \item Expanda y simplifique el resultado hasta la forma general $P_2(x)=a_0+a_1x+a_2x^2.$
        \item Verifique los valores de $P_2(-1)$, $P_2(0)$ y $P_2(2)$.
    \end{enumerate}
\end{enumerate}
\end{Ejer}


\begin{Ejer}

\begin{enumerate}
    \item Para la funci\'on tabulada
    \begin{eqnarray*}
    \begin{array}{c|ccccc}
    x & 0 & 1 & 2 & 3 & 4\\ \hline
    f(x) & 1 & 2 & 4 & 8 & 16
    \end{array}
    \end{eqnarray*}
    \begin{enumerate}
        \item Construya la \textbf{tabla de diferencias progresivas} $\Delta y_i$.  
        \item Determine el polinomio interpolante con Newton hacia adelante.  
        \item Calcule $f(2.5)$ y comp\'arelo con $2^{2.5}$.
    \end{enumerate}
    \vspace{1em}

    \item Para los valores:
    \begin{eqnarray*}
    \begin{array}{c|cccc}
    x & 1 & 2 & 3 & 4\\ \hline
    f(x) & 1 & 8 & 27 & 64
    \end{array}
    \end{eqnarray*}
    \begin{enumerate}
        \item Construya la tabla de \textbf{diferencias regresivas} $\nabla y_i$.  
        \item Aplique la f\'ormula de Newton hacia atr\'as para estimar $f(3.4)$.  
        \item Compare con el valor exacto de $f(x)=x^3$.
    \end{enumerate}
    \vspace{1em}

    \item Para los datos experimentales:
    \begin{eqnarray*}
    \begin{array}{c|ccccc}
    x & 10 & 20 & 30 & 40 & 50\\ \hline
    y & 20.5 & 24.0 & 32.0 & 42.5 & 51.0
    \end{array}
    \end{eqnarray*}
    \begin{enumerate}
        \item Verifique que los nodos sean equiespaciados.  
        \item Calcule las diferencias progresivas hasta el tercer orden.  
        \item Estime $y(25)$ usando Newton hacia adelante.  
        \item Comente si el uso de un polinomio de orden m\'as alto podr\'ia 
              mejorar o empeorar la estabilidad num\'erica.
    \end{enumerate}
\end{enumerate}

\end{Ejer}



\begin{Ejer}
Implemente los siguientes ejercicios utilizando las funciones 
\texttt{newton\_interp\_equispaced()}, \texttt{forward\_diff\_table()} 
y \texttt{backward\_diff\_table()}.

\begin{enumerate}
    \item Replique los ejemplos manuales anteriores en R y 
          compare los resultados obtenidos manualmente con los valores calculados por el script.
          Incluya en su reporte:
          \begin{itemize}
              \item La tabla de diferencias generada.  
              \item El valor estimado y el error de truncamiento.  
              \item Una gr\'afica de los puntos originales y el polinomio interpolante.
          \end{itemize}
    \vspace{1em}

    \item Genere aleatoriamente 5 puntos de una funci\'on cuadr\'atica y aplique:
          \begin{itemize}
              \item Interpolaci\'on de Lagrange (con la f\'ormula cl\'asica).  
              \item Interpolaci\'on de Newton (diferencias divididas).  
              \item Interpolaci\'on hacia adelante (diferencias finitas).  
          \end{itemize}
          Compare los tres resultados gr\'aficamente y analice diferencias num\'ericas.

    \vspace{1em}

    \item Usando los datos:
    \begin{eqnarray*}
    x = [0,1,2,3,4], \quad
    y = [2.0, 2.7, 4.8, 8.9, 16.0],
    \end{eqnarray*}
    \begin{enumerate}
        \item Aplique el m\'etodo autom\'atico \texttt{newton\_interp\_equispaced()} 
              para evaluar la funci\'on en $x = 1.5, 2.5, 3.5$.  
        \item Reporte el m\'etodo seleccionado (adelante o atr\'as) y el error estimado.  
        \item Compare con una funci\'on de referencia $f(x) = 2^{x}$.
    \end{enumerate}
\end{enumerate}
\end{Ejer}


\begin{Ejer}
Puntos: $(0,1),(1,3),(2,11)$.
\begin{itemize}
\item Lagrange / Forma general: Sea $P_2(x)=a_0+a_1x+a_2x^2$. Con $x=0\Rightarrow a_0=1$; $x=1\Rightarrow 1+a_1+a_2=3$; $x=2\Rightarrow 1+2a_1+4a_2=11$. De $a_1+a_2=2$ y $a_1+2a_2=5\Rightarrow a_2=3,\ a_1=-1$.\begin{eqnarray*}
P_2(x)=1 - x + 3x^2.
\end{eqnarray*}
\item Evaluaci\'on: $P_2(1.5)=1-1.5+3(2.25)=6.25$.
\item Newton: tabla de diferencias divididas produce los mismos coeficientes.
\end{itemize}
\end{Ejer}

\begin{Ejer}
Puntos: $(1,2),(2,5),(3,10),(4,17)$.
\begin{itemize}
\item Observa que $y=x^2+1$ calza exactamente con los datos.
\item \textbf{Newton}: diferencias
\begin{eqnarray*}
\Delta y=\{3,5,7\},\quad \Delta^2 y=\{2,2\},\quad \Delta^3 y=\{0\}.
\end{eqnarray*}
\item \textbf{Polinomio}: de grado $2$ (t\'ermino c\'ubico nulo), equivale a $P(x)=x^2+1$.

\item \textbf{Evaluaci\'on}: $P(2.5)=2.5^2+1=7.25$.
\end{itemize}
\end{Ejer}

\begin{Ejer}
Puntos: $(-1,4),(0,1),(2,3)$.
\begin{itemize}
\item Forma general $P_2(x)=a_0+a_1x+a_2x^2$. De $x=0\Rightarrow a_0=1$.

\item Sistema: $-a_1+a_2=3$ y $a_1+2a_2=1$. Soluci\'on: $a_2=\tfrac{4}{3}$, $a_1=-\tfrac{5}{3}$.

\begin{eqnarray*}
P_2(x)=1 - \frac{5}{3}x + \frac{4}{3}x^2.
\end{eqnarray*}

\item Verifica: $P_2(-1)=4,\ P_2(0)=1,\ P_2(2)=3$.
\end{itemize}
\end{Ejer}

\begin{Ejer}
Tabla $2^x$ en $x=0,1,2,3,4$: $y=\{1,2,4,8,16\}$.
\begin{itemize}
\item Diferencias progresivas:$\Delta y_0=1,\ \Delta^2 y_0=1,\ \Delta^3 y_0=1,\ \Delta^4 y_0=1$.
\item Newton adelante (grado $4$), $x_0=0,h=1$, $p=2.5$:
\begin{eqnarray*}
P(2.5) &=& y_0 + p\Delta y_0 + \frac{p(p-1)}{2}\Delta^2 y_0+ \frac{p(p-1)(p-2)}{6}\Delta^3 y_0+ \frac{p(p-1)(p-2)(p-3)}{24}\Delta^4 y_0\\
&=& 1 + 2.5 + 1.875 + 0.3125 - 0.0390625\\
&= &5.6484375.
\end{eqnarray*}

\item Comparaci\'on: $2^{2.5}=\sqrt{32}\approx 5.65685425$. Error $\approx -8.4168\times 10^{-3}$ (el grado $4$ no puede reproducir una funci\'on exponencial).
\end{itemize}
\end{Ejer}

\begin{Ejer}
$x=1,2,3,4$ con $f(x)=x^3=\{1,8,27,64\}$; estimar $f(3.4)$ por \textbf{atr\'as}.
\begin{itemize}
\item \textbf{Regresivas en $x_4=4$:} $\nabla y_4=37,\ \nabla^2 y_4=18,\ \nabla^3 y_4=6$.
\item $h=1,\ q=(3.4-4)=-0.6$.
\begin{eqnarray*}
P(3.4) &=& y_4 + q\nabla y_4 + \frac{q(q+1)}{2}\nabla^2 y_4+ \frac{q(q+1)(q+2)}{6}\nabla^3 y_4\\
&=& 64 - 22.2 - 2.16 - 0.336\\
&= &39.304. 
\end{eqnarray*}

\item \textbf{Exacto}: $3.4^3=39.304$. (Para polinomio c\'ubico con $4$ nodos, el m\'etodo reproduce exactamente.)
\end{itemize}
\end{Ejer}

\begin{Ejer}
Datos experimentales equiespaciados ($h=10$):
\begin{eqnarray*}
\begin{array}{c|ccccc}
x & 10 & 20 & 30 & 40 & 50\\ \hline
y & 20.5 & 24.0 & 32.0 & 42.5 & 51.0
\end{array}
\end{eqnarray*}
\begin{itemize}
\item \textbf{Progresivas en $x_0=10$:} 
\begin{eqnarray*}
    \Delta y_0=3.5,\quad \Delta^2 y_0=4.5,\quad \Delta^3 y_0=-2.0,\quad \Delta^4 y_0=-2.5.
\end{eqnarray*}

\item \textbf{Estimar $y(25)$} (adelante, hasta orden $3$): $p=\tfrac{25-10}{10}=1.5$.

\begin{eqnarray*}
\begin{aligned}
P(25) &= y_0 + p\Delta y_0 + \frac{p(p-1)}{2}\Delta^2 y_0+ \frac{p(p-1)(p-2)}{6}\Delta^3 y_0\\
&= 20.5 + (1.5)(3.5) + \frac{1.5\cdot 0.5}{2}(4.5)+ \frac{1.5\cdot 0.5\cdot (-0.5)}{6}(-2.0)\\
&= 27.5625.
\end{aligned}
\end{eqnarray*}

\item \textbf{T\'ermino siguiente (cota pragm\'atica)}:

\begin{eqnarray*}
\frac{p(p-1)(p-2)(p-3)}{24}\,\Delta^4 y_0
= \frac{1.5\cdot 0.5\cdot (-0.5)\cdot (-1.5)}{24}(-2.5)
\approx \boxed{-0.0586}.
\end{eqnarray*}
(Magnitud $\approx 5.86\times 10^{-2}$: sugiere el orden del error de truncamiento.)
\end{itemize}
\end{Ejer}

\begin{Ejer}
Para $5$ puntos de una cuadr\'atica simulada $y=ax^2+bx+c$ con ruido $N(0,\sigma)$:
\begin{itemize}
\item Lagrange (cl\'asico) y Newton (divididas) deben coincidir en precisi\'on con datos sin ruido.
\item Con ruido, truncar el orden (p.ej. usar $3$--$4$ nodos o limitar \texttt{max\_order})  reduce oscilaciones y mejora estabilidad.
\end{itemize}
\end{Ejer}

\begin{Ejer}
Con $x=\{0,1,2,3,4\}$, $y=\{2.0,2.7,4.8,8.9,16.0\}$:
\begin{itemize}
\item \texttt{newton\_interp\_equispaced} seleccionar\'a \emph{adelante} cerca de $x_0$  y \emph{atr\'as} cerca de $x_4$ (o el que quede m\'as cercano).

\item La columna \texttt{est\_trunc\_error} reportar\'a el tamaño del t\'ermino siguiente (gu\'ia de error).
\item Al comparar con $2^x$, se observar\'an discrepancias crecientes fuera del rango central (interpolaci\'on no reproduce exponenciales con grado bajo de forma exacta).
\end{itemize}
\end{Ejer}

\begin{Ejem}
Datos:
\begin{eqnarray*}
(x_i,y_i)=(0,1),\ (1,2.2),\ (2,2.8),\ (3,3.6).
\end{eqnarray*}
Pendientes por tramo
\begin{eqnarray*}
b_0=\frac{2.2-1}{1-0}=1.2,\quad
b_1=\frac{2.8-2.2}{2-1}=0.6,\quad
b_2=\frac{3.6-2.8}{3-2}=0.8.
\end{eqnarray*}
Por tramos:
\begin{eqnarray*}
S(x)=
\begin{cases}
1+1.2(x-0), & 0\le x\le 1,\\
2.2+0.6(x-1), & 1\le x\le 2,\\
2.8+0.8(x-2), & 2\le x\le 3.
\end{cases}
\end{eqnarray*}
Evaluaciones:
\begin{eqnarray*}
S(1.7)=2.2+0.6(0.7)=2.62,\qquad
S(2.4)=2.8+0.8(0.4)=3.12.
\end{eqnarray*}
Verificaciones:
\begin{eqnarray*}
S(1^-)=2.2,\ S(1^+)=2.2;\quad
S(2^-)=2.8,\ S(2^+)=2.8.
\end{eqnarray*}
(S\'olo continuidad de la funci\'on; derivadas cambian en los nudos.)
\end{Ejem}


\begin{Ejem}
Datos:
\begin{eqnarray*}
(x_0,y_0)=(0,0),\ (x_1,y_1)=(1,1),\ (x_2,y_2)=(2,0),\ (x_3,y_3)=(3,1).
\end{eqnarray*}
Paso $h_i=1$. Pendientes locales:
\begin{eqnarray*}
m_0=\frac{1-0}{1}=1,\quad m_1=\frac{0-1}{1}=-1,\quad m_2=\frac{1-0}{1}=1.
\end{eqnarray*}
Spline \emph{natural}: $M_0=M_3=0$. Sistema para $M_1,M_2$ (tridiagonal cl\'asico):
\begin{eqnarray*}
\begin{cases}
M_0+4M_1+M_2=6(m_1-m_0)=6(-1-1)=-12,\\
M_1+4M_2+M_3=6(m_2-m_1)=6(1-(-1))=12.
\end{cases}
\end{eqnarray*}
Con $M_0=M_3=0$:
\begin{eqnarray*}
4M_1+M_2=-12,\qquad M_1+4M_2=12.
\end{eqnarray*}
Resolviendo:
\begin{eqnarray*}
M_2=4,\quad M_1=-4,\quad (M_0=0,\ M_3=0).
\end{eqnarray*}
Coeficientes por tramo ($h_i=1$):
\begin{eqnarray*}
\begin{aligned}
a_i&=y_i,\\
b_i&=m_i-\frac{(2M_i+M_{i+1})h_i}{6},\\
c_i&=\frac{M_i}{2},\\
d_i&=\frac{M_{i+1}-M_i}{6h_i}.
\end{aligned}
\end{eqnarray*}
Tramo $[0,1]$ ($i=0$): $m_0=1,\ M_0=0,\ M_1=-4$:
\begin{eqnarray*}
a_0=0,\ b_0=1-\tfrac{(0-4)}{6}=\tfrac{5}{3},\ c_0=0,\ d_0=\tfrac{-4-0}{6}=-\tfrac{2}{3}.
\end{eqnarray*}
\begin{eqnarray*}
S_0(x)=\tfrac{5}{3}x-\tfrac{2}{3}x^3.
\end{eqnarray*}
Tramo $[1,2]$ ($i=1$): $m_1=-1,\ M_1=-4,\ M_2=4$:
\begin{eqnarray*}
a_1=1,\ b_1=-1-\tfrac{(2(-4)+4)}{6}=-\tfrac{4}{3},\ c_1=-2,\ d_1=\tfrac{4-(-4)}{6}=\tfrac{4}{3},
\end{eqnarray*}
\begin{eqnarray*}
S_1(x)=1-\tfrac{4}{3}(x-1)-2(x-1)^2+\tfrac{4}{3}(x-1)^3.
\end{eqnarray*}
Tramo $[2,3]$ ($i=2$): $m_2=1,\ M_2=4,\ M_3=0$:
\begin{eqnarray*}
a_2=0,\ b_2=1-\tfrac{(8+0)}{6}=-\tfrac{1}{3},\ c_2=2,\ d_2=\tfrac{0-4}{6}=-\tfrac{2}{3},
\end{eqnarray*}
\begin{eqnarray*}
S_2(x)=-\tfrac{1}{3}(x-2)+2(x-2)^2-\tfrac{2}{3}(x-2)^3+0\ (\text{y desplazar por }y_2=0).
\end{eqnarray*}
Evaluaciones:
\begin{eqnarray*}
S(0)=0,\ S(1)=1,\ S(2)=0,\ S(3)=1,\quad S'(0)=\tfrac{5}{3},\ S''(0)=0,\ S''(3)=0.
\end{eqnarray*}
(Estructura suave: continuidad de $S,S',S''$ en $x=1,2$.)
\end{Ejem}



\begin{Ejem}
Datos:
\begin{eqnarray*}
(0,0),\ (1,2),\ (2,3),\qquad S'(0)=1,\ S'(2)=0.
\end{eqnarray*}
$h_0=h_1=1$, $m_0=2$, $m_1=1$.
Frontera clamped y ecuaciones interiores:
\begin{eqnarray*}
\begin{cases}
2h_0M_0+h_0M_1=6(m_0-S'(0))=6,\\
h_0M_0+2(h_0+h_1)M_1+h_1M_2=6(m_1-m_0)=-6,\\
h_1M_1+2h_1M_2=6(S'(2)-m_1)=-6.
\end{cases}
\end{eqnarray*}
Resoluci\'on:
\begin{eqnarray*}
M_0=4,\quad M_1=-2,\quad M_2=-2.
\end{eqnarray*}
Coeficientes (con $h=1$):
\begin{eqnarray*}
\begin{aligned}
a_0&=0,\quad b_0=m_0-\tfrac{2M_0+M_1}{6}=2-\tfrac{8-2}{6}=1,\\
c_0&=\tfrac{M_0}{2}=2,\quad d_0=\tfrac{M_1-M_0}{6}=-1,\\begin{eqnarray*}4pt]
a_1&=2,\quad b_1=m_1-\tfrac{2M_1+M_2}{6}=1-\tfrac{-4-2}{6}=2,\\
c_1&=\tfrac{M_1}{2}=-1,\quad d_1=\tfrac{M_2-M_1}{6}=0.
\end{aligned}
\end{eqnarray*}
Por tramos:
\begin{eqnarray*}
S(x)=
\begin{cases}
x+2x^2-x^3, & 0\le x\le 1,\\
2+2(x-1)-(x-1)^2, & 1\le x\le 2.
\end{cases}
\end{eqnarray*}
Verif.: $S(0)=0,\ S(1)=2,\ S(2)=3$; $S'(0)=1$ y $S'(2)=0$ (clamped cumplido).
\end{Ejem}

\begin{Ejem}
\begin{enumerate}
\item \textbf{Spline lineal:} Con $(0,1.2),(1,2.0),(3,3.1),(4,4.0)$:

\begin{enumerate}
\item Escriba $S_i(x)$ por tramos y eval\'ue $S(2.5)$.
\item Grafique a mano los segmentos y comente la falta de suavidad en los nudos.
\end{enumerate}

\item \textbf{Spline c\'ubico natural:} Con $(0,0),(1,1),(2,0),(3,1)$:

\begin{enumerate}
\item Arme el sistema tridiagonal para $M_1,M_2$ y resu\'elvalo.
\item Obtenga los coeficientes $(a_i,b_i,c_i,d_i)$ y eval\'ue $S(1.5)$ y $S(2.3)$.
\end{enumerate}


\item \textbf{Spline c\'ubico completo:} Con $(0,0),(1,2),(2,3)$ y $S'(0)=1,\ S'(2)=0$:
\begin{enumerate}
\item Arme el sistema para $M_0,M_1,M_2$. 
\item Calcule $S(0.5)$ y $S(1.8)$.
\end{enumerate}
\end{enumerate}
\end{Ejem}

%<<==>><<==>><<==>><<==>><<==>><<==>>
\section{Implementaciones Computacionales}
%<<==>><<==>><<==>><<==>><<==>><<==>>

\subsubsection*{Pseudoc\'odigo (forma cl\'asica)}
Sea \texttt{x[0..n]}, \texttt{y[0..n]} los datos, y \texttt{xq} el punto a evaluar.
\begin{verbatim}
ALGORITMO LagrangeEval(x[0..n], y[0..n], xq):
    p := 0
    para i := 0..n hacer
        Li := 1
        para j := 0..n hacer
            si j != i entonces
                Li := Li * (xq - x[j]) / (x[i] - x[j])
            fin si
        fin para
        p := p + y[i] * Li
    fin para
    retornar p
FIN
\end{verbatim}

\subsubsection*{Pseudoc\'odigo (baric\'entrico, estable)}
Precompute los pesos baric\'entricos $w_i=\displaystyle\frac{1}{\prod_{j\neq i}(x_i-x_j)}$ una sola vez.
\begin{verbatim}
ALGORITMO BarycentricPrecompute(x[0..n]):
    para i := 0..n hacer
        wi := 1
        para j := 0..n hacer
            si j != i entonces
                wi := wi * 1.0 / (x[i] - x[j])
            fin si
        fin para
        w[i] := wi
    fin para
    retornar w
FIN

ALGORITMO BarycentricEval(x[0..n], y[0..n], w[0..n], xq):
    // Si xq coincide con un nodo, devuelve su y exacta
    para i := 0..n hacer
        si xq == x[i] entonces retornar y[i]
    fin para
    num := 0 ; den := 0
    para i := 0..n hacer
        term := w[i] / (xq - x[i])
        num := num + term * y[i]
        den := den + term
    fin para
    retornar num / den
FIN
\end{verbatim}

\subsubsection*{Implementaci\'on en R (forma cl\'asica y baric\'entrica)}
\begin{verbatim}
# --- Forma clasica O(n^2) por evaluacion ---
lagrange_eval <- function(x, y, xq) {
  # x, y: vectores de longitud n+1
  # xq: escalar o vector
  sapply(xq, function(xx) {
    n <- length(x) - 1
    p <- 0
    for (i in 0:n) {
      Li <- 1
      for (j in 0:n) {
        if (j != i) {
          Li <- Li * (xx - x[j+1]) / (x[i+1] - x[j+1])
        }
      }
      p <- p + y[i+1] * Li
    }
    p
  })
}

# --- Pesos baricentricos (precomputo O(n^2)) ---
barycentric_weights <- function(x) {
  n <- length(x) - 1
  w <- rep(1, n+1)
  for (i in 0:n) {
    for (j in 0:n) {
      if (j != i) w[i+1] <- w[i+1] / (x[i+1] - x[j+1])
    }
  }
  w
}

# --- Evaluacion baricentrica O(n) por punto ---
barycentric_eval <- function(x, y, w, xq) {
  sapply(xq, function(xx) {
    # Coincidencia exacta con un nodo
    idx <- which(xx == x)
    if (length(idx) > 0) return(y[idx[1]])
    terms <- w / (xx - x)
    sum(terms * y) / sum(terms)
  })
}

# --- Ejemplo de uso ---
# Datos: f(x)=x^2 en nodos 0,1,2
x <- c(0,1,2); y <- x^2

# Evaluar en una malla
xx <- seq(0,2,length.out=21)
yy_lagr <- lagrange_eval(x,y,xx)

w  <- barycentric_weights(x)
yy_bar <- barycentric_eval(x,y,w,xx)

# yy_lagr y yy_bar deben coincidir (salvo error redondeo) con xx^2
# plot(xx, yy_lagr, type="l"); points(x,y)
\end{verbatim}


\subsubsection*{Pseudoc\'odigo}
\begin{verbatim}
ALGORITMO NewtonDiffDiv(x[0..n], y[0..n]):
    para i := 0..n hacer
        F[i,0] := y[i]
    fin para
    para j := 1..n hacer
        para i := 0..n-j hacer
            F[i,j] := (F[i+1,j-1] - F[i,j-1]) / (x[i+j] - x[i])
        fin para
    fin para
    retornar F[0,0..n] // Coeficientes f[x0], f[x0,x1], ...
FIN

ALGORITMO NewtonEval(x[0..n], F[0,0..n], xq):
    p := F[0,0]
    prod := 1
    para j := 1..n hacer
        prod := prod * (xq - x[j-1])
        p := p + F[0,j]*prod
    fin para
    retornar p
FIN
\end{verbatim}

\subsubsection*{Implementaci\'on en R}
\begin{verbatim}
# --- Tabla de diferencias divididas ---
newton_diffdiv <- function(x, y) {
  n <- length(x)
  F <- matrix(0, n, n)
  F[,1] <- y
  for (j in 2:n) {
    for (i in 1:(n-j+1)) {
      F[i,j] <- (F[i+1,j-1] - F[i,j-1]) / (x[i+j-1] - x[i])
    }
  }
  F
}

# --- Evaluacion del polinomio de Newton ---
newton_eval <- function(x, F, xq) {
  n <- length(x)
  sapply(xq, function(xx) {
    p <- F[1,1]
    prod <- 1
    for (j in 2:n) {
      prod <- prod * (xx - x[j-1])
      p <- p + F[1,j]*prod
    }
    p
  })
}

# --- Ejemplo ---
x <- c(0,1,2,3)
y <- c(1,0,-1,2)
F <- newton_diffdiv(x,y)
xx <- seq(0,3,length.out=41)
yy <- newton_eval(x,F,xx)

# plot(xx,yy,type="l",col="blue"); points(x,y,col="red",pch=19)
\end{verbatim}

\begin{verbatim}

ALGORITMO TablaDiferenciasProgresivas(x[0..n], y[0..n]):
    // Requiere nodos equiespaciados: x[i] = x[0] + i*h
    VerificarEquiespaciado(x)
    T := matriz (n+1) x (n+1) llena de 0
    para i := 0..n hacer
        T[i,0] := y[i]
    fin para
    para j := 1..n hacer
        para i := 0..n-j hacer
            T[i,j] := T[i+1,j-1] - T[i,j-1]   // 
        fin para
    fin para
    retornar T
FIN



ALGORITMO NewtonAdelanteEval(x[0..n], T, xq):
    // T es la tabla de diferencias progresivas 
    h := x[1] - x[0]
    p := (xq - x[0]) / h
    P := T[0,0]
    prod := 1
    para k := 1..n hacer
        prod := prod * (p - (k-1))          // p(p-1)(p-2)...
        P := P + (prod / k!) * T[0,k]       // usa factorial de k
    fin para
    retornar P
FIN



ALGORITMO TablaDiferenciasRegresivas(x[0..n], y[0..n]):
    VerificarEquiespaciado(x)
    T := matriz (n+1) x (n+1) llena de 0
    para i := 0..n hacer
        T[i,0] := y[i]
    fin para
    para j := 1..n hacer
        para i := n..j hacer
            T[i,j] := T[i,j-1] - T[i-1,j-1]  //
        fin para
    fin para
    retornar T
FIN


ALGORITMO NewtonAtrasEval(x[0..n], T, xq):
    h := x[1] - x[0]
    q := (xq - x[n]) / h
    P := T[n,0]
    prod := 1
    para k := 1..n hacer
        prod := prod * (q + (k-1))          // q(q+1)(q+2)...
        P := P + (prod / k!) * T[n,k]
    fin para
    retornar P
FIN


# =========================================================
# Utilidades
# =========================================================

is_equispaced <- function(x, tol = 1e-9) {
  if (length(x) < 2) return(TRUE)
  h <- diff(x)
  max(abs(h - h[1])) < tol
}

falling_prod <- function(p, k) {
  # p(p-1)(p-2)...(p-k+1), falling factorial
  if (k == 0) return(1)
  prod(p - 0:(k-1))
}

rising_prod <- function(q, k) {
  # q(q+1)(q+2)...(q+k-1), rising factorial
  if (k == 0) return(1)
  prod(q + 0:(k-1))
}

# =========================================================
# 1) Tabla de diferencias progresivas (Newton hacia adelante)
# =========================================================
forward_diff_table <- function(x, y, check_equispaced = TRUE) {
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  if (check_equispaced && !is_equispaced(x)) {
    stop("Los nodos no son equiespaciados.")
  }
  T <- matrix(0, nrow = n, ncol = n)
  T[, 1] <- y
  if (n >= 2) {
    for (j in 2:n) {
      for (i in 1:(n - j + 1)) {
        T[i, j] <- T[i + 1, j - 1] - T[i, j - 1]  # Delta^j y_i
      }
    }
  }
  T
}

# =========================================================
# 2) Evaluacion Newton hacia adelante
#    P(x) = y0 + p y0 + p(p-1)/2! ^2 y0 + ...
# =========================================================
newton_forward_eval <- function(x, T, xq) {
  n <- length(x)
  h <- x[2] - x[1]
  p <- (xq - x[1]) / h
  # T[1, k] contiene ^(k-1) y_0, con k empezando en 1
  P <- T[1, 1]
  for (k in 2:n) {
    coef <- falling_prod(p, k - 1) / factorial(k - 1)
    P <- P + coef * T[1, k]
  }
  P
}

# =========================================================
# 3) Tabla de diferencias regresivas (Newton hacia atras)
# =========================================================
backward_diff_table <- function(x, y, check_equispaced = TRUE) {
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  if (check_equispaced && !is_equispaced(x)) {
    stop("Los nodos no son equiespaciados.")
  }
  T <- matrix(0, nrow = n, ncol = n)
  T[, 1] <- y
  if (n >= 2) {
    for (j in 2:n) {
      for (i in n:j) {
        T[i, j] <- T[i, j - 1] - T[i - 1, j - 1]  # Nabla^j y_i
      }
    }
  }
  T
}

# =========================================================
# 4) Evaluacion Newton hacia atras
#    P(x) = y_n + q y_n + q(q+1)/2! ^2 y_n + ...
# =========================================================
newton_backward_eval <- function(x, T, xq) {
  n <- length(x)
  h <- x[2] - x[1]
  q <- (xq - x[n]) / h
  P <- T[n, 1]
  for (k in 2:n) {
    coef <- rising_prod(q, k - 1) / factorial(k - 1)
    P <- P + coef * T[n, k]
  }
  P
}

# =========================================================
# 5) Helpers para tabla "bonita" (opcional)
# =========================================================
format_diff_table <- function(x, T, type = c("forward", "backward")) {
  type <- match.arg(type)
  n <- length(x)
  df <- data.frame(x = x, y = T[, 1])
  colnames(df) <- c("x", "y")
  for (j in 2:n) {
    colname <- if (type == "forward") paste0("Delta^", j - 1)
               else paste0("Nabla^", j - 1)
    df[[colname]] <- T[, j]
  }
  df
}

# =========================================================
# 6) Ejemplos de uso
# =========================================================

## Ejemplo A: f(x) = x^2, x = 0,1,2,3; evaluar en 0.5 (adelante)
xA <- 0:3
yA <- xA^2
TA <- forward_diff_table(xA, yA)           # tabla 
PA <- newton_forward_eval(xA, TA, 0.5)     # ~ 0.25
dfA <- format_diff_table(xA, TA, "forward")
# print(dfA); cat("P(0.5) =", PA, "\n")

## Ejemplo B: f(x) = x^3, x = 1,2,3,4; evaluar en 1.5 (adelante)
xB <- 1:4
yB <- xB^3
TB <- forward_diff_table(xB, yB)
PB <- newton_forward_eval(xB, TB, 1.5)     # ~ 3.375
dfB <- format_diff_table(xB, TB, "forward")

## Ejemplo C: f(x) = x^2, x = 0,1,2,3; evaluar en 2.6 (atras)
xC <- 0:3
yC <- xC^2
TC <- backward_diff_table(xC, yC)
PC <- newton_backward_eval(xC, TC, 2.6)    # ~ 6.76
dfC <- format_diff_table(xC, TC, "backward")

## Ejemplo D: f(x) = x^3, x = 1,2,3,4; evaluar en 3.2 (atras)
xD <- 1:4
yD <- xD^3
TD <- backward_diff_table(xD, yD)
PD <- newton_backward_eval(xD, TD, 3.2)    # ~ 32.768
dfD <- format_diff_table(xD, TD, "backward")


\end{verbatim}
