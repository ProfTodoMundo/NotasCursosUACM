%===========================================
\documentclass[12pt]{article}
%===========================================
\usepackage[utf8]{inputenc}
%\usepackage[margin=2.5in]{geometry}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{hyperref}
\usepackage[spanish]{babel}
\decimalpoint
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{graphicx,graphics}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{color}
\usepackage{float} 
\usepackage{subfig}
\usepackage[figuresright]{rotating}
\usepackage{enumerate}
\usepackage{anysize} 
\usepackage{url}
\usepackage{imakeidx}
\usepackage[left=0.5in, right=0.5in, top=1in, bottom=1in]{geometry}
% Opcional: para incluir gráficos con control de tamaño
\usepackage{float}

\hyphenation{mo-de-ra-da-men-te}
\addto\captionsspanish{\renewcommand{\figurename}{Figura}}

%===========================================
% Ajustes de Sweave
%\usepackage{Sweave}
%===========================================
\title{Notas sobre Métodos Numéricos con R}
\author{
Carlos E. Martínez-Rodríguez \\
Universidad Autónoma de la Ciudad de México \\
Academia de Matemáticas \\
\texttt{carlos.martinez@uacm.edu.mx}
}
\date{Agosto 2025}
\date{}
%===========================================
\newtheorem{Criterio}{Criterio}%[section]
\newtheorem{Sup}{Supuesto}[section]
\newtheorem{Note}{Nota}[section]
\newtheorem{Ejem}{Ejemplo}[section]
\newtheorem{Ejer}{Ejercicio}[section]
\newtheorem{Prop}{Proposici\'on}[section]
\newtheorem{Def}{Definici\'on}[section]
\newtheorem{Teo}{Teorema}[section]
\newtheorem{Algthm}{Algoritmo}[section]
\newtheorem{Sol}{Soluci\'on}[section]
\newtheorem{Ses}{Sesi\'on}[section]
\newtheorem{Result}{Resultado}[section]
%===========================================
\begin{document}
\maketitle
\tableofcontents
\newpage

%===========================================
\section{Sobre el curso}
%===========================================

El curso se llevará a cabo tres veces a la semana,  con sesiones de 1.5 horas,  de las cuales una de ellas se realizará en el laboratorio de cómputo 3.

%---------------------------------------------------
\subsection{Requisitos para acreditar la materia}
%---------------------------------------------------

El curso por su naturaleza implica que la/el estudiante implemente los distintos algoritmos que se revisan en el curso,  por lo tanto es importante que demuestre que efectivamente puede implementar, revisar y mejorar los algoritmos ya existentes.\bigskip

Hay dos maneras de certificar la materia: a) Portafolio y b) Examen de certificación.  Al menos una semana antes de que termine el curso las y los estudiantes tendrán conocimiento de sus calificaciones parciales y por tanto de la calificación promedio obtenida al momento, para que sea el/la mismo(a) estudiante quién decida si certifica por la modalidad de portafolio,  o por la modalidad de examen de certificación.\bigskip


El portafolio se conforma de evaluaciones ($40\%$), programas, ($40\%$) tareas ($10\%$), tareitas ($5\%$)y trabajos adicionales ($5\%$). Mientras que la certificación es un examen elaborado por el comité de certificación y que será presentado por todas y todos los estudiantes que se inscriban en esta modalidad en las fechas establecidas por la Coordinación de Certificación y Registro.  En cualquiera de las dos modalidades es indispensable que el/la estudiante se registre a este proceso para que su calificación pueda ser asignada al final del proceso de Certificación.

%---------------------------------------------------
\subsection{De la naturaleza del curso}
%---------------------------------------------------

El curso tiene un fuerte sustento en la programación constante, sin embargo, es importante resaltar que los conceptos teóricos deben ser dominados por las y los estudiantes, por lo tanto las evaluaciones y las tareas tendrán estas dos componentes principales.  Al contrario de lo que pueda pensarse la asistencia al curso es obligatoria pero no influye directamente en la calificación obtenida.  Sin embargo,  hay que mencionar que si la asistencia se realiza de manera intermitente es probable que cueste un poco de trabajo reincorporarse a la dinámica de trabajo que se irá construyendo con el grupo con el transcurso de las clases. 

%===========================================
\section{Introducción}
%===========================================

En este curso estudiaremos los elementos básicos de los métodos numéricos, utilizando el programa de distribución libre \textit{R}. Antes de iniciar propiamente con el estudio de los métodos numéricos realizaremos un breve repaso de algunos conceptos de álgebra lineal, cálculo diferencial mismos que son fundamentales en esta materia y de la que se supone las y los estudiantes se encuentran familiarizados con ellos.\medskip

\textit{An\'alisis Num\'erico} es una rama de las matem\'aticas que, mediante el uso de algoritmos iterativos, obtiene soluciones num\'ericas a problemas en los cuales la matem\'atica simb\'olica (o anal\'itica) resulta poco eficiente o no puede ofrecer un resultado. En particular, a estos algoritmos se les denomina \textit{m\'etodos num\'ericos}.Por lo general los m\'etodos num\'ericos se componen de un n\'umero de pasos finitos que se ejecutan de manera l\'ogica, mejorando aproximaciones iniciales a cierta cantidad, tal como la ra\'iz de una ecuaci\'on, hasta que se cumple con cierta cota de error. A esta operaci\'on c\'iclica de mejora del valor se le conoce como \textit{iteraci\'on}. El an\'alisis num\'erico es una alternativa muy eficiente para la resoluci\'on de ecuaciones, tanto algebraicas (polinomios) como trascendentes teniendo una ventaja muy importante respecto a otro tipo de m\'etodos: La repetici\'on de instrucciones l\'ogicas (iteraciones), proceso que permite mejorar los valores inicialmente considerados como soluci\'on. Dado que se trata siempre de la misma operaci\'on l\'ogica, resulta muy pertinente el uso de recursos de c\'omputo para realizar esta tarea. Sin embargo, debe haber claridad en el sentido de que el an\'alisis num\'erico no es la panacea en la soluci\'on de problemas matem\'aticos; los m\'etodos num\'ericos arrojan \textit{aproximaciones}, es decir, est\'an sujetos a un error. Esto quiere decir que si se puede ser tan preciso como los recursos de c\'alculo lo permitan, siempre est\'a presente y debe considerarse su manejo en el desarrollo de las soluciones requeridas. El uso de diversos sistemas de c\'omputo determina qu\'e soluciones anal\'itico–num\'ericas son viables en la pr\'actica, lo que implica que se deben tomar en cuenta el proceso iterativo, el costo de los recursos f\'isicos que se emplean en el an\'alisis, y el tipo de pr\'actica de la Ingenier\'ia. 

%===========================================
\section{Revisión de temas importantes}
%===========================================

%-------------------------------------------
\subsection{Cálculo}
%-------------------------------------------

Comenzamos el capítulo con un repaso de algunos aspectos importantes del cálculo que son necesarios a lo largo del texto. Suponemos que los estudiantes que lean este texto conocen la terminología, la notación y los resultados que se dan en un curso típico de cálculo.

%...............................................................
\subsubsection{Límites y continuidad}
%...............................................................

El límite de una función nos dice qué tan cerca se encuentran las imágenes de una función si dos elementos de su dominio se encuentran lo suficientemente cerca, es decir, decimos que una función $f(x)$ definida en el intervalo $(a,b)$ tiene \textbf{límite} $L$ en el punto $x = x_0$, lo que denotamos por $$\lim_{x \to x_0} f(x) = L,$$ si para cualquier $\varepsilon > 0$, existe un número real $\delta > 0$ tal que $|f(x) - L| < \varepsilon$ siempre que $0 < |x - x_0| < \delta $. Es decir, que los valores de la función estarán cerca de $L$ siempre que $x$ esté suficientemente cerca de $x_0$.
\bigskip

Se dice que una función $f$ es continua en $a$ si cuando $x$ se aproxima al valor de $a$, entonces también $f(x)$ se aproxima a $f(a)$, es decir, $f(x)$ es \textbf{continua} en el punto $x = a $ si $$\lim_{x \to a} f(x) = f(a),$$
y se dice que $f$ es \textbf{continua en el conjunto} $(a,b)$ si es continua en cada uno de los puntos del intervalo. Denotaremos el conjunto de todas las funciones $f$ que son continuas en $E=(a,b)$ por $C(E)$. \bigskip

Se dice que una sucesión $\{x_n\}_{n=1}^\infty$ \textbf{converge} a un número $x$, si
$\lim_{n \to \infty} x_n = x$ o bien, $x_n \to x$ cuando $n \to \infty$, si para cualquier $\varepsilon > 0$, existe un número natural $N(\varepsilon)$ tal que $|x_n - x| < \varepsilon$ para cada $n > N(\varepsilon)$. Cuando una sucesión tiene límite, se dice que la \textbf{sucesión converge}.

%........................................................
\subsubsection{Continuidad y convergencia de sucesiones}
%........................................................

Si $f(x)$ es una función definida en un conjunto $S$ de números reales y $x_0 \in S$, entonces las siguientes afirmaciones son equivalentes:
\begin{enumerate}
\item $f(x) $ es continua en $x = x_0 $,
\item Si $\{x_n\}_{n=1}^\infty $ es cualquier sucesión en $S $ que converge a $x_0 $, entonces $\lim_{n \to \infty} f(x_n) = f(x_0).$
\end{enumerate}

\begin{Teo}[Teorema del valor intermedio o de Bolzano.]
Si $f \in C[a, b] $ y $\ell $ es un número cualquiera entre $f(a) $ y $f(b) $, entonces existe al menos un número $c \in (a, b) $ tal que $f(c) = \ell $. Véase la Figura~\ref{fig:bolzano}.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Fig1.png}
\caption{Teorema del valor intermedio o de Bolzano}
\label{fig:bolzano}
\end{figure}
\end{Teo}


\begin{Note}
Todas las funciones con las que se van a trabajar en este curso de métodos numéricos serán continuas, ya que esto es lo mínimo que debemos exigir para asegurar que la conducta de un método se puede predecir.
\end{Note}

%........................................................
\subsubsection{Derivabilidad}
%........................................................

Si $f(x) $ es una función definida en un intervalo abierto que contiene un punto $x_0 $, entonces se dice que $f(x) $ es \textbf{derivable} en $x = x_0 $ cuando existe el límite
$$f'(x_0) = \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0}.$$

El número $f'(x_0)$ se llama \textbf{derivada} de $f$ en $x_0$ y coincide con la pendiente de la recta tangente a la gráfica de $f$ en el punto $(x_0, f(x_0))$, Figura~\ref{fig:derivada}.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Fig2.png}
\caption{Derivada de una función en un punto}
\label{fig:derivada}
\end{figure}

\textbf{Derivabilidad implica continuidad.} Si la función $f(x)$ es derivable en $x = x_0$, entonces $f(x)$ es continua en $x = x_0$. El conjunto de todas las funciones que admiten $n$ derivadas continuas en $S $ se denota por $C^n(S)$, mientras que el conjunto de todas las funciones indefinidamente derivables en $S$ se denota por $C^\infty(S)$. Las funciones polinómicas, racionales, trigonométricas, exponenciales y logarítmicas están en $C^\infty(S)$, siendo $S$ el conjunto de puntos en los que están definidas.

\begin{Teo}[Teorema del valor medio o de Lagrange.]
Si $f\in C[a, b]$ y es derivable en $(a,b)$, entonces existe un punto $c\in (a,b)$ tal que
$$f'(c) = \frac{f(b) - f(a)}{b - a}.$$
\end{Teo}

Geométricamente hablando, Figura~\ref{fig:lagrange}, el teorema del valor medio dice que hay al menos un número $c \in (a, b) $ tal que la pendiente de la recta tangente a la curva $y = f(x) $ en el punto $(c, f(c)) $ es igual a la pendiente de la recta secante que pasa por los puntos $(a, f(a)) $ y $(b, f(b)) $.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Fig3.png}
\caption{Teorema del valor medio o de Lagrange}
\label{fig:lagrange}
\end{figure}

\begin{Teo}[Teorema de los valores extremos.]  
Si $f \in C[a,b] $, entonces existen $c_1 $ y $c_2 $ en $(a,b) $ tales que $f(c_1) \leq f(x) \leq f(c_2) $ para todo $x \in [a,b] $. Si además, $f $ es derivable en $(a,b) $, entonces los puntos $c_1 $ y $c_2 $ están en los extremos de $[a,b] $ o bien son puntos críticos.
\end{Teo}

%........................................................
\subsubsection{Integración}
%........................................................


\begin{Teo}[Primer teorema fundamental o regla de Barrow.]
  Si $f \in C[a, b] $ y $F $ es una primitiva cualquiera de $f $ en $[a, b] $ (es decir, $F'(x) = f(x) $), entonces $$\int_a^b f(x) \, dx = F(b) - F(a). $$
\end{Teo}

\begin{Teo}[Segundo teorema fundamental.]
  Si $f \in C[a, b] $ y $x \in (a, b) $, entonces $$\frac{d}{dx} \int_a^x f(t) \, dt = f(x).$$
\end{Teo}

\begin{Teo}[Teorema del valor medio para integrales.]
  Si $f \in C[a, b] $, $g $ es integrable en $[a, b] $ y $g(x) $ no cambia de signo en $[a, b] $, entonces existe un punto $c \in (a, b) $ tal que  $$\int_a^b f(x)g(x) \, dx = f(c) \int_a^b g(x) \, dx.$$
\end{Teo}

\begin{Note}
Cuando $g(x) = 1 $, véase la Figura~\ref{fig:valormedio-integral}, este resultado es el habitual teorema del valor medio para integrales y proporciona el valor medio de la función $f $ en el intervalo $[a, b] $, que está dado por $$f(c) = \frac{1}{b-a} \int_a^b f(x) \, dx.$$

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Fig4.png}
\caption{Teorema del valor medio para integrales}
\label{fig:valormedio-integral}
\end{figure}
\end{Note}

%...............................................................
\subsubsection{Polinomios de Taylor}
%...............................................................

\begin{Teo}[Teorema de Taylor.]
Supongamos que $f \in C^{(n)}[a,b] $ y que $f^{(n+1)} $ existe en $[a,b] $. Sea $x_0 $ un punto en $[a,b] $. Entonces, para cada $x $ en $[a,b] $, existe un punto $\xi(x) $ entre $x_0 $ y $x $ tal que
$$f(x) = P_n(x) + R_n(x),$$
donde
\begin{eqnarray}
P_n(x) &=& f(x_0) + f'(x_0)h + \frac{f''(x_0)}{2!}h^2 + \cdots + \frac{f^{(n)}(x_0)}{n!}h^n = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} h^k,\\
R_n(x)&=& \frac{f^{(n+1)}(\xi(x))}{(n+1)!} h^{n+1}, \quad \text{y} \quad h = x - x_0.
\end{eqnarray}
\end{Teo}


El polinomio $P_n(x) $ se llama \textbf{n-ésimo polinomio de Taylor} de $f $ alrededor de $x_0 $ (véase la Figura~\ref{fig:taylor}).   $R_n(x) $ se llama \textbf{error de truncamiento} (o \textit{resto de Taylor}) asociado a $P_n(x) $.   Como el punto $\xi(x) $ en el error de truncamiento $R_n(x) $ depende del punto $x $ en el que se evalúa el polinomio $P_n(x) $, podemos verlo como una función de la variable $x $.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{Fig5.png}
\caption{Gráficas de $y = f(x) $ y de su polinomio de Taylor $y = P_2(x) $ alrededor de $x_0 $.}
\label{fig:taylor}
\end{figure}

\begin{Note}
La serie infinita que resulta al tomar límite en la expresión de $P_n(x) $ cuando $n \to \infty $ se llama \textbf{serie de Taylor} de $f $ alrededor de $x_0 $. Cuando $x_0 = 0 $, el polinomio de Taylor se suele denominar \textbf{polinomio de Maclaurin}, y la serie de Taylor se llama \textbf{serie de Maclaurin}.\bigskip

La denominación \textit{error de truncamiento} en el teorema de Taylor se refiere al error que se comete al usar una suma truncada al aproximar la suma de una serie infinita.
\end{Note}

%...............................................................
\subsubsection{Teoremas adicionales}
%...............................................................

A continuación enunciaremos algunas de las definiciones y teoremas básicos que utilizaremos a lo largo de estas notas.

\begin{Def}$f$ es de clase $C^1$ en el intervalo $[a;b]$ si $f'$ es continua en $[a;b]$.
\end{Def}

\begin{Def}$f$ es de clase $C^n$ en el intervalo $[a;b]$ si $f^{(n)}$ es continua en $[a;b]$.
\end{Def}

\begin{Def}$f$ es de clase $C^\infty$ en el intervalo $I$ si $f$ es infinitas veces derivable y continua en $I$.
\end{Def}

\begin{Teo} (Teorema de los valores intermedios). Sea $f$ continua en el intervalo $[a;b]$. Si $k \in \mathbb{R}$ es un número comprendido entre $f(a)$ y $f(b)$, entonces existe al menos un punto $\xi$ perteneciente al intervalo $(a;b)$ tal que $f(\xi) = k$.
\end{Teo}

\begin{Teo}[Bolzano]Si $f$ es continua en el intervalo $[a;b]$ y $f(a) \cdot f(b) < 0$, entonces existe un $\xi$ perteneciente al $(a;b)$ tal que $f(\xi) = 0$.
\end{Teo}

\begin{Teo}[Teorema de acotabilidad] Si $f : [a;b] \mapsto \mathbb{R}$ es continua en $[a;b]$, entonces $f$ está acotada en $[a;b]$.
\end{Teo}

\begin{Teo}[Teorema de Weierstrass] Si $f : [a;b] \mapsto \mathbb{R}$ es continua en $[a;b]$, entonces $f$ tiene un máximo global y un mínimo global en $[a;b]$.
\end{Teo}

\begin{Teo}[Teorema Generalizado de Rolle] Si $f$ continua en $[a;b]$, y existen las derivadas $f'(x), f''(x), \ldots, f^{(n)}(x)$ en $(a;b)$ y $f(x_0) = f(x_1) = \cdots = f(x_n) = 0$ (con $x_0, x_1, \ldots, x_n \in [a;b]$) entonces existe $\xi$ perteneciente al $(a;b)$ tal que $f^{(n)}(\xi) = 0$.
\end{Teo}

\begin{Teo}[Teorema de Lagrange] Si $f$ continua en $[a;b]$ y derivable en $(a;b)$ entonces existe $\xi$ perteneciente al $(a;b)$ tal que $f(b) - f(a) = f'(\xi)(b - a)$.
\end{Teo}

\begin{Teo}[Teorema del valor medio ponderado] Sea $f$ continua en $[a,b]$ y $g$ una función integrable Riemann en $[a,b]$. Si $g$ no cambia de signo en $[a,b]$, entonces existe un número $c \in (a,b)$ tal que:
$$\int_a^b f(x)g(x)dx = f(c) \int_a^b g(x)dx$$
\end{Teo}

%-------------------------------------------
\subsection{Álgebra Lineal}
%-------------------------------------------

%...............................................................
\subsubsection{Matrices}
%...............................................................

Una \textbf{matriz} es un arreglo multidimensional de escalares, llamados \textit{elementos}, ordenados en filas y columnas.  Una matriz de $ m $ filas y $ n $ columnas, o \textit{matriz (de orden) $ m \times n $}, es un conjunto de $ m \cdot n $ elementos $ a_{ij} $, con $ i = 1, 2, \ldots, m $ y $ j = 1, 2, \ldots, n $, que se representa de la siguiente forma:
$$A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}$$

Se puede abreviar la representación de la matriz anterior de la forma $ A = (a_{ij}) $ con $ i = 1, 2, \ldots, m $; $ j = 1, 2, \ldots, n $.

Hay una relación directa entre matrices y vectores puesto que podemos pensar una matriz como una composición de vectores fila o de vectores columna. Además, un vector es un caso especial de matriz: un \textit{vector fila} es una matriz con una sola fila y varias columnas, y un \textit{vector columna} es una matriz con varias filas y una sola columna. 

%...............................................................
\subsubsection{Operaciones con matrices}
%...............................................................

\begin{itemize}
\item Si $ A = (a_{ij}) $ y $ B = (b_{ij}) $ son dos matrices que tienen el mismo orden, $ m \times n $, decimos que $ A $ y $ B $ son \textbf{iguales} si $ a_{ij} = b_{ij} $ para todo $ i = 1, \ldots, m $ y $ j = 1, \ldots, n $.

\item Si $ A = (a_{ij}) $ y $ B = (b_{ij}) $ son dos matrices que tienen el mismo orden $ m \times n $, la \textbf{suma} de $ A $ y $ B $ es una matriz $ C = (c_{ij}) $ del mismo orden con $ c_{ij} = a_{ij} + b_{ij} $ para todo $ i = 1, \ldots, m $ y $ j = 1, \ldots, n $.

\item Si $ A = (a_{ij}) $ es una matriz de orden $ m \times n $, la \textbf{multiplicación de $ A $ por un escalar} $ \lambda $ es una matriz $ C = (c_{ij}) $ del mismo orden $ m \times n $ con $ c_{ij} = \lambda a_{ij} $ para todo $ i = 1, \ldots, m $ y $ j = 1, \ldots, n $.

\item Si $ A = (a_{ij}) $ es una matriz de orden $ m \times n $, la \textbf{matriz traspuesta} de $ A $ es la matriz que resulta de intercambiar sus filas por sus columnas, se denota por $ A^T $ y es de orden $ n \times m $.

\item Si $ A = (a_{ij}) $ es una matriz de orden $ m \times p $ y $ B = (b_{ij}) $ es una matriz de orden $ p \times n $, el \textbf{producto de $ A $ por $ B $} es una matriz $ C = (c_{ij}) $ de orden $ m \times n $ con $$c_{ij} = \sum_{k=1}^p a_{ik}b_{kj}, \quad \text{para todo } i = 1, \ldots, m \text{ y } j = 1, \ldots, n.$$ Obsérvese que el producto de dos matrices solo está definido si el número de columnas de la primera matriz coincide con el número de filas de la segunda.
\end{itemize}

%...............................................................
\subsubsection{Matrices especiales}
%...............................................................
\begin{itemize}

\item Una matriz $ A = (a_{ij}) $ es \textbf{cuadrada} si tiene el mismo número de filas que de columnas, y de orden $ n $ si tiene $ n $ filas y $ n $ columnas. Se llama \textbf{diagonal principal} al conjunto de elementos $ a_{11}, a_{22}, \ldots, a_{nn} $.

\item Una \textbf{matriz diagonal} es una matriz cuadrada que tiene algún elemento distinto de cero en la diagonal principal y ceros en el resto de elementos.

\item Una matriz cuadrada con ceros en todos los elementos por encima (debajo) de la diagonal principal se llama \textbf{matriz triangular inferior (superior)}.

\item Una matriz diagonal con unos en la diagonal principal se denomina \textbf{matriz identidad} y se denota por $I$. Es la única matriz cuadrada tal que $ AI = IA = A $ para cualquier matriz cuadrada $A$.

\item Una \textbf{matriz simétrica} es una matriz cuadrada $A$ tal que $A=A^T$.

\item La \textbf{matriz cero} es una matriz con todos sus elementos iguales a cero.

\item Decimos que una matriz cuadrada $A$ es \textbf{invertible} (o \textit{regular} o \textit{no singular}) si existe una matriz cuadrada $ B $ tal que $AB = BA = I$. Se dice entonces que $ B $ es la \textbf{matriz inversa} de $A$ y se denota por $A^{-1}$. (Una matriz que no es invertible se dice \textit{singular}.)

\item Si una matriz $ A $ es invertible, su inversa también lo es y $A^{-1})^{-1} = A$.

\item Si $A$ y $B$ son dos matrices invertibles, su producto también lo es y $(AB)^{-1} = B^{-1}A^{-1}$.
\end{itemize}


%...............................................................
\subsubsection{Determinante de una matriz}
%...............................................................

El \textbf{determinante} de una matriz solo está definido para matrices cuadradas y su valor es un escalar.  El determinante de una matriz $ A $ cuadrada de orden $ n $ se denota por $ |A| $ o $ \det(A) $, y se define como
$$\det(A) = \sum_{j} (-1)^{i+j} a_{ij} \cdot \det(A_{ij}),$$
donde la suma se toma para todas las $ n! $ permutaciones de grado $n$ y $s$ es el número de intercambios necesarios para poner el segundo subíndice en el orden $1, 2, \ldots, n$.

\textbf{Algunas propiedades de los determinantes son:}
\begin{itemize}
\item $\det(A^T) = \det(A)$
\item $\det(AB) = \det(A)\det(B)$
\item $\det(A^{-1}) = \frac{1}{\det(A)}$
\item Si dos filas o dos columnas de una matriz coinciden, el determinante de esta matriz es cero.
\item Cuando se intercambian dos filas o dos columnas de una matriz, su determinante cambia de signo.
\item El determinante de una matriz diagonal es el producto de los elementos de la diagonal.
\item Si denotamos por $A_{ij}$ la matriz de orden $ (n-1) $ que se obtiene de eliminar la fila $i$ y la columna $j$ de la matriz $A$, llamamos \textbf{menor complementario} asociado al elemento $a_{ij}$ de la matriz $A$ al $\det(A_{ij})$.

\item Se llama \textbf{$k$-ésimo menor principal} de la matriz $A$ al determinante de la submatriz principal de orden $k$.

\item Definimos el \textbf{cofactor} del elemento $a_{ij}$ de la matriz $A$ por $\Delta_{ij} = (-1)^{i+j} \det(A_{ij})$.

\item Si $A$ es una matriz invertible de orden $ n $, entonces $$A^{-1} = \frac{1}{\det(A)} C,$$ donde $ C $ es la matriz de elementos $\Delta_{ij}$ para todo $i,j = 1,2,\ldots,n$. Obsérvese entonces que una matriz cuadrada es invertible si y s\'olo si su determinante es distinto de cero.
\end{itemize}

%...............................................................
\subsubsection{Valores propios y vectores propios}
%...............................................................

\begin{itemize}
\item Si $A$ es una matriz cuadrada de orden $n$, un número $\lambda$ es un \textbf{valor propio} de $A$ si existe un vector no nulo $v$ tal que $Av = \lambda v$. Al vector $v$ se le llama \textbf{vector propio} asociado al valor propio $\lambda$.

\item El valor propio $\lambda$ es solución de la \textbf{ecuación característica} $$\det(A - \lambda I) = 0,$$ donde $\det(A - \lambda I)$ se llama \textbf{polinomio característico}.  Este polinomio es de grado $n$ en $\lambda$  y tiene $n$ valores propios (no necesariamente distintos).
\end{itemize}

%...............................................................
\subsubsection{Normas vectoriales y normas matriciales}
%...............................................................
Para medir la \textbf{longitud} de los vectores y el \textbf{tamaño} de las matrices se suele utilizar el concepto de \textbf{norma}, que es una función que toma valores reales. Un ejemplo simple en el espacio euclidiano tridimensional es un vector $v = (v_1, v_2, v_3)$, donde $v_1, v_2$ y $v_3$ son las distancias a lo largo de los ejes $x, y, z$ respectivamente.  \bigskip

La \textbf{longitud del vector} $v$ (es decir, la distancia del punto $(0,0,0)$ al punto $(v_1, v_2, v_3)$) se calcula como $$\|v\| = \sqrt{v_1^2 + v_2^2 + v_3^2},$$
donde la notación $\|v\|$ indica que esta longitud se refiere a la \textit{norma euclidiana} del vector $v$.   De forma similar, para un vector $v$ de dimensión $n$, $v = (v_1, v_2, \ldots, v_n)$, la norma euclidiana se calcula como $$\|v\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.$$ Este concepto puede extenderse a una matriz $ m \times n $, $ A = (a_{ij}) $, de la siguiente manera: $$\|A\|_F = \sqrt{ \sum_{i=1}^m \sum_{j=1}^n a_{ij}^2 },$$
que recibe el nombre de \textbf{norma de Frobenius}.

Hay otras alternativas a las normas euclidiana y de Frobenius. Dos normas usuales son la \textbf{norma 1} y la \textbf{norma infinito}:

\begin{itemize}
\item La \textbf{norma 1} de un vector $ v = (v_1, v_2, \ldots, v_n) $ se define como $ \|v\|_1 = \sum_{i=1}^n |v_i| $.  De forma similar, la norma 1 de una matriz $ m \times n $, $ A = (a_{ij}) $, se define como $$\|A\|_1 = \max_{1 \leq j \leq n} \sum_{i=1}^m |a_{ij}|.$$

\item La \textbf{norma infinito} de un vector $ v = (v_1, v_2, \ldots, v_n) $ se define como $ \|v\|_\infty = \max_i |v_i| $.   La norma infinito de una matriz $ m \times n $, $ A = (a_{ij}) $, se define como  $$\|A\|_\infty = \max_{1 \leq i \leq m} \sum_{j=1}^n |a_{ij}|.$$

\item Todas las normas son equivalentes en un espacio vectorial de dimensión finita. 

\end{itemize}

%===========================================
\section{Operaciones de punto flotante}
%===========================================

%-------------------------------------------
\subsection{Sistemas decimal y binario}
%-------------------------------------------

El sistema numérico que se utiliza frecuentemente es el sistema decimal, en la que la base de expresión es el $10$. Sin embargo las computadoras utilizan el sistema binario, sistema de base 2, es decir solamente $\left\{0,1\right\}$.

\begin{Prop}
Para cualquier número natural $N$, existen $a_0,a_1,a_2,\ldots,a_K$, con $a_i\in \mathbb{R}$ tales que
\begin{equation}\label{Ec.Expansion.Binaria}
N=a_{K}\times2^{K}+a_{K-1}\times2^{K-1}+a_{K-2}\times2^{K-2}+\cdots+a_{1}\times2+a_{0}\times2^0
\end{equation}

Para ver lo anterior lo que tenemos que hacer es calcula $\frac{N}{2}$, es decir, $\frac{N}{2}=P_{0}+\frac{a_{0}}{2}$, donde $P_{0}=a_{K}\times2^{K-1}+a_{K-1}\times2^{K-2}+a_{K-2}\times2^{K-3}+\cdots+a_{1}\times2^{0}$, es decir $a_{0}$ es el resto de dividir $N$ entre $2$. Ahora hagamos lo mismo para $P_{0}$: $$\frac{P_{0}}{2}=a_{K}\times2^{K-2}+a_{K-1}\times2^{K-3}+a_{K-2}\times2^{K-4}+\cdots+a_{2}\times2^{0}+\frac{a_{1}}{2},$$ por lo tanto $$\frac{P_{0}}{2}=P_{1}+\frac{a_{1}}{2},$$ donde $$P_{1}=a_{K}\times2^{K-2}+a_{K-1}\times2^{K-3}+a_{K-2}\times2^{K-4}+\cdots+a_{2}\times2^{0},$$ es decir $P_1$ es el resto de dividir $P_0$ entre $2$. Siguiendo este procedimiento de manera análoga hasta que encontremos un valor $K$ tal que $P_K=0$. Por lo tanto tenemos el siguiente algoritmo:
\begin{Algthm}
Para un valor $N$ natural, los términos $a_{k}$ en la ecuación \ref{Ec.Expansion.Binaria} se encuentran
\begin{eqnarray}
\begin{array}{l}
N=2P_{0}+a_{0},\\
P_{0}=2P_{1}+a_{1},\\
\vdots\\
P_{K-2}=2P_{K-1}+a_{K-1},\\
P_{K-1}=2P_{K}+a_{K},\\
P_{K}=0.
\end{array}
\end{eqnarray}
\end{Algthm}
\end{Prop}

\begin{Ejem}
Convertir $24563$ 
\begin{eqnarray*}
24563 &=& 12281 \times 2 + 1, \quad a_{0} = 1\\
12281 &=& 6140 \times 2 + 1, \quad a_{1} = 1\\
6140 &=& 3070 \times 2 + 0, \quad a_{2} = 0\\
3070 &=& 1535 \times 2 + 0, \quad a_{3} = 0\\
1535 &=& 767 \times 2 + 1, \quad a_{4} = 1\\
767 &=& 383 \times 2 + 1, \quad a_{5} = 1\\
383 &=& 191 \times 2 + 1, \quad a_{6} = 1\\
191 &=& 95 \times 2 + 1, \quad a_{7} = 1\\
95 &=& 47 \times 2 + 1, \quad a_{8} = 1\\
47 &=& 23 \times 2 + 1, \quad a_{9} = 1\\
23 &=& 11 \times 2 + 1, \quad a_{10} = 1\\
11 &=& 5 \times 2 + 1, \quad a_{11} = 1\\
5 &=& 2 \times 2 + 1, \quad a_{12} = 1\\
2 &=& 1 \times 2 + 0, \quad a_{13} = 0\\
1 &=& 0 \times 2 + 1, \quad a_{14} = 1
\end{eqnarray*}
Por lo tanto el número binario es: $(24563)_{10}=(101111111110011)_{2}$.

\end{Ejem}


\begin{Prop}
Sea $Q\in\mathbb{R}$, tal que $0<Q<1$, entonces existen términos $b_{1},b_{2},\ldots,b_{k}$ tales que $Q=0.b_{1}b_{2}b_{3}\cdots b_{k}$, y por tanto

\begin{eqnarray}
Q=b_{1}\times2^{-1}+b_{2}\times2^{-2}+b_{3}\times2^{-3}+\cdots+b_{k}\times2^{-k}+\cdots
\end{eqnarray}

Si multiplicamos $Q$ por $2$, se tiene que
$$2Q=b_{1}+b_{2}\times2^{-1}+b_{3}\times2^{-2}+b_{4}\times2^{-3}+\cdots+b_{k}\times2^{-k+1}+\cdots$$

Si $F_{1}=frac(2Q)$, con $frac(x)$ la parte fraccionaria de $x$, y $b_{1}=[[2Q]]$, donde $[[x]]$ es la parte entera de $x$, entonces $$F_1=b_{2}\times2^{-1}+b_{3}\times2^{-2}+b_{4}\times2^{-3}+\cdots+b_{k}\times2^{-k+1}+\cdots,$$ de donde $$2F_1=b_{2}\times2^{0}+b_{3}\times2^{-1}+b_{4}\times2^{-2}+\cdots+b_{k}\times2^{-k+2}+\cdots=b_{2}+F_{2},$$ donde $F_{2}=frac(2F_{1})$, y $b_{2}=[[2F_1]]$. Procediendo de manera análoga para el resto de los términos se tienen las suceciones $\left\{b_{k}\right\}$ y $\left\{F_{k}\right\}$, dadas por $b_{k}=[[2F_{k-1}]]$ y $F_{k}=frac(2F_{k-1})$, con $b_{1}=[[2Q]]$ y $F_{1}=frac(2Q)$. Por lo tanto se tiene la representación binaria de Q dada por

\begin{equation}
Q=\sum_{i=1}^{\infty}b_{i}2^{-i}
\end{equation}
\end{Prop}


\begin{Ejem}
Convertir el número $3.5786$. Sea $Q = 0.5786$, entonces
\begin{eqnarray*}
2Q &=& 1.1572, b_{1} = [[1.1572]]= 1, F_{1} = frac(1.1572) = 0.1572\\
2F_{1}& =& 0.3144, b_{2} = [[0.3144 ]]= 0,F_{2} = frac(0.3144) = 0.3144\\
2F_{2} &=& 0.6288, b_{3} = [[0.6288 ]]= 0, F_{3} = frac(0.6288) = 0.6288\\
2F_{3} &=& 1.2576, b_{4} = [[1.2576 ]]= 1 F_{4} = frac(1.2576) = 0.2576\\
2F_{4} &=& 0.5152, b_{5} = [[0.5152 ]]= 0, F_{5} = frac(0.5152) = 0.5152\\
2F_{5} &=& 1.0304, b_{6} = [[1.0304 ]] = 1 F_{6} = frac(1.0304) = 0.0304\\
2F_{6} &=& 0.0608, b_{7} = [[0.0608]]= 0, F_{7} = frac(0.0608) = 0.0608\\
2F_{7} &=& 0.1216, b_{8} = [[0.1216]] = 0, F_{8} = frac(0.1216) = 0.1216
\end{eqnarray*}

De lo anterior se tiene que:
$$0.5786 = (0.10010100\ldots)_2$$
Por lo tanto:$$3.5786 = (11.10010100\ldots)_2.$$


\end{Ejem}

\begin{Ejer}
Convertir los siguientes números de base 10 a base 2.
\begin{enumerate}
\item $324$
\item $27$
\item $1423$
\item $235.25$
\item $41.596$
\end{enumerate}
\end{Ejer}

%-------------------------------------------
\subsection{Números en punto flotante}
%-------------------------------------------

\begin{Def}
Los n\'umeros en punto flotante son n\'umero reales de la forma 
\begin{equation}
\pm\alpha \times \beta^{e},
\end{equation}

donde $\alpha$ tiene un n\'umero de d\'igitos limitados, $\beta$ es la base y $e$ es el exponente que modifica la posici\'on del punto decimal.
\end{Def}

\begin{Def}
Un n\'umero real $x$ tiene una representaci\'on punto flotante normalizada si 
\begin{equation}
\pm\alpha \times \beta^{e},
\end{equation}
con $\frac{1}{\beta}<|\alpha|<1$
\end{Def}

\begin{Note}
En este caso $x$ se puede escribir en la forma 
\begin{equation}
x=\pm0,d_{1}d_{2}\cdots d_{k}\times\beta^{e},
\end{equation} 

donde si $x\neq0$,$d_{1}\neq0$, y adem\'as $0\leq d_{i}<\beta$, para $i=1,2,3\dots,k$ y $L\leq e\leq U$.
\end{Note}

\begin{Def}
El conjunto de los n\'umeros en punto flotante se le llama \textbf{conjunto de n\'umeros m\'aquina}.
\end{Def}

\begin{Note}
El conjunto de n\'umeros m\'aquina es finito. Para ver esto consideremos que si $x$ es de la forma 
\begin{equation}\label{Eq.Punto.Flotate}
\pm0,d_{1}d_{2}\cdots d_{t}\times\beta^{e},
\end{equation}
dado que $d_{1}\neq0$ y $0\leq d_{i}<\beta$ entonces $d_1$ puede tomar $\beta-1$ valores distintos, mientras que para $d_{i}$ hay $\beta$ posibilidades. Por lo tanto se tienen $\left(\beta-1\right)\beta\beta\cdots\beta=\left(\beta-1\right)\beta^t$. El n\'umero de exponentes posibles son $U-L+1$, en total hay $\left(\beta-1\right)\beta^t\left(U-L+1\right)$ n\'umeros m\'aquina positivos, por lo tanto, considerando positivos y negativos hay $2\left(\beta-1\right)\beta^t\left(U-L+1\right)$  n\'umeros m\'aquina, considerando que el creo tambi\'es es un n\'umero de m\'aquina hay en realidad $2\left(\beta-1\right)\beta^t\left(U-L+1\right)+1$. Es decir, cualquier n\'umero real puede ser representado por uno de los $2\left(\beta-1\right)\beta^t\left(U-L+1\right)+1$ n\'umeros de m\'aquina.
\end{Note}


\begin{Ejem}
Recordemos la expresi\'on (\ref{Eq.Punto.Flotate}), consideremos $\beta=2$, $t=3$, $L=-2$ y $U=2$. 
Entonces $x=\pm d_{1}d_{2}d_{3}\times\left(2\right)^{e}$, con $-2\leq e\leq 2$ y $0\leq d_{1},d_{2}<2$. Por lo tanto se tiene que $d_{1},d_{2},d_{3}=1$; $e=\left\{-2,-1,0,1,2\right\}$.  Entonces $d_{1}d_{2}d_{3}=\left\{100,101,110,111\right\}=\left\{\frac{1}{2},\frac{5}{8},\frac{3}{4},\frac{7}{8}\right\}$
\begin{eqnarray}
\begin{array}{|c|c|c|c|c|}\hline
-2&-1&0&1&2\\\hline
0.111\times 2^{-2}&0.111\times 2^{-1}&0.111\times 2^{0}&0.111\times 2^{1}&0.111\times 2^{2}\\\hline
0.110\times 2^{-2}&0.110\times 2^{-1}&0.110\times 2^{0}&0.110\times 2^{1}&0.110\times 2^{2}\\\hline
0.101\times 2^{-2}&0.101\times 2^{-1}&0.101\times 2^{0}&0.101\times 2^{1}&0.101\times 2^{2}\\\hline
0.100\times 2^{-2}&0.100\times 2^{-1}&0.100\times 2^{0}&0.100\times 2^{1}&0.100\times 2^{2}\\\hline
\end{array}
\end{eqnarray}
sustituyendo y resolviendo las operaciones

\begin{eqnarray}
\begin{array}{|c|c|c|c|c|}\hline
-2&-1&0&1&2\\\hline
\frac{7}{8}\times 2^{-2}=\frac{7}{32}&\frac{7}{8}\times 2^{-1}=\frac{7}{16}&\frac{7}{8}\times 2^{0}=\frac{7}{8}&\frac{7}{8}\times 2^{1}=\frac{7}{4}&\frac{7}{8}\times 2^{2}=\frac{7}{2}\\\hline\hline
\frac{3}{4}\times 2^{-2}=\frac{3}{16}&\frac{3}{4}\times 2^{-1}=\frac{3}{8}&\frac{3}{4}\times 2^{0}=\frac{3}{4}&\frac{3}{4}\times 2^{1}=\frac{3}{2}&\frac{3}{4}\times 2^{2}=3\\\hline\hline
\frac{5}{8}\times 2^{-2}=\frac{5}{32}&\frac{5}{8}\times 2^{-1}=\frac{5}{16}&\frac{5}{8}\times 2^{0}=\frac{5}{8}&\frac{5}{8}\times 2^{1}=\frac{5}{4}&\frac{5}{8}\times 2^{2}=\frac{5}{2}\\\hline\hline
\frac{1}{2}\times 2^{-2}=\frac{1}{8}&\frac{1}{2}\times 2^{-1}=\frac{1}{4}&\frac{1}{2}\times 2^{0}=\frac{1}{2}&\frac{1}{2}\times 2^{1}=1&\frac{1}{2}\times 2^{2}=2\\\hline\hline
\end{array}
\end{eqnarray}
Por tanto el n\'umero total de n\'umeros m\'aquina son 41.
\end{Ejem}

\begin{Ejer}
\begin{enumerate}
\item Describir todos los n\'umeros m\'aquina para $\beta=2$, $t=4$, $L=-3$ y $U=3$. 
\item Escribir el n\'umero $732.5051$ en notaci\'on de punto flotante. Respuesta $0.7325051\times10^{-3}$
\item Escribir el n\'umero $0.006521$ en notaci\'on de punto flotante.  Respuesta $0.06521\times10^{-2}$
\item $\left(101.01\right)_2$. Respuesta $0.10101\times2^{3}$
\item $\left(0.00101111\right)_2$. Respuesta $0.101111\times2^{-2}$
\end{enumerate}
\end{Ejer}

\begin{Note}
$\left(0.1\right)_2=1\times2^{-1}=0.5$
\end{Note}


\subsection{Representaci\'on}

En una computadora los n\'umeros se expresan como se ha descrito, pero con restricciones sobre $q$ y $m$ dadas por la longitud de la palabra. Supongamos que la longitud de la palabra es de $32$ bits, los cuales se distribuyen de la siguiente manera: los dos primeros se reservan para los signos: es $0$ si es positivo y $1$ si es negativo; los siguientes $7$ espacios para el exponente, y los restantes para la mantisa. Considerando que cualquier n\'umero puede normalizarse, recordar $x=\pm q\times 2^{m}$, con $\frac{1}{2}\leq q<1$, se puede asumir que el primer bit en $q$ es $1$ y por tanto no se requeire almacenar,.

\begin{Ejem}
Representar y almacenar en punto flotante normalizado  el n\'umero $-0.125$.  A saber $(-0.125)=(-0.001)_{2}=(-0.1)_2\times2^{-2}$,  adem\'as $2=(10)_{2}$; por lo tanto su representaci\'on es: $1,1|,$ $0,0,0,0,0,1,0|$ $0,0,0,0,0,0,0,\dots0$.
\end{Ejem}

\begin{Ejem}
Represente y almacene el n\'umero $117.125$. Respuesta $117=(1110101)_2$ y $0.125=(0.001)_2$, por tanto $117.125=(1110101.001)_2=(0.1110101001)_2\times2^{7}$ y $y=(111)_{2}$, por tanto se almacena: $0,0||0000111||110101\dots0$
\end{Ejem}


\begin{Note}
$|m|$ no requiere m\'as de $7$ bits, es decir $|m|\leq(1111111)_{2}=2^{7}-1=127$, por tanto el exponente de $7$ d\'igitos binarios proporciona un intervalo de $0$ a $127$, para n\'umeros peque\~ nos se toma el exponente en el intervalo $[-63,64]$. Adem\'as $q$ requiere de a lo m\'as 24 bits, por tanto la m\'aquina de $32$ bits tienen una precisi\'on limitada entre $7$ y $8$ d\'igitos decimales, ya que el bit menos significativo en la mantisa representa unidades del orden $2^{-24}\approx10^{-7}$. Esto quiere decir que n\'umeros expresados por m\'as de siete d\'igitos decimales ser\'an aproximados cuando se dan como datos de entrada o como resultados de operaciones.
\end{Note}

\subsection{Errores}

A la hora de realizar un c\'alculo es importante asegurarse de que los n\'umeros que intervienen en el c\'alculo se pueden utilizar con confianza.   Para ello, se introduce el concepto de \textbf{cifras significativas}, que designa formalmente la notaci\'on de un valor num\'erico, y se usa en aquellos que gu\'ian visualmente la precisi\'on.\medskip

Los \textbf{errores de truncamiento} se producen cuando utilizamos una aproximaci\'on en lugar de un procedimiento matem\'atico exacto. Para conocer las caracter\'isticas de estos errores se suelen considerar los polinomios de Taylor.   Cuando se aproxima un proceso continuo por uno discreto, para errores provocados por un tama\~no de paso finito $h$, resulta a menudo \'util describir la dependencia del error $e$ con $ h $ cuando $h$ tiende a cero.\medskip

Decimos que una funci\'on $ f(h) $ es una $ \mathcal{O} $ grande de $ h^n $ si $ |f(h)| \leq c h^n $ para alguna constante $ c $, cuando $ h $ es cercano a cero; se escribe \begin{equation} f(h) = \mathcal{O}(h^n)\end{equation}.

Si un m\'etodo tiene un t\'ermino de error que es $\mathcal{O}(h^n)$, se suele decir que es un \textbf{m\'etodo de orden $n$}.  Por ejemplo, si utilizamos un polinomio de Taylor para aproximar la funci\'on $ g $ en $ x = a + h $, tenemos:v$$g(x) = g(a + h) = g(a) + h g'(a) + \frac{h^2}{2!} g''(a) + \frac{h^3}{3!} g^{(3)}(\xi), \quad \text{para alg\'un } \xi \in [a, a+h].$$

Suponiendo que $ g $ es suficientemente derivable, la aproximaci\'on anterior es $ \mathcal{O}(h^3) $, puesto que el error $$\frac{h^3}{3!} g^{(3)}(\xi), \quad \text{satisface} \quad \left| \frac{h^3}{3!} g^{(3)}(\xi) \right| \leq \frac{M}{3!} |h^3|,$$
donde $ M $ es el m\'aximo de $ g^{(3)}(x) $ para $ x \in [a, a+h] $.

Las diferencias (errores) son m\'ultiples y de diversa naturaleza, aunque pueden separarse en dos grupos gen\'ericos:

\begin{itemize}
    \item \textbf{Los errores que provienen del modelado te\'orico} (o abstracci\'on matem\'atica) del fen\'omeno real; estos errores se denominan \textit{Errores del modelo o inherentes}. Los errores inherentes son producto de factores intr\'insecos a la naturaleza, al ambiente y las personas mismas. Los errores inherentes son imposibles de remediar aunque pueden minimizarse; en consecuencia, no pueden cuantificarse.

    \begin{quote}
    Se distinguen dos tipos de errores inherentes: \textbf{Las incertidumbres} hacen referencia a las dimensiones f\'isicas que nunca podr\'an ser medidas en forma exacta debido a la naturaleza de la materia y a las imperfecciones de los instrumentos de medici\'on. \textbf{Las verdaderas equivocaciones} son las situaciones que se producen en la lectura de instrumentos de medici\'on o en el traslado de informaci\'on y que son inadvertidas a las personas; un claro ejemplo de estas situaciones es la denominada \textit{ceguera de taller}.
    \end{quote}

    \item \textbf{Los errores del m\'etodo} son producto de la limitante en la representaci\'on y manipulaci\'on de cantidades num\'ericas utilizadas en los c\'alculos necesarios en el desarrollo del modelo matem\'atico. Es de destacar que los dispositivos de c\'alculo (tales como calculadoras y computadoras) utilizan y manipulan cantidades en forma imprecisa.
\end{itemize}
    Existen dos grandes tipos de errores del m\'etodo: \textit{El truncamiento} se provoca ante la imposibilidad de manipular, por parte de un instrumento de c\'omputo, una cantidad infinita de t\'erminos o cifras. Los t\'erminos o cifras omitidas (que son infinitas en n\'umero) introducen un error en los resultados calculados. \textit{El redondeo} se produce por el mismo motivo que el truncamiento pero, a diferencia de \'este, las cifras omitidas s\'i son consideradas en la cifra resultante

  
En general, si se incrementa el n\'umero de cifras significativas en el ordenador, se minimizan los errores de redondeo, y los errores de truncamiento disminuyen a medida que los errores de redondeo se incrementan.  Por lo tanto, para disminuir uno de los dos sumandos del error total debemos incrementar el otro.  Como el error total no se puede calcular en la mayor\'ia de los casos, se suelen utilizar otras medidas para estimar la exactitud de un m\'etodo num\'erico, que suelen depender del m\'etodo espec\'ifico.  En algunos m\'etodos el error num\'erico se puede acotar, mientras que en otros se determina una estimaci\'on del \textit{orden de magnitud del error}. Cuando se buscanlas soluciones num\'ericas de un problema real los resultados que se obtienen por lo general no son exactos.  

\subsection{Cuantificaci\'on de errores}

Los errores se cuantifican de dos formas diferentes:

\begin{enumerate}
  \item \textbf{Error absoluto}. El error absoluto es la diferencia absoluta entre un valor real y un aproximado. Est\'a dado por la siguiente f\'ormula:

  $$E = \left| V_{real} - V_{aprox} \right| $$

  El error absoluto recibe este nombre ya que posee las mismas dimensiones que la variable bajo estudio.

  \item \textbf{Error relativo}. Corresponde a la expresi\'on en porcentaje de un error absoluto; en consecuencia, este error es adimensional.

  $$  e = \left| \frac{V_{real} - V_{aprox}}{V_{real}} \right| \cdot 100\%$$
\end{enumerate}

La diferencia entre la preferencia en el uso de los dos tipos de error consiste precisamente en la presencia de las dimensiones f\'isicas. Debido a las unidades de medici\'on utilizadas, el manejo y la percepci\'on del error absoluto suele ser enga\~noso o dif\'icil de comprender r\'apidamente. Sin embargo, el manejo de porcentajes (o valores relativos) resulta m\'as natural y sencillo de comprender. Sin embargo, el uso de estos dos tipos de errores est\'a sujeto siempre al objetivo de las actividades desarrolladas.

Las expresiones que definen a los errores absoluto y relativo requieren del conocimiento de la variable $V_{real}$ que representa un valor ideal que no posee error alguno. Como podr\'a suponerse, en la realidad resulta imposible determinar este valor. Una pr\'actica com\'un en los an\'alisis elementales sobre errores es considerar como un valor real a los resultados arrojados por la medici\'on experimental de los fen\'omenos y a los valores aproximados como los proporcionados por los modelos matem\'aticos (o viceversa).  En realidad, ambos valores son valores aproximados.  Para lograr un resultado coherente, en la pr\'actica debe sustituirse al valor real por un valor que se considere posee un error menor. 

En el caso del an\'alisis num\'erico, dado que los resultados se obtienen a partir de procesos iterativos que se mejoran los inicialmente obtenidos, debe partirse del supuesto que el \'ultimo valor obtenido posee un nivel menor de error que el valor previo. Dado lo anterior, los errores absoluto y relativo se calcular\'an de la siguiente forma:

\textbf{Error absoluto:}
$$E = \left| V_i - V_{i-1} \right|$$

\textbf{Error relativo:}
$$e = \left| \frac{V_i - V_{i-1}}{V_i} \right| \cdot 100\%$$

En ambas ecuaciones, $V_i$ es el valor de la \'ultima iteraci\'on y $V_{i-1}$ es el valor de la iteraci\'on anterior $i – 1$.

\begin{itemize}
    \item \textbf{Error absoluto:} Se define como la diferencia entre el valor real (experimental o exacto) y el valor aproximado (te\'orico o calculado):

    $$
    E_a = |x_{\text{real}} - x_{\text{aproximado}}|
    $$

    \item \textbf{Error relativo:} Es la relaci\'on entre el error absoluto y el valor real:

    $$
    E_r = \frac{E_a}{|x_{\text{real}}|}
    $$

    \item \textbf{Error porcentual:} Es el error relativo expresado en porcentaje:

    $$
    E_p = E_r \cdot 100 = \frac{E_a}{|x_{\text{real}}|} \cdot 100
    $$
\end{itemize}

\subsection{Errores en punto flotante}

Un n\'umero real $ x \in \mathbb{R} $, cuando no pertenece a $ F $, se representa mediante una aproximaci\'on flotante $ fl(x) \in F $, de modo que:

$$x = fl(x)(1 + \varepsilon), \quad |\varepsilon| \leq \epsilon_{\text{mach}}.$$

Este $ \varepsilon $ se llama \textbf{error relativo} y $ \epsilon_{\text{mach}} $ es la \textbf{precisi\'on de la m\'aquina}.  En general se tiene que:

$$\epsilon_{\text{mach}} = \frac{1}{2} \beta^{1 - t}.$$

\begin{Note}
Los ordenadores almacenan los n\'umeros reales en forma binaria (base 2).   Cada d\'igito binario (0 \'o 1) se llama \textbf{bit} (por digital binary).   La memoria de los ordenadores est\'a organizada en \textbf{bytes}, siendo un byte = 8 bits.   En el est\'andar IEEE, los ordenadores representan n\'umeros reales en precisi\'on simple (32 bits) y en precisi\'on doble (64 bits).  Este est\'andar tambi\'en define los c\'odigos para representar valores especiales como NaN (Not a Number), infinitos, y ceros con signo. Para representar un n\'umero real, se utiliza la forma normalizada:
$$x = (-1)^s \cdot (1 + f) \cdot 2^e,$$
donde: $ s $: es el bit de signo (0 para positivo, 1 para negativo),  $ f $: es la fracci\'on,  $ e $: es el exponente con sesgo.\medskip

Por ejemplo, el n\'umero 0.15625 en binario es 0.00101, que se escribe como $ 1.01 \cdot 2^{-3} $ y se almacena como: signo = 0, exponente = $124$ (con sesgo de $127$), y mantisa = $010000...$ En este formato, la precisi\'on depende de la cantidad de bits reservados a la fracci\'on $ f $.   En doble precisi\'on (64 bits), se reservan 52 bits para la fracci\'on, 11 para el exponente y 1 para el signo.
\end{Note}


\subsection{Aproximaci\'on num\'erica y errores}

Una \textit{aproximaci\'on} es un valor cercano a uno considerado como real o verdadero. Esta cercan\'ia, o diferencia, se conoce como \textit{error}. Normalmente, la consideraci\'on de la validez de una aproximaci\'on depende de la cota de error que el experimentador considere pertinente en funci\'on del contexto del fen\'omeno bajo estudio. Esto implica que tambi\'en debe considerarse que una magnitud debe ser un valor real, que en el \'ambito de la Ingenier\'ia pocas veces se conoce, lo que obliga a adoptar convenciones. En Ingenier\'ia, se denomina \textit{exactitud} a la capacidad de un instrumento de medir un valor cercano al de la magnitud real. Exactitud implica precisi\'on, pero no al contrario. Exactitud y precisi\'on no son equivalentes. Exactitud es capacidad para acercarse a la magnitud real, y precisi\'on es la capacidad de generar resultados similares. La precisi\'on se logra cuando un instrumento para repetir mediciones exactas cuando \'estas se realizan consecutivamente. De acuerdo con la definici\'on de aproximaci\'on num\'erica, la exactitud se aplica en los m\'etodos num\'ericos en cuanto a la capacidad del m\'etodo de generar un resultado muy cercano al valor real; se percibe la cercan\'ia entre la exactitud y el concepto de error. Por otra parte, los m\'etodos num\'ericos a trav\'es de iteraciones generan valores aproximados cada vez m\'as exactos, es decir, estas iteraciones deber\'an ser precisas. Dado lo anterior, los m\'etodos num\'ericos deber\'an tener como cualidades la exactitud y la precisi\'on. Matem\'aticamente, la \textbf{convergencia} es la propiedad de algunas sucesiones y series de tender progresivamente a un l\'imite, de tal forma, si este l\'imite existe, se dice que la sucesi\'on o la serie \textit{converge}.  En forma an\'aloga, si un m\'etodo num\'erico en su funcionamiento iterativo nos proporciona aproximaciones cada vez m\'as cercanas al valor buscado, se dice que el m\'etodo converge. La convergencia se mide a trav\'es de los errores; si el error entre dos aproximaciones sucesivas se reduce, el m\'etodo converge; se debe cumplir que:

$$|x_n - x_{n-1}| \leq |x_{n-1} - x_{n-2}|$$

Es decir, la diferencia en\'esima $ (x_n - x_{n-1}) $ debe ser menor que la diferencia $ (n-1)$\'esima $ x_{n-1} - x_{n-2} $. Se dice que un sistema (o un proceso) es \textbf{estable} si a peque\~nas variaciones en la entrada o en la excitaci\'on corresponden peque\~nas variaciones en la salida o en la respuesta. La estabilidad de un m\'etodo num\'erico tiene que ver con la manera en que los errores num\'ericos se propagan a lo largo del algoritmo.  Cuando un m\'etodo converge, lo m\'as deseable es que en los resultados que se obtengan, los niveles de error se disminuyan en la forma m\'as r\'apida posible. Sin embargo, ocurre que durante la operaci\'on del algoritmo, ya sea por el manejo de los datos num\'ericos o bien por la naturaleza propia del modelo matem\'atico con el que se est\'e trabajando, los errores entre aproximaciones no disminuyan en forma progresiva, sino que incluso aumenten en alguna etapa del proceso para despu\'es reducirse mostrando un comportamiento aleatorio.

La \textbf{robustez} de un m\'etodo num\'erico radica en su convergencia y su estabilidad. Pueden utilizarse m\'etodos cuya prueba de convergencia indique la pertinencia de su uso, pero que durante su aplicaci\'on se obtengan resultados inestables que repercutan en el n\'umero de iteraciones y en consecuencia en el tiempo invertido en la soluci\'on. El ideal lo constituyen m\'etodos que a la vez de ser convergentes resulten estables.

\begin{Note}
\textbf{Convergencia} de un m\'etodo se refiere a que sea posible la obtenci\'on del valor buscado cuando el n\'umero de pasos tiende a infinito.  T\'ipicamente, la convergencia se analiza en m\'etodos iterativos, es decir, aquellos en los que el resultado final se obtiene tras una repetici\'on de c\'alculos.  Cuando repetimos estas iteraciones, los datos iniciales producen valoraciones progresivas del resultado.
\end{Note}




%-------------------------------------------
\subsection{Ejercicios}
%-------------------------------------------
\begin{enumerate}
\item Realizar una revisión de la historia de los m\'etodos num\'ericos, elaborar un documento de hasta dos cuartillas.
\item Realiza las siguientes conversiones de base $10$ a base $2$:
\begin{enumerate}
\item 246
\item 345.68
\item 4586632.2846
\item 984365.27463
\item 79905523
\end{enumerate}
\item Elabora el c\'odigo en R para realizar la conversi\'on de base $10$ a base $2$.
\end{enumerate}


%===========================================
\section{Solución de Sistemas de Ecuaciones Lineales}
%===========================================

\subsubsection{Definiciones sobre matrices}

\begin{Def}[Matriz transpuesta]
Sea $A=(a_{ij})\in \mathbb{R}^{m\times n}$.  
La \textbf{matriz transpuesta} de $A$, denotada $A^\top$, es la matriz $n\times m$ definida por
\begin{eqnarray*}
(A^\top)_{ij}=a_{ji}.
\end{eqnarray*}
\end{Def}

\begin{Def}[Matriz simétrica]
Una matriz $A\in \mathbb{R}^{n\times n}$ es \textbf{simétrica} si
\begin{eqnarray*}
A^\top=A.
\end{eqnarray*}
\end{Def}

\begin{Def}[Matriz definida positiva]
Una matriz simétrica $A\in \mathbb{R}^{n\times n}$ es \textbf{definida positiva} si
\begin{eqnarray*}
x^\top A x > 0 \quad \text{para todo } x\in \mathbb{R}^n, \; x\neq 0.
\end{eqnarray*}
\end{Def}

\begin{Def}[Matriz semidefinida positiva]
Una matriz simétrica $A\in \mathbb{R}^{n\times n}$ es \textbf{semidefinida positiva} si
\begin{eqnarray*}
x^\top A x \geq 0 \quad \text{para todo } x\in \mathbb{R}^n.
\end{eqnarray*}
\end{Def}


Hasta ahora hemos visto métodos \textbf{directos} (eliminación Gaussiana, factorizaciones $LU$, Doolittle, Cholesky).  
En problemas de gran tamaño, estos métodos pueden ser muy costosos en tiempo y memoria.  
Por ello se usan \textbf{métodos iterativos}, que generan una sucesión de aproximaciones $\{x^{(k)}\}$ que converge a la solución exacta $x$ de
\begin{eqnarray*}
Ax=b.
\end{eqnarray*}


Un sistema de ecuaciones lineales puede escribirse como:
\begin{eqnarray*}
Ax=b, \qquad A=(a_{ij})\in\mathbb{R}^{n\times n},\quad x,b\in\mathbb{R}^n.
\end{eqnarray*}

Nuestro objetivo es encontrar $x$. La eliminación gaussiana consiste en aplicar operaciones de filas equivalentes que transforman $A$ en una matriz triangular superior $U$, de modo que el sistema resultante pueda resolverse por sustitución regresiva.

\subsection{Eliminación gaussiana simple}

\paragraph{Planteamiento.}
Sea $A=(a_{ij})\in\mathbb{R}^{n\times n}$ y $b=(b_i)\in\mathbb{R}^n$.  
El sistema $Ax=b$ se expresa como:
\begin{eqnarray*}
\sum_{j=1}^n a_{ij}x_j=b_i,\qquad i=1,\dots,n.
\end{eqnarray*}

\paragraph{Etapas de eliminación.}
En el paso $k$ se anulan las entradas de la columna $k$ por debajo del pivote $a_{kk}^{(k)}$.  
Para cada $i=k+1,\dots,n$:
\begin{eqnarray*}
m_{ik}=\frac{a_{ik}^{(k)}}{a_{kk}^{(k)}},\qquad
a_{ij}^{(k+1)}=a_{ij}^{(k)}-m_{ik}\,a_{kj}^{(k)},\quad
b_i^{(k+1)}=b_i^{(k)}-m_{ik}\,b_k^{(k)}.
\end{eqnarray*}

\paragraph{Ejemplo simbólico $3\times 3$.}
\begin{eqnarray*}
A=\begin{bmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}
\end{bmatrix},\quad
b=\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 1 ($k=1$):}  
\begin{eqnarray*}
m_{21}=\tfrac{a_{21}}{a_{11}},\qquad m_{31}=\tfrac{a_{31}}{a_{11}}.
\end{eqnarray*}
\begin{eqnarray*}
\begin{aligned}
a_{22}^{(2)}&=a_{22}-m_{21}a_{12}, & a_{23}^{(2)}&=a_{23}-m_{21}a_{13}, & b_2^{(2)}&=b_2-m_{21}b_1,\\
a_{32}^{(2)}&=a_{32}-m_{31}a_{12}, & a_{33}^{(2)}&=a_{33}-m_{31}a_{13}, & b_3^{(2)}&=b_3-m_{31}b_1.
\end{aligned}
\end{eqnarray*}

\emph{Paso 2 ($k=2$):}  
\begin{eqnarray*}
m_{32}=\frac{a_{32}^{(2)}}{a_{22}^{(2)}},\quad
a_{33}^{(3)}=a_{33}^{(2)}-m_{32}a_{23}^{(2)},\quad
b_3^{(3)}=b_3^{(2)}-m_{32}b_2^{(2)}.
\end{eqnarray*}

Resultado: sistema triangular superior $Ux=c$.  

En el paso $k$ de la eliminación:
\begin{eqnarray*}
a_{ij}^{(k+1)}=a_{ij}^{(k)}-m_{ik}a_{kj}^{(k)},\qquad
b_i^{(k+1)}=b_i^{(k)}-m_{ik}b_k^{(k)},
\end{eqnarray*}
para $i=k+1,\dots,n$ y $j=k,\dots,n$, donde
\begin{eqnarray*}
m_{ik}=\frac{a_{ik}^{(k)}}{a_{kk}^{(k)}}.
\end{eqnarray*}

Es decir:
\begin{itemize}
  \item $m_{ik}$ mide cuántas veces la fila $k$ debe restarse a la fila $i$ para anular $a_{ik}$.
  \item Cada $m_{ik}$ se almacena en la matriz $L$.
\end{itemize}

\paragraph{Idea general.}
Transformar el sistema $Ax=b$ en uno triangular superior $Ux=c$ mediante operaciones elementales de filas, y resolver luego por sustitución regresiva.

\paragraph{Ejemplo.}
Resolver el sistema
\begin{eqnarray*}
\begin{cases}
2x+y-z=8,\\
-3x-y+2z=-11,\\
-2x+y+2z=-3.
\end{cases}
\end{eqnarray*}
Forma matricial:
\begin{eqnarray*}
\begin{bmatrix}
2&1&-1\\
-3&-1&2\\
-2&1&2
\end{bmatrix}
\begin{bmatrix}x\\y\\z\end{bmatrix}
=
\begin{bmatrix}8\\-11\\-3\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 1.} Pivote en $a_{11}=2$.  
Eliminamos en la primera columna:
\begin{eqnarray*}
m_{21}=\tfrac{-3}{2},\quad m_{31}=\tfrac{-2}{2}=-1.
\end{eqnarray*}
Se obtiene
\begin{eqnarray*}
\begin{bmatrix}
2 & 1 & -1 & | & 8\\
0 & 0.5 & 0.5 & | & 1\\
0 & 2 & 1 & | & 5
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 2.} Pivote en $a_{22}=0.5$.  
Eliminamos debajo:
\begin{eqnarray*}
m_{32}=\tfrac{2}{0.5}=4.
\end{eqnarray*}
\begin{eqnarray*}
\begin{bmatrix}
2 & 1 & -1 & | & 8\\
0 & 0.5 & 0.5 & | & 1\\
0 & 0 & -1 & | & 1
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 3.} Sustitución regresiva:
\begin{eqnarray*}
z=-1,\quad 0.5y+0.5z=1 \Rightarrow y=3,\quad 2x+y-z=8 \Rightarrow x=2.
\end{eqnarray*}
\begin{eqnarray*}
\boxed{x=2,\;y=3,\;z=-1}.
\end{eqnarray*}


\subsection{Eliminación con pivoteo y escalamiento}

Mismo proceso, pero en cada paso $k$ se intercambia la fila $k$ con la fila $p$ tal que
\begin{eqnarray*}
|a_{pk}^{(k)}|=\max_{i\geq k}|a_{ik}^{(k)}|.
\end{eqnarray*}
Esto asegura que $a_{kk}^{(k)}$ sea lo suficientemente grande.  


Antes de seleccionar pivote, cada fila $i$ se escala por
\begin{eqnarray*}
s_i=\max_j |a_{ij}|,
\end{eqnarray*}
y se elige como pivote aquel que maximiza
\begin{eqnarray*}
\frac{|a_{ik}^{(k)}|}{s_i}.
\end{eqnarray*}

\paragraph{Idea.}
Si en algún paso el pivote es $0$ o muy pequeño, se intercambia la fila pivote con otra fila inferior que tenga el mayor valor absoluto en la misma columna. Esto aumenta la estabilidad.

\paragraph{Ejemplo.}
\begin{eqnarray*}
\begin{bmatrix}
0 & 2 & 1 & | & 4\\
1 & -1 & 2 & | & 6\\
2 & 1 & -1 & | & 1
\end{bmatrix}.
\end{eqnarray*}
\emph{Problema:} $a_{11}=0$.  
\emph{Solución:} intercambiamos fila 1 con fila 2.  

Nuevo sistema:
\begin{eqnarray*}
\begin{bmatrix}
1 & -1 & 2 & | & 6\\
0 & 2 & 1 & | & 4\\
2 & 1 & -1 & | & 1
\end{bmatrix}.
\end{eqnarray*}

Ahora se procede con eliminación gaussiana simple. El pivoteo evita la división por cero.

\subsubsection{Eliminación con Pivoteo y Escalamiento}
\paragraph{Idea.}
Si los coeficientes varían mucho en magnitud, usar sólo pivoteo puede ser insuficiente.  
\emph{Escalamiento}: dividir cada fila por su elemento de mayor valor absoluto antes de seleccionar pivote.  
Así, se compara \(|a_{ik}|/s_i\), donde $s_i=\max_j |a_{ij}|$ es el factor de escala de la fila $i$.

\paragraph{Comentario.}
Esto evita que números muy grandes o muy pequeños enmascaren el pivote real, aumentando la precisión numérica en cómputo.

\subsubsection{Pivoteo y escalamiento}
\begin{itemize}
  \item \textbf{Pivoteo parcial:} intercambiar la fila $k$ con la fila $p$ tal que 
  \(|a_{pk}^{(k)}|=\max_{i\geq k}|a_{ik}^{(k)}|\).
  \item \textbf{Escalamiento:} definir factores $s_i=\max_j |a_{ij}|$ y escoger como pivote el mayor cociente
  \begin{eqnarray*}
  \frac{|a_{ik}^{(k)}|}{s_i}.
  \end{eqnarray*}
  Esto controla la propagación de errores de redondeo.
\end{itemize}


\subsection{Gauss--Jordan y cálculo de inversas}
\paragraph{Idea.}
Extender la eliminación hasta reducir $A$ a la matriz identidad. Si se parte de la matriz aumentada $(A|I)$, el resultado final es $(I|A^{-1})$.

Si extendemos la eliminación a toda la matriz (no solo a triangular superior), obtenemos:
\begin{eqnarray*}
A \;\longrightarrow\; I, \quad (A|I)\;\longrightarrow\;(I|A^{-1}).
\end{eqnarray*}

Algebraicamente:
\begin{eqnarray*}
A^{-1}=M^{(n-1)^{-1}}\cdots M^{(2)^{-1}}M^{(1)^{-1}},
\end{eqnarray*}
donde cada $M^{(k)}$ es la matriz elemental usada en la etapa $k$ de la eliminación.

\subsubsection{Gauss--Jordan e inversas}
En vez de detener la eliminación en forma triangular superior, se continúa hasta obtener la matriz identidad:
\begin{eqnarray*}
(A|I)\;\longrightarrow\;(I|A^{-1}).
\end{eqnarray*}
Así, cada paso se formaliza como multiplicación por matrices elementales:
\begin{eqnarray*}
A^{-1}=M_1^{-1}M_2^{-1}\cdots M_r^{-1}.
\end{eqnarray*}
\paragraph{Ejemplo.}
Calcular $A^{-1}$ con
\begin{eqnarray*}
A=\begin{bmatrix}
2 & 1\\
5 & 3
\end{bmatrix}.
\end{eqnarray*}
Matriz aumentada:
\begin{eqnarray*}
\begin{bmatrix}
2 & 1 & | & 1 & 0\\
5 & 3 & | & 0 & 1
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 1.} Pivote $2$. Normalizar fila 1:
\begin{eqnarray*}
\begin{bmatrix}
1 & 0.5 & | & 0.5 & 0\\
5 & 3 & | & 0 & 1
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 2.} Eliminar columna 1 de fila 2:
\begin{eqnarray*}
R_2 \leftarrow R_2-5R_1.
\end{eqnarray*}
\begin{eqnarray*}
\begin{bmatrix}
1 & 0.5 & | & 0.5 & 0\\
0 & 0.5 & | & -2.5 & 1
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 3.} Normalizar fila 2:
\begin{eqnarray*}
\begin{bmatrix}
1 & 0.5 & | & 0.5 & 0\\
0 & 1 & | & -5 & 2
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 4.} Eliminar columna 2 de fila 1:
\begin{eqnarray*}
R_1 \leftarrow R_1-0.5R_2.
\end{eqnarray*}
\begin{eqnarray*}
\begin{bmatrix}
1 & 0 & | & 3 & -1\\
0 & 1 & | & -5 & 2
\end{bmatrix}.
\end{eqnarray*}

\paragraph{Resultado.}
\begin{eqnarray*}
A^{-1}=
\begin{bmatrix}
3 & -1\\
-5 & 2
\end{bmatrix}.
\end{eqnarray*}

\subsection{Factorizaci\'on  $LU$}

La \textbf{factorización $LU$} consiste en descomponer una matriz cuadrada 
\begin{eqnarray*}A \in \mathbb{R}^{n \times n}\end{eqnarray*}
en el producto de dos matrices:
\begin{eqnarray}A = LU,\end{eqnarray}
donde:
\begin{itemize}
    \item $L$ es una matriz \textbf{triangular inferior} con unos en la diagonal.
    \item $U$ es una matriz \textbf{triangular superior}.
\end{itemize}

\begin{Note}
En algunos casos es necesario introducir una matriz de permutación $P$ para realizar el proceso de eliminación gaussiana de manera estable:
\begin{eqnarray}
PA = LU.
\end{eqnarray}
\end{Note}

\begin{eqnarray}
L=\begin{bmatrix}
1 & 0 & \cdots & 0\\
m_{21} & 1 & \cdots & 0\\
\vdots & \ddots & \ddots & \vdots\\
m_{n1} & \cdots & m_{n,n-1} & 1
\end{bmatrix},\end{eqnarray}

\begin{eqnarray*}
U=\text{matriz triangular superior obtenida tras la eliminación}.
\end{eqnarray*}

Por tanto:
\begin{eqnarray*}A=LU.\end{eqnarray*}

El procedimiento se basa en la eliminación gaussiana. Dada
\begin{eqnarray}
A = (a_{ij}) \in \mathbb{R}^{n \times n},
\end{eqnarray}
se definen los \textbf{multiplicadores}:
\begin{eqnarray}
m_{ij} = \frac{a_{ij}^{(k)}}{a_{kk}^{(k)}} \quad \text{para } i > k,
\end{eqnarray}
donde $a_{ij}^{(k)}$ denota la entrada de la matriz en la etapa $k$ de la eliminación.

Los pasos son:
\begin{enumerate}
    \item Inicialmente $L = I_n$ y $U = A$.
    \item Para cada paso $k = 1,2,\dots,n-1$:
    \begin{enumerate}
        \item Calcular los multiplicadores
        \begin{eqnarray}l_{ik} = \frac{u_{ik}}{u_{kk}}, \quad i = k+1, \dots, n.\end{eqnarray}
        \item Restar $l_{ik}$ veces la fila $k$ a la fila $i$ de $U$.
        \item Almacenar cada $l_{ik}$ en la posición correspondiente de $L$.
    \end{enumerate}
\end{enumerate}

De este modo se obtiene:
\begin{eqnarray}
A = LU, \qquad
L = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
l_{21} & 1 & 0 & \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
l_{n1} & \cdots & l_{n,n-1} & 1
\end{bmatrix}, \qquad
U = \begin{bmatrix}
u_{11} & u_{12} & \cdots & u_{1n} \\
0 & u_{22} & \cdots & u_{2n} \\
\vdots & \ddots & \ddots & \vdots \\
0 & \cdots & 0 & u_{nn}
\end{bmatrix}.
\end{eqnarray}

\begin{Note}
\begin{itemize}
    \item La factorización $LU$ es el corazón de la eliminación gaussiana.
    \item Permite resolver sistemas lineales $Ax = b$ mediante:
    \begin{eqnarray*}Ax = b \quad \Rightarrow \quad LUx = b.\end{eqnarray*}
    \item Se resuelve en dos etapas:
    \begin{eqnarray*}Ly = b, \qquad Ux = y,\end{eqnarray*}
    usando sustitución progresiva y regresiva.
\end{itemize}
\end{Note}

\begin{Note}
En los métodos directos como $LU$ o Cholesky, al factorizar la matriz $A$ en dos bloques (p.ej.\ $A=LU$ o $A=LL^\top$), la resolución de $Ax=b$ se divide en dos etapas fundamentales:

\begin{Note}[Sustitución hacia adelante (forward substitution).]
Se aplica cuando se resuelve un sistema triangular inferior:
\begin{eqnarray}
Ly=b,\qquad L=(\ell_{ij})\ \text{triangular inferior con $\ell_{ii}\neq 0$}.
\end{eqnarray}
El proceso es:
\begin{eqnarray}
y_1=\frac{b_1}{\ell_{11}},\qquad
y_i=\frac{1}{\ell_{ii}}\left(b_i-\sum_{j=1}^{i-1}\ell_{ij}y_j\right),\quad i=2,\dots,n.
\end{eqnarray}
\end{Note}

\begin{Note}[Sustitución hacia atrás (backward substitution).]
Se aplica cuando se resuelve un sistema triangular superior:
\begin{eqnarray}
Ux=y,\qquad U=(u_{ij})\ \text{triangular superior con $u_{ii}\neq 0$}.
\end{eqnarray}
El proceso es:
\begin{eqnarray}
x_n=\frac{y_n}{u_{nn}},\qquad
x_i=\frac{1}{u_{ii}}\left(y_i-\sum_{j=i+1}^n u_{ij}x_j\right),\quad i=n-1,\dots,1.
\end{eqnarray}
\end{Note}

\begin{Ejem}
Sea
\begin{eqnarray}
U=\begin{bmatrix}
u_{11} & u_{12} & u_{13}\\
0 & u_{22} & u_{23}\\
0 & 0 & u_{33}
\end{bmatrix},\qquad
y=\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}.
\end{eqnarray}
La sustitución hacia atrás da:
\begin{eqnarray}
x_3=\tfrac{y_3}{u_{33}},\quad
x_2=\tfrac{y_2-u_{23}x_3}{u_{22}},\quad
x_1=\tfrac{y_1-u_{12}x_2-u_{13}x_3}{u_{11}}.
\end{eqnarray}
\end{Ejem}
\end{Note}

\subsubsection{Método con permutaciones}

Si en algún paso se cumple $u_{kk} = 0$, el algoritmo falla. Para evitarlo se aplica \textbf{pivoteo parcial}: se intercambia la fila $k$ con otra fila $p > k$ tal que
\begin{eqnarray}
|u_{pk}| = \max_{i \geq k} |u_{ik}|.
\end{eqnarray}
Esto se representa mediante una matriz de permutación $P$, resultando:
\begin{eqnarray*}
PA = LU.
\end{eqnarray*}



\begin{Ejem} $A=LU$]
Sea
\begin{eqnarray*}
A=\begin{bmatrix}
2 & 3 & 1\\
4 & 7 & 7\\
-2 & 4 & 5
\end{bmatrix}.
\end{eqnarray*}
Aplicamos eliminación gaussiana guardando los multiplicadores en $L$ (con $1$'s en la diagonal).
\begin{itemize}
\item Paso $k=1$.
Pivote $u_{11}=2$. Multiplicadores:
\begin{eqnarray*}
\ell_{21}=\frac{4}{2}=2,\qquad \ell_{31}=\frac{-2}{2}=-1.
\end{eqnarray*}
Actualizamos filas de $U$:
\begin{eqnarray*}
R_2 \leftarrow R_2-2R_1,\quad R_3 \leftarrow R_3-(-1)R_1=R_3+R_1.
\end{eqnarray*}
Esto produce
\begin{eqnarray*}
U^{(1)}=\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 7 & 6
\end{bmatrix},\qquad
L^{(1)}=\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 0 & 1
\end{bmatrix}.
\end{eqnarray*}

\item Paso $k=2$.
Pivote $u_{22}=1$. Multiplicador:
\begin{eqnarray*}
\ell_{32}=\frac{7}{1}=7.
\end{eqnarray*}
Actualizamos fila 3 de $U$:
\begin{eqnarray*}
R_3 \leftarrow R_3-7R_2 \;\;\Longrightarrow\;\;
U=\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 0 & -29
\end{bmatrix}.
\end{eqnarray*}
Insertamos $\ell_{32}$ en $L$:
\begin{eqnarray*}
L=\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 7 & 1
\end{bmatrix}.
\end{eqnarray*}

\item Resultado. Se tiene
\begin{eqnarray*}
\boxed{
A=LU,\quad
L=\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 7 & 1
\end{bmatrix},\quad
U=\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 0 & -29
\end{bmatrix}.
}
\end{eqnarray*}
(Verificación rápida: $LU=A$ por multiplicación directa).
\end{itemize}


\end{Ejem}
\bigskip

\begin{Ejem} [$PA=LU$]
Sea
\begin{eqnarray*}
A=\begin{bmatrix}
0 & 2 & 1\\
1 & -2 & -3\\
-1 & 1 & 2
\end{bmatrix}.
\end{eqnarray*}
El pivote inicial $a_{11}=0$ es inválido; aplicamos \textbf{pivoteo parcial} intercambiando $R_1 \leftrightarrow R_2$.
La matriz de permutación que permuta las primeras dos filas es
\begin{eqnarray*}
P=\begin{bmatrix}
0 & 1 & 0\\
1 & 0 & 0\\
0 & 0 & 1
\end{bmatrix},\qquad
PA=\begin{bmatrix}
1 & -2 & -3\\
0 &  2 &  1\\
-1&  1 &  2
\end{bmatrix}.
\end{eqnarray*}
\begin{itemize}
\item Paso $k=1$ sobre $PA$.
Pivote $u_{11}=1$. Multiplicador:
\begin{eqnarray*}
\ell_{31}=\frac{-1}{1}=-1.
\end{eqnarray*}
Actualizamos fila 3:
\begin{eqnarray*}
R_3 \leftarrow R_3-(-1)R_1=R_3+R_1 \;\;\Longrightarrow\;\;
U^{(1)}=\begin{bmatrix}
1 & -2 & -3\\
0 &  2 &  1\\
0 & -1 & -1
\end{bmatrix},\quad
L^{(1)}=\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-1& 0 & 1
\end{bmatrix}.
\end{eqnarray*}

\item Paso $k=2$. Pivote $u_{22}=2$. Multiplicador:
\begin{eqnarray*}
\ell_{32}=\frac{-1}{2}=-\tfrac{1}{2}.
\end{eqnarray*}
Actualizamos fila 3:
\begin{eqnarray*}
R_3 \leftarrow R_3-(-\tfrac{1}{2})R_2=R_3+\tfrac{1}{2}R_2 \;\;\Longrightarrow\;\;
U=\begin{bmatrix}
1 & -2 & -3\\
0 &  2 &  1\\
0 &  0 & -\tfrac{1}{2}
\end{bmatrix}.
\end{eqnarray*}
Insertamos $\ell_{32}$ en $L$:
\begin{eqnarray*}
L=\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-1& -\tfrac{1}{2} & 1
\end{bmatrix}.
\end{eqnarray*}

\item Resultado. Se obtiene
\begin{eqnarray*}
\boxed{
PA=LU,\quad
P=\begin{bmatrix}
0 & 1 & 0\\
1 & 0 & 0\\
0 & 0 & 1
\end{bmatrix},\quad
L=\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-1& -\tfrac{1}{2} & 1
\end{bmatrix},\quad
U=\begin{bmatrix}
1 & -2 & -3\\
0 &  2 &  1\\
0 &  0 & -\tfrac{1}{2}
\end{bmatrix}.
}
\end{eqnarray*}
(Comprobación: $LU=PA$ y, por tanto, $A=P^{\!\top}LU$ ya que $P^{-1}=P^{\!\top}$.)
\end{itemize}


\end{Ejem}

\begin{Ejem}Resolver $Ax=b$ vía $LU$
Consideremos
\begin{eqnarray*}
A=\begin{bmatrix}
2 & 3 & 1\\
4 & 7 & 7\\
-2 & 4 & 5
\end{bmatrix}, 
\qquad 
b=\begin{bmatrix}
1\\
2\\
3
\end{bmatrix}.
\end{eqnarray*}

Ya hemos obtenido la factorización
\begin{eqnarray*}
A=LU,\quad
L=\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 7 & 1
\end{bmatrix},\quad
U=\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 0 & -29
\end{bmatrix}.
\end{eqnarray*}

El sistema $Ax=b$ se resuelve en dos etapas:

\paragraph{1. Resolver $Ly=b$.}
\begin{eqnarray*}
\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 7 & 1
\end{bmatrix}
\begin{bmatrix}
y_1\\y_2\\y_3
\end{bmatrix}
=
\begin{bmatrix}
1\\2\\3
\end{bmatrix}.
\end{eqnarray*}
De aquí:
\begin{eqnarray*}
y_1=1, \quad
2y_1+y_2=2 \;\Rightarrow\; y_2=0, \quad
-1\cdot y_1+7y_2+y_3=3 \;\Rightarrow\; y_3=4.
\end{eqnarray*}
Por lo tanto,
\begin{eqnarray*}
y=\begin{bmatrix}1\\0\\4\end{bmatrix}.
\end{eqnarray*}

\paragraph{2. Resolver $Ux=y$.}
\begin{eqnarray*}
\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 0 & -29
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2\\x_3
\end{bmatrix}
=
\begin{bmatrix}
1\\0\\4
\end{bmatrix}.
\end{eqnarray*}
De abajo hacia arriba:
\begin{eqnarray*}
-29x_3=4 \;\Rightarrow\; x_3=-\tfrac{4}{29},
\end{eqnarray*}
\begin{eqnarray*}
x_2+5x_3=0 \;\Rightarrow\; x_2=-5x_3=\tfrac{20}{29},
\end{eqnarray*}
\begin{eqnarray*}
2x_1+3x_2+x_3=1 \;\Rightarrow\; 2x_1+3\cdot\tfrac{20}{29}-\tfrac{4}{29}=1.
\end{eqnarray*}
\begin{eqnarray*}
2x_1+\tfrac{60-4}{29}=1 \;\Rightarrow\; 2x_1+\tfrac{56}{29}=1.
\end{eqnarray*}
\begin{eqnarray*}
2x_1=\tfrac{29}{29}-\tfrac{56}{29}=-\tfrac{27}{29} \;\Rightarrow\;
x_1=-\tfrac{27}{58}.
\end{eqnarray*}

\paragraph{Solución final:}
\begin{eqnarray*}
\boxed{
x=\begin{bmatrix}
-\tfrac{27}{58}\\begin{eqnarray*}6pt]
\tfrac{20}{29}\\begin{eqnarray*}6pt]
-\tfrac{4}{29}
\end{bmatrix}.
}
\end{eqnarray*}
\end{Ejem}


\subsubsection{Factorización de Cholesky}

Si $A$ es simétrica y definida positiva, entonces admite una factorización especial:
\begin{eqnarray*}
A = LL^\top,
\end{eqnarray*}
donde $L$ es triangular inferior con entradas reales y diagonales positivas.  
Esta factorización es más eficiente y estable que la $LU$ en estos casos.

Sea $A\in\mathbb{R}^{n\times n}$ simétrica definida positiva. La factorización de Cholesky busca
\begin{eqnarray*}
A = LL^\top,
\end{eqnarray*}
con $L$ triangular inferior y diagonal positiva. Las fórmulas recursivas son, para $i=1,\dots,n$:
\begin{eqnarray*}
\ell_{ii}=\sqrt{\,a_{ii}-\sum_{k=1}^{i-1}\ell_{ik}^2\,},\qquad
\ell_{ji}=\frac{1}{\ell_{ii}}\Bigl(a_{ji}-\sum_{k=1}^{i-1}\ell_{jk}\,\ell_{ik}\Bigr)\quad (j=i+1,\dots,n).
\end{eqnarray*}

\begin{Ejem}
Considérese
\begin{eqnarray*}
A=\begin{bmatrix}
4 & 2 & 2\\
2 & 2 & 1\\
2 & 1 & 2
\end{bmatrix}.
\end{eqnarray*}
Es simétrica y definida positiva (sus menores principales son positivos). Apliquemos las fórmulas:
\begin{itemize}
\item Columna $i=1$.
\begin{eqnarray*}
\ell_{11}=\sqrt{4}=2,\qquad
\ell_{21}=\frac{2}{2}=1,\qquad
\ell_{31}=\frac{2}{2}=1.
\end{eqnarray*}

\item Columna $i=2$.
\begin{eqnarray*}
\ell_{22}=\sqrt{\,2-\ell_{21}^2\,}=\sqrt{2-1}=1,\qquad
\ell_{32}=\frac{1-\ell_{31}\ell_{21}}{\ell_{22}}=\frac{1-1\cdot1}{1}=0.
\end{eqnarray*}

\item Columna $i=3$.
\begin{eqnarray*}
\ell_{33}=\sqrt{\,2-\ell_{31}^2-\ell_{32}^2\,}=\sqrt{2-1-0}=1.
\end{eqnarray*}

\item Resultado.
\begin{eqnarray*}
L=\begin{bmatrix}
2&0&0\\
1&1&0\\
1&0&1
\end{bmatrix},\qquad
\boxed{A=LL^\top}.
\end{eqnarray*}
(Verificación rápida: $LL^\top=\begin{bmatrix}4&2&2\\2&2&1\\2&1&2\end{bmatrix}$.)

\end{itemize}
\end{Ejem}
\bigskip

\begin{Ejem}
Considérese
\begin{eqnarray*}
A=\begin{bmatrix}
9 & 3 & 6\\
3 & 5 & 3\\
6 & 3 & 14
\end{bmatrix}.
\end{eqnarray*}
También es simétrica definida positiva. Procedamos:
\begin{itemize}
\item Columna $i=1$.
\begin{eqnarray*}
\ell_{11}=\sqrt{9}=3,\qquad
\ell_{21}=\frac{3}{3}=1,\qquad
\ell_{31}=\frac{6}{3}=2.
\end{eqnarray*}

\item Columna $i=2$.
\begin{eqnarray*}
\ell_{22}=\sqrt{\,5-\ell_{21}^2\,}=\sqrt{5-1}=2,\qquad
\ell_{32}=\frac{3-\ell_{31}\ell_{21}}{\ell_{22}}=\frac{3-2\cdot1}{2}=\tfrac{1}{2}.
\end{eqnarray*}

\item Columna $i=3$.
\begin{eqnarray*}
\ell_{33}=\sqrt{\,14-\ell_{31}^2-\ell_{32}^2\,}
=\sqrt{\,14-4-\tfrac{1}{4}\,}
=\sqrt{\tfrac{39}{4}}=\frac{\sqrt{39}}{2}.
\end{eqnarray*}

\item Resultado.
\begin{eqnarray*}
L=\begin{bmatrix}
3&0&0\\
1&2&0\\
2&\tfrac{1}{2}&\tfrac{\sqrt{39}}{2}
\end{bmatrix},\qquad
\boxed{A=LL^\top}.
\end{eqnarray*}
(Verificación: $LL^\top$ reproduce $A$; la última diagonal verifica $2^2+(\tfrac{1}{2})^2+(\tfrac{\sqrt{39}}{2})^2=14$.)
\end{itemize}
\end{Ejem}

\begin{itemize}
  \item \textbf{Forma:} $A=LL^\top$, con $L$ triangular inferior y diagonal positiva.
  \item \textbf{Requisitos:} $A$ debe ser \textbf{simétrica definida positiva (SDP)}.
  \item \textbf{Pivoteo:} No requiere pivoteo si $A$ es SDP (bien condicionada).
  \item \textbf{Ventajas:} Más rápida y estable (sin pivoteo) para matrices SDP; mejor aprovechamiento de memoria (simetría).
  \item \textbf{Cuándo usarla:} Sistemas simétricos definidos positivos (p.ej.\ matrices de Gram, problemas de mínimos cuadrados regulares, discretizaciones elípticas).
\end{itemize}

\begin{Note}Si $A$ es SDP, use Cholesky. En caso general, use $LU$ con pivoteo.
\end{Note}


\subsubsection{Factorización de Doolittle}

Es una variante de la factorización $LU$ en la cual la matriz $L$ tiene $1$'s en la diagonal principal:
\begin{eqnarray*}
A = LU, \quad L \text{ con unos en la diagonal}, \; U \text{ triangular superior}.
\end{eqnarray*}

\begin{itemize}
  \item \textbf{Forma:} $A=LU$, con $L$ triangular inferior con $1$ en la diagonal y $U$ triangular superior.
  \item \textbf{Requisitos:} Válida para matrices cuadradas no singulares (en la práctica, puede requerir permutaciones: $PA=LU$).
  \item \textbf{Pivoteo:} Suele emplear \emph{pivoteo parcial} para estabilidad numérica.
  \item \textbf{Cuándo usarla:} Sistemas generales (no necesariamente simétricos ni definidos positivos), múltiples RHS $b$ con la misma $A$.
\end{itemize}


\begin{Note}[Esquema general]
Si $A=LL^\top$ con $L$ triangular inferior y diagonal positiva, resolver $Ax=b$ equivale a:
\begin{eqnarray*}
Ly=b \quad\text{(sustitución progresiva)}, \qquad
L^\top x=y \quad\text{(sustitución regresiva)}.
\end{eqnarray*}
\end{Note}

\begin{Ejem}
Sea
\begin{eqnarray*}
A=\begin{bmatrix}
4 & 2 & 2\\
2 & 2 & 1\\
2 & 1 & 2
\end{bmatrix}, 
\qquad
b=\begin{bmatrix}1\\2\\3\end{bmatrix}.
\end{eqnarray*}
Su factorización de Cholesky es
\begin{eqnarray*}
L=\begin{bmatrix}
2&0&0\\
1&1&0\\
1&0&1
\end{bmatrix}
\quad\Longrightarrow\quad
A=LL^\top.
\end{eqnarray*}
\begin{itemize}

\item Paso 1: $Ly=b$.

\begin{eqnarray*}
\begin{bmatrix}
2&0&0\\
1&1&0\\
1&0&1
\end{bmatrix}
\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}
=
\begin{bmatrix}1\\2\\3\end{bmatrix}
\end{eqnarray*}
\begin{eqnarray*}
2y_1=1 \ \Rightarrow\ y_1=\tfrac{1}{2},\\
y_1+y_2=2 \ \Rightarrow\ y_2=\tfrac{3}{2},\\
y_1+y_3=3 \ \Rightarrow\ y_3=\tfrac{5}{2}.
\end{eqnarray*}
Por tanto, $y=\left[\tfrac{1}{2},\ \tfrac{3}{2},\ \tfrac{5}{2}\right]^\top$.

\item Paso 2: $L^\top x=y$.
\begin{eqnarray*}
L^\top=\begin{bmatrix}
2&1&1\\
0&1&0\\
0&0&1
\end{bmatrix},\quad
\end{eqnarray*}

\begin{eqnarray*}
\begin{bmatrix}
2&1&1\\
0&1&0\\
0&0&1
\end{bmatrix}
\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}
=
\begin{bmatrix}\tfrac{1}{2}\\
\tfrac{3}{2}\\
\tfrac{5}{2}
\end{bmatrix}
\Rightarrow
\begin{cases}
x_3=\tfrac{5}{2},\\
x_2=\tfrac{3}{2},\\
2x_1+x_2+x_3=\tfrac{1}{2}\ \Rightarrow\ 2x_1=\tfrac{1}{2}-\tfrac{3}{2}-\tfrac{5}{2}=-\tfrac{7}{2}\ \Rightarrow\ x_1=-\tfrac{7}{4}.
\end{cases}
\end{eqnarray*}

\begin{eqnarray*}
\boxed{
x=\begin{bmatrix}
-\tfrac{7}{4}\\
\tfrac{3}{2}\\
\tfrac{5}{2}
\end{bmatrix}}
\end{eqnarray*}
\end{itemize}
\end{Ejem}


\begin{Ejem}
Sea
\begin{eqnarray*}
A=\begin{bmatrix}
9 & 3 & 6\\
3 & 5 & 3\\
6 & 3 & 14
\end{bmatrix}, 
\qquad
b=\begin{bmatrix}1\\2\\3\end{bmatrix}.
\end{eqnarray*}
Una factorización de Cholesky es
\begin{eqnarray*}
L=\begin{bmatrix}
3&0&0\\
1&2&0\\
2&\tfrac{1}{2}&\tfrac{\sqrt{39}}{2}
\end{bmatrix}
\quad\Longrightarrow\quad
A=LL^\top.
\end{eqnarray*}

\begin{itemize}
\item Paso 1: $Ly=b$.

\begin{eqnarray*}
\begin{bmatrix}
3&0&0\\
1&2&0\\
2&\tfrac{1}{2}&\tfrac{\sqrt{39}}{2}
\end{bmatrix}
\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}
=
\begin{bmatrix}1\\2\\3\end{bmatrix}
\end{eqnarray*}

\begin{eqnarray*}
3y_1&=&1 \ \Rightarrow\ y_1=\tfrac{1}{3},\\
y_1+2y_2&=&2 \ \Rightarrow\ y_2=\tfrac{2-\tfrac{1}{3}}{2}=\tfrac{5}{6},\\
2y_1+\tfrac{1}{2}y_2+\tfrac{\sqrt{39}}{2}y_3&=&3 
\ \Rightarrow\ 
\tfrac{\sqrt{39}}{2}y_3=3-\tfrac{2}{3}-\tfrac{5}{12}=\tfrac{23}{12}\\
y_3&=&\frac{23}{12}\cdot\frac{2}{\sqrt{39}}=\frac{23}{6\sqrt{39}}.
\end{eqnarray*}

Por tanto, $y=\bigl[\tfrac{1}{3},\ \tfrac{5}{6},\ \tfrac{23}{6\sqrt{39}}\bigr]^\top$.

\item Paso 2: $L^\top x=y$.

\begin{eqnarray*}
L^\top=\begin{bmatrix}
3&1&2\\
0&2&\tfrac{1}{2}\\
0&0&\tfrac{\sqrt{39}}{2}
\end{bmatrix},
\end{eqnarray*}


\begin{eqnarray*}
\begin{bmatrix}
3&1&2\\
0&2&\tfrac{1}{2}\\
0&0&\tfrac{\sqrt{39}}{2}
\end{bmatrix}
\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}
=
\begin{bmatrix}\tfrac{1}{3}\\
\tfrac{5}{6}\\
\tfrac{23}{6\sqrt{39}}\end{bmatrix}
\Rightarrow
\begin{cases}
\tfrac{\sqrt{39}}{2}\,x_3=\tfrac{23}{6\sqrt{39}} \ \Rightarrow\ x_3=\tfrac{23}{117},\\
2x_2+\tfrac{1}{2}x_3=\tfrac{5}{6} \ \Rightarrow\ 2x_2=\tfrac{5}{6}-\tfrac{1}{2}\cdot\tfrac{23}{117}=\tfrac{86}{117} \ \Rightarrow\ x_2=\tfrac{43}{117},\\
3x_1+x_2+2x_3=\tfrac{1}{3}=\tfrac{39}{117} \ \Rightarrow\ 3x_1=\tfrac{39-43-46}{117}=-\tfrac{50}{117} \ \Rightarrow\\\
x_1=-\tfrac{50}{351}.
\end{cases}
\end{eqnarray*}

\begin{eqnarray*}
\boxed{
x=\begin{bmatrix}
-\tfrac{50}{351}\\
\tfrac{43}{117}\\
\tfrac{23}{117}
\end{bmatrix}}
\end{eqnarray*}
\end{itemize}
\end{Ejem}


\begin{Ejem}
\begin{eqnarray*}
A=\begin{pmatrix}
2 & -1 & 3\\
0 & 4 & 5\\
0 & 0 & -2
\end{pmatrix},\qquad
L=\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}=L^{T},\qquad
U=\begin{pmatrix}
2 & -1 & 3\\
0 & 4 & 5\\
0 & 0 & -2
\end{pmatrix}.
\end{eqnarray*}

\begin{eqnarray*}
A = LU = I \cdot U = U, \quad \text{y } L=L^{T}.
\end{eqnarray*}
\end{Ejem}


\begin{Ejem}
\begin{eqnarray*}
\text{Tomemos } 
L=\begin{pmatrix}
2 & 0 & 0\\
0 & 3 & 0\\
0 & 0 & 4
\end{pmatrix}
\quad\text{(simétrica y triangular inferior, }L=L^{T}\text{),}
\end{eqnarray*}

\begin{eqnarray*}
U=\begin{pmatrix}
1 & -1 & 2\\
0 & 2  & 5\\
0 & 0  & -3
\end{pmatrix}\ \text{(triangular superior).}
\end{eqnarray*}

\begin{eqnarray*}
A=LU=
\begin{pmatrix}
2 & 0 & 0\\
0 & 3 & 0\\
0 & 0 & 4
\end{pmatrix}
\begin{pmatrix}
1 & -1 & 2\\
0 & 2  & 5\\
0 & 0  & -3
\end{pmatrix}
=
\begin{pmatrix}
2 & -2 & 4\\
0 & 6  & 15\\
0 & 0  & -12
\end{pmatrix}.
\end{eqnarray*}
\end{Ejem}



\begin{Ejem}
\begin{eqnarray*}
A=\begin{pmatrix}
2 & -1 & 1 & 3\\
4 &  1 & 0 & 2\\
-2 & 2 & 5 & 1\\
6 & 0 & 2 & 4
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
L=\begin{pmatrix}
1 & 0 & 0 & 0\\
2 & 1 & 0 & 0\\
-1 & 1 & 1 & 0\\
3 & -1 & 0 & 1
\end{pmatrix}, \qquad
U=\begin{pmatrix}
2 & -1 & 1 & 3\\
0 & 3 & -2 & -4\\
0 & 0 & 6 & 3\\
0 & 0 & 0 & -2
\end{pmatrix}.
\end{eqnarray*}

\end{Ejem}

\subsection{Descomposición de $A$}

Sea $A=D+L+U$, con:
\begin{eqnarray*}
D&=&\text{diag}(a_{11},\dots,a_{nn}),\\
L&=&\text{parte estrictamente triangular inferior},\\
U&=&\text{parte estrictamente triangular superior}.
\end{eqnarray*}

De $Ax=b$ obtenemos:
\begin{eqnarray}
x = -D^{-1}(L+U)x + D^{-1}b,
\end{eqnarray}
lo que sugiere la iteración:
\begin{eqnarray}
x^{(k+1)}&=&Tx^{(k)}+c,\\
T&=&-D^{-1}(L+U),\\
c&=&D^{-1}b.
\end{eqnarray}

\subsubsection{Método de Jacobi}

\begin{eqnarray}
x_i^{(k+1)}=\frac{1}{a_{ii}}\Biggl(b_i-\sum_{\substack{j=1\\j\neq i}}^n a_{ij}x_j^{(k)}\Biggr).
\end{eqnarray}
Se calcula cada $x_i^{(k+1)}$ sólo con valores de la iteración anterior.

En forma matricial:
\begin{eqnarray}
T_J=-D^{-1}(L+U),\quad c_J=D^{-1}b.
\end{eqnarray}



\begin{Ejem}
\begin{eqnarray*}
\begin{cases}
a_{11}x_1+a_{12}x_2+a_{13}x_3=b_1,\\
a_{21}x_1+a_{22}x_2+a_{23}x_3=b_2,\\
a_{31}x_1+a_{32}x_2+a_{33}x_3=b_3.
\end{cases}
\end{eqnarray*}
Iteración:
\begin{eqnarray*}
x_1^{(k+1)}&=&\tfrac{1}{a_{11}}(b_1-a_{12}x_2^{(k)}-a_{13}x_3^{(k)}),\\
x_2^{(k+1)}&=&\tfrac{1}{a_{22}}(b_2-a_{21}x_1^{(k)}-a_{23}x_3^{(k)}),\\
&\dots&
\end{eqnarray*}
\end{Ejem}


\subsubsection{Método de Gauss--Seidel}

Aprovecha los valores nuevos tan pronto como se calculan:

\begin{eqnarray}
x^{(k+1)}=(D+L)^{-1}\bigl(-Ux^{(k)}+b\bigr).
\end{eqnarray}

Por componentes:
\begin{eqnarray}
x_i^{(k+1)}=\frac{1}{a_{ii}}\left(b_i-\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)}-\sum_{j=i+1}^n a_{ij}x_j^{(k)}\right).
\end{eqnarray}


\begin{Note}[Condiciones de convergencia]
Ambos métodos convergen si el radio espectral de la matriz de iteración es menor que uno:
\begin{eqnarray*}
\rho(T)<1.
\end{eqnarray*}
Casos suficientes:
\begin{itemize}
  \item $A$ diagonalmente dominante $\;\Rightarrow\;$ Jacobi y Gauss--Seidel convergen.
  \item $A$ simétrica definida positiva $\;\Rightarrow\;$ Gauss--Seidel converge.
\end{itemize}

\begin{itemize}
  \item Convergencia $\iff \rho(T)<1$, donde $\rho$ es el radio espectral.
  \item Suficiente: $A$ diagonalmente dominante estricta $\Rightarrow$ Jacobi y GS convergen.
  \item Suficiente: $A$ simétrica definida positiva $\Rightarrow$ GS siempre converge.
\end{itemize}
\end{Note}


\subsubsection{Método SOR (Successive Over-Relaxation)}

Generaliza Gauss--Seidel:
\begin{eqnarray*}
x_i^{(k+1)}=(1-\omega)x_i^{(k)}+
\frac{\omega}{a_{ii}}\left(b_i-\sum_{j<i}a_{ij}x_j^{(k+1)}-\sum_{j>i}a_{ij}x_j^{(k)}\right).
\end{eqnarray*}

donde
\begin{itemize}
  \item $\omega=1$ reproduce Gauss--Seidel.
  \item $1<\omega<2$: \emph{sobrerrelajación}, posible aceleración.
  \item $0<\omega<1$: \emph{sub-relajación}, usada en algunos problemas mal condicionados.
\end{itemize}

Reescribiendo $Ax=b$:
\begin{eqnarray}
x&=&Tx+c,\\
T&=&-D^{-1}(L+U),\\
c&=&D^{-1}b.
\end{eqnarray}
De aquí nace el esquema iterativo:
\begin{eqnarray}
x^{(k+1)}=Tx^{(k)}+c.
\end{eqnarray}



\begin{Note}
\begin{eqnarray*}
T_J &=& -D^{-1}(L+U),  c_J=D^{-1}b,\\
T_{GS}&=&-(D+L)^{-1}U,  c_{GS}=(D+L)^{-1}b,\\
T_{SOR}&=&(D+\omega L)^{-1}\big((1-\omega)D-\omega U\big), c_{SOR}=\omega(D+\omega L)^{-1}b.
\end{eqnarray*}
\end{Note}





\subsubsection{Descomposición de la matriz}
Sea $A\in\mathbb{R}^{n\times n}$. Se descompone como
\begin{eqnarray*}
A = D - L - U,
\end{eqnarray*}
donde:
\begin{itemize}
  \item $D$: matriz diagonal de $A$,
  \item $-L$: parte estrictamente triangular inferior,
  \item $-U$: parte estrictamente triangular superior.
\end{itemize}
Así,
\begin{eqnarray*}
A = D + L + U, \quad
Ax=b \;\;\Longleftrightarrow\;\; (D+L+U)x=b.
\end{eqnarray*}

\subsubsection{Método de Jacobi}
A partir de la descomposición, se obtiene el esquema iterativo de Jacobi:
\begin{eqnarray*}
x^{(k+1)} = D^{-1}\bigl(b - (L+U)x^{(k)}\bigr).
\end{eqnarray*}
De manera componente a componente:
\begin{eqnarray*}
x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{\substack{j=1 \\ j\neq i}}^n a_{ij} x_j^{(k)}\right), 
\quad i=1,\dots,n.
\end{eqnarray*}
\begin{itemize}
  \item Cada componente $x_i^{(k+1)}$ se calcula usando exclusivamente valores de la iteración previa $x^{(k)}$.
  \item Es fácil de implementar en paralelo.
\end{itemize}

\subsubsection{Método de Gauss--Seidel}
En Gauss--Seidel se aprovecha que, en cada paso, los nuevos valores calculados pueden usarse de inmediato:
\begin{eqnarray*}
x^{(k+1)} = (D+L)^{-1}\bigl(b - Ux^{(k)}\bigr).
\end{eqnarray*}
En forma componente:
\begin{eqnarray*}
x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)}\right).
\end{eqnarray*}
\begin{itemize}
  \item La diferencia clave con Jacobi: $x_j^{(k+1)}$ ya calculados se usan de inmediato.
  \item Suele converger más rápido que Jacobi.
\end{itemize}

\subsubsection{Condiciones de convergencia}
Un resultado clásico:
\begin{itemize}
  \item Si $A$ es \textbf{diagonal dominante estricta} (es decir, $|a_{ii}| > \sum_{j\neq i}|a_{ij}|$ para todo $i$), entonces Jacobi y Gauss--Seidel convergen.
  \item Si $A$ es \textbf{simétrica definida positiva}, Gauss--Seidel siempre converge.
  \item En general, la convergencia depende de que el radio espectral de la matriz de iteración sea menor que $1$:
  \begin{eqnarray*}
  \rho(T)<1.
  \end{eqnarray*}
\end{itemize}

\begin{Note}[Esquema general] Se procede de la siguiente manera:

\begin{enumerate}
  \item Elegir un vector inicial $x^{(0)}$ (p.ej.\ el vector nulo).
  \item Repetir hasta convergencia:
  \begin{enumerate}
    \item Calcular $x^{(k+1)}$ según el método elegido (Jacobi o Gauss--Seidel).
    \item Medir el error, p.ej.\ $\|x^{(k+1)}-x^{(k)}\|$.
    \item Detenerse cuando el error sea menor que una tolerancia $\varepsilon$.
  \end{enumerate}
\end{enumerate}
\end{Note}

\begin{Ejem} Sistema de prueba (diagonalmente dominante)
Consideremos
\begin{eqnarray*}
A=
\begin{bmatrix}
10 & -1 & 2 & 0\\
-1 & 11 & -1 & 3\\
2 & -1 & 10 & -1\\
0 & 3 & -1 & 8
\end{bmatrix},\qquad
b=\begin{bmatrix}6\\25\\-11\\15\end{bmatrix},
\end{eqnarray*}
con vector inicial \(x^{(0)}=\mathbf{0}\).
Este sistema es \emph{diagonalmente dominante}, por lo que Jacobi y Gauss--Seidel convergen.

La solución exacta es
\begin{eqnarray*}
x^\ast=\begin{bmatrix}1\\
2\\
-1\\
1\end{bmatrix}.
\end{eqnarray*}
\end{Ejem}


\begin{Ejem} [Método de Jacobi]
Recordemos que
\begin{eqnarray*}
x^{(k+1)}=D^{-1}\bigl(b-(L+U)\,x^{(k)}\bigr),\qquad
x_i^{(k+1)}=\frac{1}{a_{ii}}\!\left(b_i-\sum_{j\neq i}a_{ij}\,x_j^{(k)}\right).
\end{eqnarray*}


\begin{eqnarray*}
x^{(0)}=\begin{bmatrix}0\\0\\0\\0\end{bmatrix},\quad
x^{(1)}=\begin{bmatrix}
0.6\\ 2.272727\\ -1.100000\\ 1.875000
\end{bmatrix},\quad
x^{(2)}=\begin{bmatrix}
1.047273\\ 1.715909\\ -0.805227\\ 0.885227
\end{bmatrix},\quad
x^{(3)}=\begin{bmatrix}
0.932636\\ 2.053306\\ -1.049341\\ 1.130881
\end{bmatrix}.
\end{eqnarray*}

Errores (norma infinito)
\begin{eqnarray*}
\|x^{(0)}-x^\ast\|_\infty=2.000000,\\
\|x^{(1)}-x^\ast\|_\infty=0.875000,\\
\|x^{(2)}-x^\ast\|_\infty=0.284091,\\
\|x^{(3)}-x^\ast\|_\infty=0.130881.
\end{eqnarray*}
\end{Ejem}

\begin{Ejem}[Método de Gauss--Seidel]
\begin{eqnarray*}
x^{(k+1)}&=&(D+L)^{-1}\bigl(b-U\,x^{(k)}\bigr),\\
x_i^{(k+1)}&=&\frac{1}{a_{ii}}\!\left(b_i-\sum_{j<i}a_{ij}\,x_j^{(k+1)}-\sum_{j>i}a_{ij}\,x_j^{(k)}\right).
\end{eqnarray*}

Iteraciones (redondeadas a 6 decimales).
\begin{eqnarray*}
x^{(0)}=\begin{bmatrix}0\\0\\0\\0\end{bmatrix},\quad
x^{(1)}=\begin{bmatrix}
0.6\\ 2.327273\\ -0.987273\\ 0.878864
\end{bmatrix},\quad
x^{(2)}=\begin{bmatrix}
1.030182\\ 2.036938\\ -1.014456\\ 0.984341
\end{bmatrix},\quad
x^{(3)}=\begin{bmatrix}
1.006585\\ 2.003555\\ -1.002527\\ 0.998351
\end{bmatrix}.
\end{eqnarray*}

Errores (norma infinito) frente a \(x^\ast\).
\begin{eqnarray*}
\|x^{(0)}-x^\ast\|_\infty=2.000000,\\
\|x^{(1)}-x^\ast\|_\infty=0.400000,\\
\|x^{(2)}-x^\ast\|_\infty=0.036938,\\
\|x^{(3)}-x^\ast\|_\infty=0.006585.
\end{eqnarray*}
\end{Ejem}
\begin{Note}
\begin{itemize}
  \item Ambos métodos convergen (la matriz es diagonalmente dominante), pero \textbf{Gauss--Seidel} reduce el error notablemente más rápido al reutilizar los valores nuevos dentro de la misma iteración.
  \item Criterio de paro típico: detener cuando \(\|x^{(k+1)}-x^{(k)}\| \le \varepsilon\) o \(\|Ax^{(k)}-b\| \le \varepsilon\).
  \item Para paralelización masiva, \textbf{Jacobi} es más simple; para rapidez en CPU única, \textbf{Gauss--Seidel} suele ser preferible.
\end{itemize}
\end{Note}

\begin{Ejem}[SOR en una sola iteración (comparación con Gauss--Seidel)]\medskip

Consideremos el mismo sistema diagonalmente dominante:
\begin{eqnarray*}
A=
\begin{bmatrix}
10 & -1 & 2 & 0\\
-1 & 11 & -1 & 3\\
2 & -1 & 10 & -1\\
0 & 3 & -1 & 8
\end{bmatrix},
\qquad
b=\begin{bmatrix}6\\25\\-11\\15\end{bmatrix},
\qquad
x^{(0)}=\mathbf{0}.
\end{eqnarray*}

con $\omega=1.2$
\begin{eqnarray*}
x_i^{(k+1)}=(1-\omega)\,x_i^{(k)}
+\frac{\omega}{a_{ii}}\!\left(b_i-\sum_{j<i}a_{ij}\,x_j^{(k+1)}-\sum_{j>i}a_{ij}\,x_j^{(k)}\right).
\end{eqnarray*}

Una iteración desde $x^{(0)}=\mathbf{0}$ (redondeado a $6$ decimales).
\begin{eqnarray*}
x_{\text{SOR}}^{(1)}=
\begin{bmatrix}
0.720000\\
2.805818\\
-1.156102\\
0.813967
\end{bmatrix}.
\end{eqnarray*}

Gauss--Seidel, una iteración
\begin{eqnarray*}
x_{\text{GS}}^{(1)}=
\begin{bmatrix}
0.600000\\
2.327273\\
-0.987273\\
0.878864
\end{bmatrix}.
\end{eqnarray*}


\begin{itemize}
  \item Con $\omega>1$ (sobrerrelajación), SOR puede \emph{acelerar} la convergencia, pero la \textbf{primera} iteración puede mostrar un “sobre-disparo” respecto a la solución exacta.
  \item La elección de $\omega$ es clave: en la práctica se prueba $\omega\in[1.1,1.5]$ y se observa el comportamiento; si $\omega$ es muy grande, puede empeorar o incluso hacer divergir el método.
  \item Si $A$ es SPD, métodos como \textbf{Cholesky} (directo) o iterativos tipo \textbf{Gradiente Conjugado} suelen ser preferibles por estabilidad y rapidez.
\end{itemize}
\end{Ejem}

\begin{Note} [Esquema general]
Dado $Ax=b$ y un vector inicial $x^{(0)}$, el método SOR genera la sucesión:
\begin{eqnarray*}
x_i^{(k+1)} = (1-\omega)\,x_i^{(k)} + 
\frac{\omega}{a_{ii}}
\left(b_i - \sum_{j<i} a_{ij} x_j^{(k+1)} - \sum_{j>i} a_{ij} x_j^{(k)}\right),
\quad i=1,\dots,n.
\end{eqnarray*}

\begin{itemize}
  \item Si $\omega=1$, se recupera exactamente el método de Gauss--Seidel.
  \item Si $0<\omega<1$, se llama \emph{relajación sub-sucessiva}, útil para estabilizar ciertos casos.
  \item Si $1<\omega<2$, se llama \emph{sobrerrelajación}, y puede acelerar notablemente la convergencia.
  \item La elección óptima de $\omega$ depende de la matriz $A$; en la práctica, se prueba con valores como $\omega\approx 1.1$ a $1.5$.
  \item El método es útil en problemas grandes, como los provenientes de discretización de ecuaciones diferenciales.
\end{itemize}

\begin{itemize}
  \item Jacobi: usa valores viejos.  
  \item Gauss--Seidel: usa valores nuevos en cuanto están disponibles.  
  \item SOR: Gauss--Seidel + parámetro $\omega$ que acelera la convergencia si se elige bien.
\end{itemize}
\end{Note}

%===========================================
\subsection{Ejercicios SEL}
%===========================================



\begin{Ejer}
Resolver por eliminación Gaussiana Simple los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
x_1-2x_2+0.5x_3&=&-5\\
-2x_1+5x_{2}-1.5x_3&=&0\\
-0.2x_1+1.75x_2-x_3&=&10
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-x_2+6x_4&=&2.3\\
4x_1+2x_2-x_3-5x_4&=&6.9\\
-5x_1+x_2-3x_3&=&-36\\
10x_2-4x_3+7x_4&=&-36
\end{eqnarray*}

\item \begin{eqnarray*}
0.003000x_1+59.14x_2&=&59.17\\
5.291x_1-6.130x_2&=&46.78
\end{eqnarray*}
utilizar redondeo a 4 cifras significativas.

\item \begin{eqnarray*}
4x_1+2x_2&=&2\\
2x_1+3x_2+x_3&=&-1\\
x_2+\frac{5}{2}x_3&=&3
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.1x_2-0.2x_3&=&7.85\\
0.1x_{1}+7x_2-0.3x_3=-19.3\\
0.3x_1-0.2x_2+10x_3&=&71.4
\end{eqnarray*}

\item \begin{eqnarray*}
8x_1+2x_2-2x_3&=&-2\\
10x_1+2x_2+4x_3&=&4\\
12x_1+2x_2+2x_3&=&6
\end{eqnarray*}

\item \begin{eqnarray*}
5x_1 + 2x_2 + x_3 - x_4 = 1\\
2x_1 - x_2 + 3x_3 + 2x_4 = 12\\
4x_1 + x_2 - 2x_3 + 3x_4 = 5\\
-2x_1 + 2x_2 + x_3 + x_4 = 2
\end{eqnarray*}

\item \begin{eqnarray*}
2x_1+3x_2+2x_3+4x_4&=&4\\
4x_1+10x_2-4x_3&=&-8\\
-3x_1-2x_2-5x_3-2x_4&=&-4\\
-2x_1+4x_2+4x_3-7x_4&=&-1
\end{eqnarray*}

\item \begin{eqnarray*}
1.133x_1+5.281x_2-2.454x_3&=&6.414\\
24.14x_1-1.21x_2+5.281x_3&=&113.8\\
-10.123x_1+6.387x_2-x_3&=&1
\end{eqnarray*}

\item $A=\left(\begin{array}{ccccc}
2 & 3 & 2 & 4\\
4 & 10 &-4 & 0\\
-3 & -2 & -5 &-2\\
-2 & 4 & 4 &-7\\
\end{array}\right)$ y $b=\left(\begin{array}{c}\\
9 \\
-15\\
6 \\
2\\
\end{array}\right)$

\end{enumerate}
\end{Ejer}

\begin{Ejer}
Resolver por eliminación gaussiana con pivoteo parcial los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
0.4x_1-1.5x_2+0.75x_3&=&-20\\
-0.5x_1-15x_2+10x_3&=&-10\\
-10x_1-9x_2+2.5x_3&=&30
\end{eqnarray*}

\item \begin{eqnarray*}
5x_1-8x_2+x_3&=&-71\\
-2x_1+6x_2-9x_3&=&134\\
3x_1-5x_2+2x_3&=&-58
\end{eqnarray*}

\item \begin{eqnarray*}
0.003000x_1+59.14x_2&=&59.17\\
5.291x_1-6.130x_2&=&46.78
\end{eqnarray*}
utilizar redondeo a 4 cifras significativas.


\item \begin{eqnarray*}
-x_2+4x_3-x_4&=&-1\\
-x_1+4x_2-x_3&=&2\\
-x_1-x_3+4x_4&=&4\\
4x_1-x_2&=&10
\end{eqnarray*}

\item \begin{eqnarray*}
0.00031000x_1+1.000000x_2&=&3.000000\\
1.00045534x_1+1.00034333x_2&=&7.000
\end{eqnarray*}


\item Resolver para $A=\left(\begin{array}{ccccc}\\
14 & 14 & -9 & 3 & -5\\
14 & 52 & -15 & 2 & -32\\
-9 & -15 & 36 &-5 & 16\\
3 &2 &-5&47 & 49\\
-5 & 32 & 16 &49 & 79\end{array}\right)$,  $b=\left(\begin{array}{c}\\
-15\\
-100\\
106\\
329\\
463\end{array}\right)$ y  $X=\left(\begin{array}{c}\\
x_1\\
x_2\\
x_3\\
x_4\\
x_5\end{array}\right)$


\item Resolver para $A=\left(\begin{array}{ccccc}
\frac{1}{4} &\frac{1}{5} &\frac{1}{6}\\
 \frac{1}{3} & \frac{1}{4} & \frac{1}{5} \\  
  \frac{1}{2} &  1& 2
\end{array}\right)$,  $b=\left(\begin{array}{c}
9\\
8\\
8\\\end{array}\right)$ y  $X=\left(\begin{array}{c}
x_1\\
x_2\\
x_3\\
\end{array}\right)$

\item  Resolver para $A=\left(\begin{array}{ccccc}
1 & \frac{1}{2} & \frac{1}{3} & \frac{1}{4}  \\
 \frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \frac{1}{5}\\
 \frac{1}{3} & \frac{1}{4} & \frac{1}{5} & \frac{1}{6}\\
 \frac{1}{4} & \frac{1}{5} & \frac{1}{6} & \frac{1}{7}
 \end{array}\right)$,  $b=\left(\begin{array}{c}
 \frac{1}{6} \\
 \frac{1}{7} \\
  \frac{1}{8}\\ 
 \frac{1}{9} \\\end{array}\right)$ y  $X=\left(\begin{array}{c}
x_1\\
x_2\\
x_3\\
x_4
\end{array}\right)$

\item Resolver el sistema
\begin{eqnarray*}
2x_1+x_2-x_3+x_4-3x_5&=&7\\
x_1+2x_3-x_4+x_5&=&2\\
-2x_2-x_3+x_4-x_5&=&-5\\
3x_1+x_2-4x_3+5x_5&=&6\\
x_1-x_2-x_3-x_4+x_5&=&3
\end{eqnarray*}



\item Resolver el sistema
\begin{eqnarray*}
3.333x_1+15920x_2-10.333x_3&=&15913\\
2.222x_1+16.71x_29.612x_2&=&28.544\\
1.5611x_1+5.1791x_2+1.6852x_3&=&8.4254
\end{eqnarray*}

\end{enumerate}
\end{Ejer}


\begin{Ejer}
Resolver por el método de Gauss-Jordan los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
x_1+2x_2+3x_3&=&1\\
-0.4x_1+2x_2-x_3&=&10\\
0.5x_1-3x_2+x_3&=&15
\end{eqnarray*}

\item \begin{eqnarray*}
2x_1-0.9x_2+3x_3&=&-3.61\\
-0.5x_1+0.1x_2-x_3&=&2.035\\
x_1-6.35x_2-0.45x_3&=&15.401
\end{eqnarray*}

\item \begin{eqnarray*}
0.7x_1+2.7x_2-6x_3+0.7x_4&=&1.6487\\
2x_1-0.8x_2+3x_3-x_4&=&-2.342\\
-x_1-1.5x_2+1.4x_3+3x_4&=&-4.189\\
7x_2-1.56x_3+x_4=15.792
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.1x_2-0.2x_3&=&7.85\\
0.1x_1+7x_2-0.3x_3&=&-19.3\\
0.3x_1-0.2x_2+10x_3&=&71.4
\end{eqnarray*}

\item \begin{eqnarray*}
10x_1+2x_2-x_3&=&27\\
-3x_1-6x_2+2x3&=&-61.5\\
x_1+x_2+5x_3&=&-21.5
\end{eqnarray*}

\item $A=\left(\begin{array}{ccccc}\\
1 &3 & -2 & 1\\
1  &3 & -1 & 2\\
0  &1 & -1 & 4\\
2  &6 & 1 & 2\\
\end{array}\right)$ y $b=\left(\begin{array}{c}
4 \\
1 \\
5\\ 
2\\\end{array}\right)$

\item \begin{eqnarray*}
6x_1-x_2-x_3+4x_4&=&17\\
x_1-10x_2+2x_3-x_4&=&-17\\
3x_1-2x_2+8x_3-x_4&=&19\\
x_1+x_2+x_3-5x_4&=&-14
\end{eqnarray*}

\item \begin{eqnarray*}
x+2y+3z+4w&=&1\\
x-4y+z+11w&=&2\\
-x+8y+7z+6w&=&-2\\
16x+8y-5z+6w&=&11
\end{eqnarray*}


\item \begin{eqnarray*}
x_1+x_2&=&3\\
x_1+2x_2+x_3&=&-1\\
x_2+3x_3+x_4&=&2\\
x_3+4x_4+x_5&=&1\\
x_4+5x_5&=&3
\end{eqnarray*}

\item \begin{eqnarray*}
15x_1-18x_2+15x_3-3x_4&=&11\\
-18x_1+24_2-18x_3+4x_4&=&10\\
15x_1-18x_2+18x_3-3x_4&=&11\\
-3x_1+4x_2-3x_3+x_4&=&13
\end{eqnarray*}

\end{enumerate}
\end{Ejer}

\begin{Ejer}
Resolver por el método de Gauss-Seidel los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
3x_1-0.2x_2-0.5x_3&=&8\\
0.1x_1+7x_2+0.4x_3&=&-19.5\\
0.4x_1-0.1x_2+10x_3&=&72.4
\end{eqnarray*}

\item \begin{eqnarray*}
-5x_1+1.4x_2-2.7x_3&=&94.2\\
0.7x_1-2.5x_2+15x_3&=&-6\\
3.3x_1-11x_2+4.4x_3&=&-27.5
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.5x_2+0.6x_3&=&5.24\\
0.3x_1-4x_2-x_3&=&-0.387\\
-0.7x_1+2x_2+7x_3&=&14.803
\end{eqnarray*}

\item \begin{eqnarray*}
5x_1-0.2x_2+x_3&=&1.5\\
0.1x_1+3x_2-0.5x_3&=&-2.7\\
-0.3x_1+x_2-7x_3&=&9.5
\end{eqnarray*}

\item \begin{eqnarray*}
-3x_2+7x_3&=&2\\
x_1+2x_2-x_3&=&3\\
12x_1+2x_2+2x_3&=&6
\end{eqnarray*}

\item \begin{eqnarray*}
0.15_1+2.11x_2+30.75x_3&=&-26.38\\
0.64x_1+1.21x_2+2.05x_3&=&1.01\\
3.21x_1+1.53x_2+1.04x_3&=&5.23
\end{eqnarray*}


\item \begin{eqnarray*}
x_1+x_2-x_3&=&-3\\
6x_1+2x_2+2x_3&=&2\\
-3x_1+4x_2+x_3&=&1
\end{eqnarray*}

\item \begin{eqnarray*}
2x_1+x_2-x3&=&1\\
5x_1+2x_2+2x_3&=&-4\\
3x_1+x_2+x_3&=&5
\end{eqnarray*}

\item \begin{eqnarray*}
3x-0.1y-0.2z&=&7.85\\
0.1x+7y-0.3z&=&-19.3\\
0.3x_1-0.2x_2+10x_3=71.4
\end{eqnarray*}


\item \begin{eqnarray*}
17x_1-2x_2-3x_3=500\\
-5x_1+21x_2-2x_3&=&200\\
-5x_1-5x_2+22x_3&=&30
\end{eqnarray*}

\end{enumerate}
\end{Ejer}



\begin{Ejer}
Aplicar el método de Jacobi para resolver los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item $A=\left(\begin{array}{cccc|c}\\
10 & 2 &  -1 &  0 &  26\\
1 & 20 & -2 & 3 & -15\\
-2 & 1 & 30 & 0 & 53\\
1 & 2 & 3 & 20 & 47
\end{array}\right)$


\item $A=\left(\begin{array}{ccc|c}\\
-1 & 2 & 10 & 11\\ 
11 & -1 & 2 & 12\\
1 & 5 & 2 & 8
\end{array}\right)$

\item $A=\left(\begin{array}{ccc|c}\\
8 & 2 & 3 & 51\\
2 & 5 & 1 & 23\\
-3 & 1 & 6 & 20
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
2 & -1 & 1 & 3 & 10\\
2 & 2 & 2 & 2 & 1\\
-1 & -1 & 2 & 2 & -5\\
3 & 1 & -1 & 4 & 6
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
3 & 1 & 1 & -1 & 5\\
0 & 2 & 1 & 4 & 0\\
1 & 1 & -1 & 9 & 1\\
2 & 4 & 6 & 3 & 0
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
10 & -1 & 2 & 0 & 6\\
-1 & 11 & -1 & 3 & 25\\
2 & -1 & 10 & -1 & -11\\
0 & 2 & -1 & 8 & 15
\end{array}\right)$

\item \begin{eqnarray*}
x_1+2x_2-2x_3&=&7\\
x_1+x_2+x_3&=&2\\
2x_1+2x_2+x_3&=&5
\end{eqnarray*}

\item \begin{eqnarray*}
-4x_1+14x_2=10\\
-5x_1+13x_2&=8\\
-x_1+2x_3&=&1
\end{eqnarray*}

\item \begin{eqnarray*}
x+y+2z&=&1\\
x+2y+z&=&1\\
2x+y+z&=&1
\end{eqnarray*}

\item \begin{eqnarray*}
6x_1-2x_2+2x_3+4x_4&=&10\\
12x_1-8x_2+6x_3+10x_4&=&20\\
3x_1-13x_2+9x_3+3x_4&=&2\\
-6x_1+4x_2+x_3-18x_4&=&-19
\end{eqnarray*}


\end{enumerate}
\end{Ejer}

\begin{Ejer}

\begin{enumerate}
  \item Resuelva el sistema
  \begin{eqnarray*}
  \begin{cases}
  2x+y-z=1,\\
  -x+3y+2z=12,\\
  x+2y+3z=7
  \end{cases}
  \end{eqnarray*}
  mediante eliminación gaussiana simple (sin pivoteo).

  \item Resuelva el mismo sistema anterior pero ahora aplicando eliminación gaussiana con pivoteo parcial. Compare los pasos con el ejercicio anterior.

  \item Aplique eliminación gaussiana con pivoteo y escalamiento al sistema
  \begin{eqnarray*}
  \begin{cases}
  10x+2y+z=7,\\
  2x+20y+2z=9,\\
  x+2y+30z=12
  \end{cases}
  \end{eqnarray*}
  y analice la importancia del escalamiento.

  \item Utilice el método de Gauss--Jordan para calcular la inversa de
  \begin{eqnarray*}
  A=\begin{bmatrix}
  1 & 2 & 1\\
  0 & 1 & -1\\
  2 & 3 & 4
  \end{bmatrix}.
  \end{eqnarray*}

  \item Resuelva $Ax=b$ con $A$ y $b$ dados por
  \begin{eqnarray*}
  A=\begin{bmatrix}
  4 & -2 & 1\\
  -2 & 4 & -2\\
  1 & -2 & 3
  \end{bmatrix},\qquad
  b=\begin{bmatrix}1\\4\\2\end{bmatrix},
  \end{eqnarray*}
  utilizando factorización $LU$ y sustitución hacia adelante y hacia atrás.

  \item Calcule la factorización de Cholesky de
  \begin{eqnarray*}
  A=\begin{bmatrix}
  25 & 15 & -5\\
  15 & 18 & 0\\
  -5 & 0 & 11
  \end{bmatrix}
  \end{eqnarray*}
  y resuelva $Ax=b$ con $b=(35,33,6)^\top$.
\end{enumerate}


\begin{enumerate}\setcounter{enumi}{6}
  \item Resuelva mediante sustitución hacia atrás el sistema triangular superior:
  \begin{eqnarray*}
  \begin{cases}
  2x+3y-z=5,\\
  -y+2z=4,\\
  3z=6.
  \end{cases}
  \end{eqnarray*}

  \item Resuelva mediante sustitución hacia adelante el sistema triangular inferior:
  \begin{eqnarray*}
  \begin{cases}
  x=3,\\
  2y+x=5,\\
  z-y+2x=10.
  \end{cases}
  \end{eqnarray*}

\end{enumerate}


\begin{enumerate}\setcounter{enumi}{9}
  \item Aplique el método de Jacobi para resolver
  \begin{eqnarray*}
  \begin{cases}
  10x-y+2z=6,\\
  -x+11y-z+3w=25,\\
  2x-y+10z-w=-11,\\
  3y-z+8w=15,
  \end{cases}
  \end{eqnarray*}
  realizando 3 iteraciones con $x^{(0)}=\mathbf{0}$.

  \item Repita el ejercicio anterior con el método de Gauss--Seidel. Compare la velocidad de convergencia con Jacobi.

  \item Aplique el método SOR con $\omega=1.25$ al mismo sistema y compare las tres trayectorias de convergencia.

  \item Escriba la matriz de iteración $T_J$ y el vector $c_J$ del método de Jacobi para el sistema
  \begin{eqnarray*}
  \begin{cases}
  4x+y=9,\\
  x+3y=7.
  \end{cases}
  \end{eqnarray*}
  Verifique si $\rho(T_J)<1$.

  \item Investigue experimentalmente en R cuál es el valor óptimo aproximado de $\omega$ para el método SOR en el sistema $4\times 4$ del ejercicio 10.
\end{enumerate}


\begin{enumerate}\setcounter{enumi}{14}
  \item Genere una matriz aleatoria simétrica definida positiva $5\times 5$ en R y resuelva $Ax=b$:
 \begin{itemize}
    \item[(a)] Usando factorización $LU$.  
    \item[(b)] Usando factorización de Cholesky.  
    \item[(c)] Usando Gauss--Seidel con 20 iteraciones.  
  \end{itemize}
  Compare el tiempo de cómputo y la precisión de cada método.
\end{enumerate}
\end{Ejer}


\newpage

%===========================================
\section{Raíces de Funciones}
%===========================================

%===========================================
\subsection{M\'etodo de Bisecci\'on}
%===========================================

Dada una función continua $f:[a,b]\to\mathbb{R}$ con $f(a)\,f(b)<0$, por el \textbf{Teorema de Bolzano} existe al menos una raíz en $(a,b)$. 
El \textbf{método de bisección} explota este cambio de signo: divide el intervalo a la mitad, selecciona la submitad donde persiste el cambio de signo, y repite. 
Es un método \emph{robusto} (garantiza convergencia si se mantienen las hipótesis) y con \emph{tasa lineal}.

\begin{Algthm} Para encontrar la ra\'iz de una funci\'on utilizando el m\'etodo de la bisecci\'on es
\begin{enumerate}
  \item Verificar continuidad de $f$ en $[a,b]$ (al menos de forma razonable) y que $f(a)\,f(b)<0$.
  \item Repetir para $n=1,2,\dots$:
  \begin{enumerate}
    \item $c=\dfrac{a+b}{2}$, evaluar $f(c)$.
    \item Si $f(c)=0$ (o $|f(c)|$ pequeño), \textbf{parar}: $c$ es la raíz aproximada.
    \item Si $f(a)\,f(c)<0$, poner $b\leftarrow c$; en caso contrario, $a\leftarrow c$.
    \item Criterio de paro: \textit{ancho} $(b-a)/2 < \varepsilon$ o $|f(c)|<\varepsilon$, o alcanzar $n_{\max}$.
  \end{enumerate}
\end{enumerate}
\end{Algthm}

\textbf{Cota del error}: Si $(a_0,b_0)$ es el intervalo inicial, tras $n$ iteraciones el punto medio $c_n$ satisface
\begin{eqnarray}
  |c_n - \alpha| \leq \frac{b_0-a_0}{2^{\,n}},
\end{eqnarray}
donde $\alpha$ es alguna raíz en $(a_0,b_0)$. 
Para asegurar $(b_n-a_n)/2 < \varepsilon$, basta con
\begin{eqnarray}
  n \geq \left\lceil \log_2\!\left(\frac{b_0-a_0}{\varepsilon}\right) \right\rceil.
\end{eqnarray}

\begin{Result}
Si $f(x)$ es continua en $[a,b]$ y $f(a)\,f(b)<0$, por el \textbf{Teorema de Bolzano} existe al menos una raíz en $(a,b)$.  
El método de bisección consiste en:
\begin{eqnarray}
c_k = \frac{a_k + b_k}{2}, \qquad
\text{y se actualiza el intervalo de b\'usqueda } 
\begin{cases}
b_{k+1} = c_k & \text{si } f(a_k)f(c_k) < 0,\\
a_{k+1} = c_k & \text{si } f(a_k)f(c_k) > 0.
\end{cases}
\end{eqnarray}
La cota del error después de $n$ iteraciones es
\begin{eqnarray}
|c_n - \alpha| \le \frac{b_0 - a_0}{2^n}.
\end{eqnarray}
\end{Result}

\begin{Ejem}
\begin{verbatim}
# ============================================
# MÉTODO DE BISECCIÓN PARA UNA CÚBICA GENERAL
# ============================================
# ---- Coeficientes del polinomio cúbico ----
A <- 1     # coeficiente de x^3
B <- -6    # coeficiente de x^2
C <- 11    # coeficiente de x
D <- -6    # término independiente

# ---- Definición de la función cúbica ----
f <- function(x) {
  A*x^3 + B*x^2 + C*x + D
}

# ---- Intervalo inicial ----
a1 <- 0
b1 <- 2.5   # asegúrate que f(a1)*f(b1) < 0

# ==== Iteración 1 ====
c1  <- (a1 + b1)/2
fa1 <- f(a1)
fb1 <- f(b1)
fc1 <- f(c1)
s1a <- sign(fa1 * fc1)
s1b <- sign(fb1 * fc1)
e1  <- (b1 - a1)/2
a2 <- if (fa1 * fc1 < 0) a1 else c1
b2 <- if (fa1 * fc1 < 0) c1 else b1

# ==== Iteración 2 ====
c2  <- (a2 + b2)/2
fa2 <- f(a2)
fb2 <- f(b2)
fc2 <- f(c2)
s2a <- sign(fa2 * fc2)
s2b <- sign(fb2 * fc2)
e2  <- (b2 - a2)/2
a3 <- if (fa2 * fc2 < 0) a2 else c2
b3 <- if (fa2 * fc2 < 0) c2 else b2

# ==== Iteración 3 ====
c3  <- (a3 + b3)/2
fa3 <- f(a3)
fb3 <- f(b3)
fc3 <- f(c3)
s3a <- sign(fa3 * fc3)
s3b <- sign(fb3 * fc3)
e3  <- (b3 - a3)/2
a4 <- if (fa3 * fc3 < 0) a3 else c3
b4 <- if (fa3 * fc3 < 0) c3 else b3

# ==== Iteración 4 ====
c4  <- (a4 + b4)/2
fa4 <- f(a4)
fb4 <- f(b4)
fc4 <- f(c4)
s4a <- sign(fa4 * fc4)
s4b <- sign(fb4 * fc4)
e4  <- (b4 - a4)/2
a5 <- if (fa4 * fc4 < 0) a4 else c4
b5 <- if (fa4 * fc4 < 0) c4 else b4

# ==== Iteración 5 ====
c5  <- (a5 + b5)/2
fa5 <- f(a5)
fb5 <- f(b5)
fc5 <- f(c5)
s5a <- sign(fa5 * fc5)
s5b <- sign(fb5 * fc5)
e5  <- (b5 - a5)/2
a6 <- if (fa5 * fc5 < 0) a5 else c5
b6 <- if (fa5 * fc5 < 0) c5 else b5

# ==== Iteración 6 ====
c6  <- (a6 + b6)/2
fa6 <- f(a6)
fb6 <- f(b6)
fc6 <- f(c6)
s6a <- sign(fa6 * fc6)
s6b <- sign(fb6 * fc6)
e6  <- (b6 - a6)/2

# ---- Aproximación final ----
raiz_aprox <- c6
error_cota <- e6

cat("Raíz aproximada (6 iteraciones):", raiz_aprox, "\n")
cat("Cota máxima del error:", error_cota, "\n\n")

# ---- Tabla resumen ----
iter <- 1:6
Acol <- c(a1,a2,a3,a4,a5,a6)
Bcol <- c(b1,b2,b3,b4,b5,b6)
Ccol <- c(c1,c2,c3,c4,c5,c6)
FA   <- c(fa1,fa2,fa3,fa4,fa5,fa6)
FB   <- c(fb1,fb2,fb3,fb4,fb5,fb6)
FC   <- c(fc1,fc2,fc3,fc4,fc5,fc6)
signFAFC <- c(s1a,s2a,s3a,s4a,s5a,s6a)
signFBFC <- c(s1b,s2b,s3b,s4b,s5b,s6b)
ERR  <- c(e1,e2,e3,e4,e5,e6)

TAB <- data.frame(iter, a=Acol, b=Bcol, c=Ccol,
                  fa=FA, fb=FB, fc=FC,
                  "sign(fa*fc)"=signFAFC,
                  "sign(fb*fc)"=signFBFC,
                  error=ERR)

print(round(TAB, 6))

\end{verbatim}
\end{Ejem}
%===========================================
\subsubsection{Ejemplos}
%===========================================

\begin{Ejem}
Consideremos la funci\'on $f(x) = x^2 - 5x + 6$. para $a=1,b=-5,c=6$.  

\begin{itemize}
\item Verificar cambio de signo.  
Evaluamos:
$f(1) = 1 - 5 + 6 = 2 > 0, \qquad f(2.5) = 6.25 - 12.5 + 6 = -0.25 < 0.$
Existe cambio de signo entre $x=1$ y $x=2.5$, por lo tanto, hay al menos una raíz en $[1,2.5]$.

\item  El punto medio inicial es: $c_1 = \frac{1+2.5}{2} = 1.75, \quad f(1.75) = 3,0625 - 8,75 + 6 = 0,3125 > 0$. Como $f(1.75)>0$ y $f(2.5)<0$, el nuevo intervalo es $[1.75,2.5]$.

\item Segunda iteración: $c_2 = \frac{1.75+2.5}{2} = 2.125, \quad f(2.125) = 4,5156 - 10,625 + 6 = -0.1094 < 0$. Nuevo intervalo: $[1.75,2.125]$.

\item Tercera iteración: $c_3 = \frac{1.75+2.125}{2} = 1.9375, \quad f(1.9375) = 3,754 - 9,6875 + 6 = 0.0665 > 0$. Nuevo intervalo: $[1.9375,2.125]$.

\item  Cuarta iteración:$c_4 = \frac{1.9375+2.125}{2} = 2.03125, \quad f(2.03125) = 4,126 - 10,156 + 6 = -0.030 < 0$. Nuevo intervalo: $[1.9375,2.03125]$.

\item Después de unas pocas iteraciones, la raíz se aproxima a $x\approx 2.0$.

\begin{center}
\begin{tabular}{@{}rrrrr@{}}
Iteración & $a_n$ & $b_n$ & $c_n$ & $f(c_n)$ \\
1 & 1.000 & 2.500 & 1.750 & $+0.3125$ \\
2 & 1.750 & 2.500 & 2.125 & $-0.1094$ \\
3 & 1.750 & 2.125 & 1.9375 & $+0.0665$ \\
4 & 1.9375 & 2.125 & 2.0313 & $-0.030$ \\
5 & 1.9375 & 2.0313 & 1.9844 & $+0.017$ \\
\end{tabular}
\end{center}

\end{itemize}
\end{Ejem}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Ejem} Sea $f(x)=x^2-2$ con $a=1$, $b=-2$. Entonces $f(1)=-1<0$, $f(2)=2>0$ $\Rightarrow$ hay raíz en $[1,2]$.

\begin{center}
\begin{tabular}{@{}rrrrr@{}}
Iter. & $a_n$ & $b_n$ & $c_n$ & $f(c_n)$ \\
1 & 1.000000 & 2.000000 & 1.500000 & $+0.250000$ \\
2 & 1.000000 & 1.500000 & 1.250000 & $-0.437500$ \\
3 & 1.250000 & 1.500000 & 1.375000 & $-0.109375$ \\
4 & 1.375000 & 1.500000 & 1.437500 & $+0.066406$ \\
5 & 1.375000 & 1.437500 & 1.406250 & $-0.022461$ \\
6 & 1.406250 & 1.437500 & 1.421875 & $+0.021729$ \\
7 & 1.406250 & 1.421875 & 1.414063 & $-0.000427$ \\
8 & 1.414063 & 1.421875 & 1.417969 & $+0.010635$ \\
\end{tabular}
\end{center}
\end{Ejem}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Ejem} Sea la funci\'on $f(x) = x^3 - 6x^2 + 11x - 6$, con $a=1, b=-6,c=11,d=-6$, en el intervalo $[1.5,2.5]$.
\begin{itemize}


\item Verificar cambio de signo.
$f(1.5) = 1.5^3 - 6(1.5)^2 + 11(1.5) - 6 = 3.375 - 13.5 + 16.5 - 6 = 0.375 > 0$, $f(2.5) = 15.625 - 37.5 + 27.5 - 6 = -0.375 < 0$. Existe cambio de signo, por lo que hay al menos una raíz en el intervalo $[1.5,2.5]$.

\item Aplicar el método de bisección. $c_1 = \frac{1.5+2.5}{2} = 2.0, \qquad f(2.0) = 8 - 24 + 22 - 6 = 0$. Se obtiene exactamente la raíz en la primera iteración.

\item Para ilustrar mejor, tomemos el intervalo más amplio $[1,3]$: $f(1) = 0, \quad f(3) = 0$. Aquí no hay cambio de signo estricto (ya que ambos extremos son raíces), por tanto el método se aplica en un subintervalo, por ejemplo $[1.2, 2.8]$.

\item Iteraciones del método:

\begin{center}
\begin{tabular}{@{}rrrrr@{}}
Iteración & $a_n$ & $b_n$ & $c_n$ & $f(c_n)$ \\
1 & 1.200 & 2.800 & 2.000 & $0.000$ \\
\end{tabular}
\end{center}

\item Nuevas evaluaciones:
$f(1.5) = 3.375 - 13.5 + 16.5 - 5.5 = 0.875 > 0$, $f(2.5) = 15.625 - 37.5 + 27.5 - 5.5 = 0.125 > 0$, $f(1.0) = 1 - 6 + 11 - 5.5 = 0.5 > 0$, $f(3.0) = 27 - 54 + 33 - 5.5 = 0.5 > 0$.

\item Busquemos ahora en el intervalo $[1.5, 2.5]$ ajustando un poco los valores hasta encontrar cambio de signo: $f(1.7) \approx 0.28$, $f(1.9) \approx 0.03$, $f(2.0) \approx -0.5$. Entonces hay cambio de signo en $[1.8, 2.0]$.

\begin{center}
\begin{tabular}{@{}rrrrr@{}}
%toprule
Iteración & $a_n$ & $b_n$ & $c_n$ & $f(c_n)$ \\
1 & 1.800 & 2.000 & 1.900 & $+0.030$ \\
2 & 1.900 & 2.000 & 1.950 & $-0.210$ \\
3 & 1.800 & 1.950 & 1.875 & $+0.130$ \\
4 & 1.875 & 1.950 & 1.9125 & $-0.040$ \\
5 & 1.875 & 1.9125 & 1.8937 & $+0.040$ \\
6 & 1.8937 & 1.9125 & 1.9031 & $-0.005$ \\
\end{tabular}
\end{center}

La raíz aproximada se encuentra en $x \approx 1.903$.
\end{itemize}
\end{Ejem}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Ejem}
Consideremos la cúbica $f(x)=x^3-6x^2+11x-5.5$, con intervalo inicial $[a_0,b_0]=[0.8,0.9]$ donde hay cambio de signo. Las primeras 8 iteraciones (valores redondeados) son:

\begin{center}
\begin{tabular}{@{}rrrrr@{}}
Iter. & $a_n$ & $b_n$ & $c_n$ & $f(c_n)$ \\
1 & 0.8000000 & 0.9000000 & 0.8500000 & $+1.29125\times10^{-1}$ \\
2 & 0.8000000 & 0.8500000 & 0.8250000 & $+5.27656\times10^{-2}$ \\
3 & 0.8000000 & 0.8250000 & 0.8125000 & $+1.29395\times10^{-2}$ \\
4 & 0.8000000 & 0.8125000 & 0.8062500 & $-7.39038\times10^{-3}$ \\
5 & 0.8062500 & 0.8125000 & 0.8093750 & $+2.80942\times10^{-3}$ \\
6 & 0.8062500 & 0.8093750 & 0.8078125 & $-2.28175\times10^{-3}$ \\
7 & 0.8078125 & 0.8093750 & 0.8085938 & $+2.66016\times10^{-4}$ \\
8 & 0.8078125 & 0.8085938 & 0.8082031 & $-1.00732\times10^{-3}$ \\
\end{tabular}
\end{center}

Después de 8 iteraciones, la aproximación de la raíz es $c_8 \approx 0{,}8082031$ y la cota de error por ancho de intervalo es
$\frac{b_8-a_8}{2} = \texttt{error\_cota} = \frac{0.8085938-0.8078125}{2} \approx 3{,}906\times10^{-4}.$

\end{Ejem}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Ejem}

Sea $f(x)=x^2 - 2x$ con $a=1$ y $b=-3$ pues $ax^2+bx+x=x^2-3x+x=x^2-2x$. Usamos el intervalo $[1.2,3.0]$: $f(1.2)=1.44-2.4=-0.96<0$, $f(3)=9-6=3>0$.

\begin{center}
\begin{tabular}{@{}rrrrr@{}}
Iter. & $a_n$ & $b_n$ & $c_n$ & $f(c_n)$ \\
1 & 1.200000 & 3.000000 & 2.100000 & $+0.210000$ \\
2 & 1.200000 & 2.100000 & 1.650000 & $-0.577500$ \\
3 & 1.650000 & 2.100000 & 1.875000 & $-0.234375$ \\
4 & 1.875000 & 2.100000 & 1.987500 & $-0.024844$ \\
5 & 1.987500 & 2.100000 & 2.043750 & $+0.089414$ \\
6 & 1.987500 & 2.043750 & 2.015625 & $+0.031494$ \\
7 & 1.987500 & 2.015625 & 2.001563 & $+0.003127$ \\
8 & 1.987500 & 2.001563 & 1.994531 & $-0.010908$ \\
\end{tabular}
\end{center}
\end{Ejem}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Ejem} Sea la función $f(x)=x^2-2x$ en el intervalo elegido: $[1.2,3.0]$  

\begin{itemize}
\item $f(1.2)=1.44-2.4=-0.96, \quad f(3)=9-6=3$.

\item Iteración 1: $c_1=\frac{1.2+3}{2}=2.1, \quad f(2.1)=4.41-4.2=0.21>0 $ entonces $[1.2,2.1]$.

\item Iteración 2: $c_2=\frac{1.2+2.1}{2}=1.65, \quad f(1.65)=2.7225-3.3=-0.5775<0 $ entonces $[1.65,2.1]$.

\item Iteración 3: $c_3=1.875, \quad f(1.875)=3.516-3.75=-0.234<0 $ entonces $[1.875,2.1]$. Y así sucesivamente hasta $c_8\approx1.9945$.

\begin{center}
\begin{tabular}{@{}rrrrr@{}}
$n$ & $a_n$ & $b_n$ & $c_n$ & $f(c_n)$ \\
1 & 1.200 & 3.000 & 2.100 & +0.210 \\
2 & 1.200 & 2.100 & 1.650 & -0.577 \\
3 & 1.650 & 2.100 & 1.875 & -0.234 \\
4 & 1.875 & 2.100 & 1.987 & -0.024 \\
5 & 1.987 & 2.100 & 2.043 & +0.089 \\
6 & 1.987 & 2.043 & 2.015 & +0.031 \\
7 & 1.987 & 2.015 & 2.001 & +0.003 \\
8 & 1.987 & 2.001 & 1.994 & -0.011 \\
\end{tabular}
\end{center}
\end{itemize}
\end{Ejem}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Ejem}
$f(x)=x^3-x-2$ en $[1,2]$. Se verifica $f(1)=-2<0$ y $f(2)=4>0$.

Primeras iteraciones (hasta alcanzar $10^{-6}$ típicamente se requieren $\sim 20$ pasos desde $[1,2]$):

\begin{table}[H]
\centering
\caption{Método de bisección para $f(x)=x^3-x-2$, intervalo inicial $[-3,2]$.}
\scalebox{0.8}{
\begin{tabular}{|c|r|r|r|r|r|r|c|c|}
\hline
Iter. & $a$ & $b$ & $c=\tfrac{a+b}{2}$ & $f(a)$ & $f(b)$ & $f(c)$ & $\mathrm{sign}(f(a)f(c))$ & $\mathrm{sign}(f(b)f(c))$ \\
\hline
1  & -3.000000000 &  2.000000000 & -0.500000000 & -26.000000000 &  4.000000000 & -1.625000000 & $+$ & $-$ \\
2  & -0.500000000 &  2.000000000 &  0.750000000 & -1.625000000  &  4.000000000 & -2.328125000 & $+$ & $-$ \\
3  &  0.750000000 &  2.000000000 &  1.375000000 & -2.328125000  &  4.000000000 & -0.775390625 & $+$ & $-$ \\
4  &  1.375000000 &  2.000000000 &  1.687500000 & -0.775390625  &  4.000000000 &  1.117919922 & $-$ & $+$ \\
5  &  1.375000000 &  1.687500000 &  1.531250000 & -0.775390625  &  1.117919922 &  0.059112549 & $-$ & $+$ \\
6  &  1.375000000 &  1.531250000 &  1.453125000 & -0.775390625  &  0.059112549 & -0.392456055 & $+$ & $-$ \\
7  &  1.453125000 &  1.531250000 &  1.492187500 & -0.392456055  &  0.059112549 & -0.172897339 & $+$ & $-$ \\
8  &  1.492187500 &  1.531250000 &  1.511718750 & -0.172897339  &  0.059112549 & -0.058979273 & $+$ & $-$ \\
9  &  1.511718750 &  1.531250000 &  1.521484375 & -0.058979273  &  0.059112549 &  0.000622176 & $-$ & $+$ \\
10 &  1.511718750 &  1.521484375 &  1.516601562 & -0.058979273  &  0.000622176 & -0.029292628 & $+$ & $-$ \\
11 &  1.516601562 &  1.521484375 &  1.519042969 & -0.029292628  &  0.000622176 & -0.013864168 & $+$ & $-$ \\
12 &  1.519042969 &  1.521484375 &  1.520263672 & -0.013864168  &  0.000622176 & -0.006627792 & $+$ & $-$ \\
\hline
\end{tabular}}
\end{table}

Con tolerancia $\varepsilon=10^{-6}$, el resultado se estabiliza alrededor de $\alpha \approx 1{,}52138$.
\end{Ejem}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Ejem}
$f(x)=\cos x - x$ en $[0,1]$. Se verifica $f(0)=1>0$ y $f(1)\approx -0{,}4597<0$.

Primeras iteraciones:
\begin{table}[H]
\centering
\caption{Método de bisección para $f(x)=\cos x - x$ en $[0,1]$.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|r|r|r|r|r|r|c|c|}
\hline
Iter. & $a$ & $b$ & $c=\tfrac{a+b}{2}$ & $f(a)$ & $f(b)$ & $f(c)$ & $\mathrm{sign}(f(a)f(c))$ & $\mathrm{sign}(f(b)f(c))$ \\
\hline
1 & 0.000000000 & 1.000000000 & 0.500000000 & 1.000000000 & -0.459697694 & 0.377582562 & $-$ & $+$ \\
2 & 0.500000000 & 1.000000000 & 0.750000000 & 0.377582562 & -0.459697694 & -0.018311132 & $+$ & $-$ \\
3 & 0.500000000 & 0.750000000 & 0.625000000 & 0.377582562 & -0.018311132 & 0.185963120 & $-$ & $+$ \\
4 & 0.625000000 & 0.750000000 & 0.687500000 & 0.185963120 & -0.018311132 & 0.085334946 & $-$ & $+$ \\
5 & 0.687500000 & 0.750000000 & 0.718750000 & 0.085334946 & -0.018311132 & 0.033879372 & $-$ & $+$ \\
6 & 0.718750000 & 0.750000000 & 0.734375000 & 0.033879372 & -0.018311132 & 0.007874725 & $-$ & $+$ \\
7 & 0.734375000 & 0.750000000 & 0.742187500 & 0.007874725 & -0.018311132 & -0.005195711 & $+$ & $-$ \\
8 & 0.734375000 & 0.742187500 & 0.738281250 & 0.007874725 & -0.005195711 & 0.001345150 & $-$ & $+$ \\
9 & 0.738281250 & 0.742187500 & 0.740234375 & 0.001345150 & -0.005195711 & -0.001923872 & $+$ & $-$ \\
10 & 0.738281250 & 0.740234375 & 0.739257812 & 0.001345150 & -0.001923872 & -0.000289009 & $+$ & $-$ \\
11 & 0.738281250 & 0.739257812 & 0.738769531 & 0.001345150 & -0.000289009 & 0.000528158 & $-$ & $+$ \\
12 & 0.738769531 & 0.739257812 & 0.739013672 & 0.000528158 & -0.000289009 & 0.000119610 & $-$ & $+$ \\
\hline
\end{tabular}
}
\end{table}

Con $\varepsilon=10^{-6}$ se obtiene aproximadamente $\alpha \approx 0{,}739085$.
\end{Ejem}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>

%===========================================
%\subsubsection{Ejercicios}
%===========================================

\begin{Ejem}
Aplica el m\'etodo de la bisecci\'on para las siguientes funciones:
\begin{enumerate}
  \item Encuentra una raíz de $x^3-6x+2$ en $[0,2]$.
  
\begin{table}[H]
\centering
\caption{M\'etodo de la Bisección para $f(x)=x^3-6x+2$ en $[0,2]$ (12 iteraciones).}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|r|r|r|r|r|r|c|c|}
\hline
Iter. & $a$ & $b$ & $c=\tfrac{a+b}{2}$ & $f(a)$ & $f(b)$ & $f(c)$ & $\mathrm{sign}(f(a)f(c))$ & $\mathrm{sign}(f(b)f(c))$ \\
\hline
1 & 0.000000000 & 2.000000000 & 1.000000000 & 2.000000000 & -2.000000000 & -3.000000000 & $-$ & $+$ \\
2 & 0.000000000 & 1.000000000 & 0.500000000 & 2.000000000 & -3.000000000 & -0.875000000 & $-$ & $+$ \\
3 & 0.000000000 & 0.500000000 & 0.250000000 & 2.000000000 & -0.875000000 & 0.515625000 & $+$ & $-$ \\
4 & 0.250000000 & 0.500000000 & 0.375000000 & 0.515625000 & -0.875000000 & -0.197265625 & $-$ & $+$ \\
5 & 0.250000000 & 0.375000000 & 0.312500000 & 0.515625000 & -0.197265625 & 0.155517578 & $+$ & $-$ \\
6 & 0.312500000 & 0.375000000 & 0.343750000 & 0.155517578 & -0.197265625 & -0.021881104 & $-$ & $+$ \\
7 & 0.312500000 & 0.343750000 & 0.328125000 & 0.155517578 & -0.021881104 & 0.066577911 & $+$ & $-$ \\
8 & 0.328125000 & 0.343750000 & 0.335937500 & 0.066577911 & -0.021881104 & 0.022286892 & $+$ & $-$ \\
9 & 0.335937500 & 0.343750000 & 0.339843750 & 0.022286892 & -0.021881104 & 0.000187337 & $+$ & $-$ \\
10 & 0.339843750 & 0.343750000 & 0.341796875 & 0.000187337 & -0.021881104 & -0.010850795 & $-$ & $+$ \\
11 & 0.339843750 & 0.341796875 & 0.340820312 & 0.000187337 & -0.010850795 & -0.005332704 & $-$ & $+$ \\
12 & 0.339843750 & 0.340820312 & 0.340332031 & 0.000187337 & -0.005332704 & -0.002572927 & $-$ & $+$ \\
\hline
\end{tabular}
}
\end{table}

  \item Estudia $f(x)=e^{-x}-x$ en $[0,1]$. 
\begin{table}[H]
\centering
\caption{M\'etodo de Bisección para $f(x)=e^{-x}-x$ en $[0,1]$ (12 iteraciones).}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|r|r|r|r|r|r|c|c|}
\hline
Iter. & $a$ & $b$ & $c=\tfrac{a+b}{2}$ & $f(a)$ & $f(b)$ & $f(c)$ & $\mathrm{sign}(f(a)f(c))$ & $\mathrm{sign}(f(b)f(c))$ \\
\hline
1 & 0.000000000 & 1.000000000 & 0.500000000 & 1.000000000 & -0.632120559 & 0.106530660 & $+$ & $-$ \\
2 & 0.500000000 & 1.000000000 & 0.750000000 & 0.106530660 & -0.632120559 & -0.277633447 & $-$ & $+$ \\
3 & 0.500000000 & 0.750000000 & 0.625000000 & 0.106530660 & -0.277633447 & -0.089738571 & $-$ & $+$ \\
4 & 0.500000000 & 0.625000000 & 0.562500000 & 0.106530660 & -0.089738571 & 0.007282825 & $+$ & $-$ \\
5 & 0.562500000 & 0.625000000 & 0.593750000 & 0.007282825 & -0.089738571 & -0.041497550 & $-$ & $+$ \\
6 & 0.562500000 & 0.593750000 & 0.578125000 & 0.007282825 & -0.041497550 & -0.017175839 & $-$ & $+$ \\
7 & 0.562500000 & 0.578125000 & 0.570312500 & 0.007282825 & -0.017175839 & -0.004963760 & $-$ & $+$ \\
8 & 0.562500000 & 0.570312500 & 0.566406250 & 0.007282825 & -0.004963760 & 0.001155202 & $+$ & $-$ \\
9 & 0.566406250 & 0.570312500 & 0.568359375 & 0.001155202 & -0.004963760 & -0.001905360 & $-$ & $+$ \\
10 & 0.566406250 & 0.568359375 & 0.567382812 & 0.001155202 & -0.001905360 & -0.000375349 & $-$ & $+$ \\
11 & 0.566406250 & 0.567382812 & 0.566894531 & 0.001155202 & -0.000375349 & 0.000389859 & $+$ & $-$ \\
12 & 0.566894531 & 0.567382812 & 0.567138672 & 0.000389859 & -0.000375349 & 0.000007238 & $+$ & $-$ \\
\hline
\end{tabular}
}
\end{table}

\item Sea la función cuadrática $f(x)=x^2-4$ en $(-1,4)$.
\begin{table}[H]
\centering
\caption{M\'etodo de Bisección para $f(x)=x^2-4$ en $(-1,4)$ (12 iteraciones).}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|r|r|r|r|r|r|c|c|}
\hline
Iter. & $a$ & $b$ & $c=\tfrac{a+b}{2}$ & $f(a)$ & $f(b)$ & $f(c)$ & $\mathrm{sign}(f(a)f(c))$ & $\mathrm{sign}(f(b)f(c))$ \\
\hline
1 & -1.000000000 & 4.000000000 & 1.500000000 & -3.000000000 & 12.000000000 & -1.750000000 & $+$ & $-$ \\
2 & 1.500000000 & 4.000000000 & 2.750000000 & -1.750000000 & 12.000000000 & 3.562500000 & $-$ & $+$ \\
3 & 1.500000000 & 2.750000000 & 2.125000000 & -1.750000000 & 3.562500000 & 0.515625000 & $-$ & $+$ \\
4 & 1.500000000 & 2.125000000 & 1.812500000 & -1.750000000 & 0.515625000 & -0.714843750 & $+$ & $-$ \\
5 & 1.812500000 & 2.125000000 & 1.968750000 & -0.714843750 & 0.515625000 & -0.122558594 & $+$ & $-$ \\
6 & 1.968750000 & 2.125000000 & 2.046875000 & -0.122558594 & 0.515625000 & 0.189208984 & $-$ & $+$ \\
7 & 1.968750000 & 2.046875000 & 2.007812500 & -0.122558594 & 0.189208984 & 0.031494141 & $-$ & $+$ \\
8 & 1.968750000 & 2.007812500 & 1.988281250 & -0.122558594 & 0.031494141 & -0.063354492 & $+$ & $-$ \\
9 & 1.988281250 & 2.007812500 & 1.998046875 & -0.063354492 & 0.031494141 & -0.007873535 & $+$ & $-$ \\
10 & 1.998046875 & 2.007812500 & 2.002929688 & -0.007873535 & 0.031494141 & 0.011726379 & $-$ & $+$ \\
11 & 1.998046875 & 2.002929688 & 2.000488281 & -0.007873535 & 0.011726379 & 0.001953602 & $-$ & $+$ \\
12 & 1.998046875 & 2.000488281 & 1.999267578 & -0.007873535 & 0.001953602 & -0.002960747 & $+$ & $-$ \\
\hline
\end{tabular}
}
\end{table}
\end{enumerate}
\end{Ejem}


%========================================
\subsubsection{Implementaciones num\'ericas}
%========================================
\begin{Algthm}
Pseudoc\'odigo

\begin{verbatim}
Inicio
    Definir f(x)
    Leer a, b, tolerancia, iter_max

    Si f(a)*f(b) > 0 Entonces
        Imprimir "No hay cambio de signo en [a,b]"
        Terminar
    FinSi

    iter ← 0
    error ← |b - a|

    Mientras (error > tolerancia) Y (iter < iter_max) Hacer
        c ← (a + b)/2

        Si f(a)*f(c) < 0 Entonces
            b ← c
        Sino
            a ← c
        FinSi

        error ← |b - a|
        iter ← iter + 1
    FinMientras

    Imprimir "Raíz aproximada:", c
    Imprimir "Iteraciones:", iter
Fin

\end{verbatim}


Implementaci\'on num\'erica

\begin{verbatim}
# Método de Bisección
biseccion <- function(f, a, b, tol = 1e-6, iter_max = 100){
  if(f(a)*f(b) > 0){
    cat("Error: no hay cambio de signo en el intervalo [a,b].\n")
    return(NA)
  }
  
  iter <- 0
  error <- abs(b - a)
  
  while(error > tol && iter < iter_max){
    c <- (a + b)/2
    
    if(f(a)*f(c) < 0){
      b <- c
    } else {
      a <- c
    }
    
    error <- abs(b - a)
    iter <- iter + 1
  }
  
  cat("Raíz aproximada:", c, "\n")
  cat("Iteraciones:", iter, "\n")
  return(c)
}

# Ejemplo de uso:
# f(x) = x^3 - x - 2  
f <- function(x) x^3 - x - 2
biseccion(f, a = 1, b = 2)
\left 
\end{verbatim}
\end{Algthm}

%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Algthm}
\begin{verbatim}
# Definir f(x) (la función matemática a evaluar)
f <- function(x) x^3 - 6*x^2 + 11*x - 5.5

# Intervalo inicial con cambio de signo
a <- 0.8;  b <- 0.9
fa <- f(a); fb <- f(b)
if (fa * fb >= 0) stop("El intervalo inicial no tiene cambio de signo.")

# Estructura para guardar el historial (n, a, b, c, f(c), ancho)
hist <- data.frame(
  n = integer(), a = numeric(), b = numeric(),
  c = numeric(), fc = numeric(), ancho = numeric(),
  stringsAsFactors = FALSE
)

# Ejecutar exactamente 8 iteraciones, sin usar ninguna funcion/recursividad
for (n in 1:8) {
  c  <- (a + b)/2
  fc <- f(c)
  ancho <- (b - a)/2
  
  # Guardar fila
  hist[n,] <- list(n, a, b, c, fc, ancho)
  
  # Mostrar en consola (opcional)
  cat(sprintf("Iter %2d | a=%.7f b=%.7f c=%.7f f(c)=%.7e ancho=%.7e\n",
              n, a, b, c, fc, ancho))
  
  # Actualizar el intervalo según el signo
  if (fa * fc < 0) {
    b <- c; fb <- fc
  } else {
    a <- c; fa <- fc
  }
}

# Resultado aproximado tras 8 iteraciones:
c_aprox <- hist$c[8]
error_cota <- hist$ancho[8]  # (b - a)/2 después del paso 8
c_aprox; error_cota
hist
\end{verbatim}
\end{Algthm}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>

%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Algthm}
\begin{verbatim}
f <- function(x) x^2 - 2
a <- 1.0; b <- 2.0
fa <- f(a); fb <- f(b)
if (fa*fb >= 0) stop("Sin cambio de signo en [a,b].")
histA <- data.frame(n=integer(), a=numeric(), b=numeric(),
                    c=numeric(), fc=numeric(), ancho=numeric())
for (n in 1:8) {
  c  <- (a + b)/2
  fc <- f(c)
  histA[n,] <- list(n, a, b, c, fc, (b-a)/2)
  cat(sprintf("A-%2d | a=%.6f b=%.6f c=%.6f f(c)=%.6e ancho=%.6e\n",
              n, a, b, c, fc, (b-a)/2))
  if (fa * fc < 0) { b <- c; fb <- fc } else { a <- c; fa <- fc }
}
\end{verbatim}
\end{Algthm}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>

%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\begin{Algthm}
\begin{verbatim}
f <- function(x) x^2 - 2*x
a <- 1.2; b <- 3.0
fa <- f(a); fb <- f(b)
if (fa*fb >= 0) stop("Sin cambio de signo en [a,b].")
histB <- data.frame(n=integer(), a=numeric(), b=numeric(),
                    c=numeric(), fc=numeric(), ancho=numeric())
for (n in 1:8) {
  c  <- (a + b)/2
  fc <- f(c)
  histB[n,] <- list(n, a, b, c, fc, (b-a)/2)
  cat(sprintf("B-%2d | a=%.6f b=%.6f c=%.6f f(c)=%.6e ancho=%.6e\n",
              n, a, b, c, fc, (b-a)/2))
  if (fa * fc < 0) { b <- c; fb <- fc } else { a <- c; fa <- fc }
}
\end{verbatim}
\end{Algthm}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\subsection{M\'etodo de la Secante}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>

El método de la secante aproxima la derivada mediante la pendiente de la recta que une dos puntos de la curva, evitando calcular $f'(x)$.
\begin{Algthm}
Dado $x_{k-1}$ y $x_k$, definimos
\begin{eqnarray*}
x_{k+1} \;=\; x_k \;-\; f(x_k)\,\frac{x_k-x_{k-1}}{\,f(x_k)-f(x_{k-1})\,}.
\end{eqnarray*}
\begin{itemize}
\item \textbf{Criterios de paro:} $|x_{k+1}-x_k|<\varepsilon$ o $|f(x_{k+1})|<\varepsilon$.  
\item \textbf{Ventajas:} no requiere derivada; suele ser más rápido que bisección.  
\item \textbf{Sugerencia:} elegir dos semillas razonables; evitar $f(x_k)\approx f(x_{k-1})$ (división por número muy pequeño).
\end{itemize}
\end{Algthm}


\begin{Ejem} Funci\'on Cuadrática: $f(x)=x^2-4$. Semillas: $x_0=1$, $x_1=3$. Buscamos la raíz positiva (2). Consideremos la aproximaci\'on inicial $x_0,x_1$ y el nuevo punto $x_2$ es el punto donde se intersecta la secante con el eje $x$.
\begin{eqnarray*}
f(x)=x^2-4,\quad f(1)=-3,\quad f(3)=5.
\end{eqnarray*}
\begin{itemize}
\item Iteración 1.
\begin{eqnarray*}
x_2=3-5\frac{(3-1)}{5-(-3)}=1.75,\qquad |x_2-x_1|=1.25.
\end{eqnarray*}
\item Iteración 2. $x_0=3$, $x_1=1.75$, $f(1.75)=-0.9375$,
\begin{eqnarray*}
x_3=1.75-(-0.9375)\frac{(1.75-3)}{-0.9375-5}=1.947368421.
\end{eqnarray*}
\item Iteración 3. $f(1.947368421)=-0.207756233$,
\begin{eqnarray*}
x_4=1.947368421-(-0.207756233)\frac{(1.947368421-1.75)}{-0.207756233-(-0.9375)}=2.003558719.
\end{eqnarray*}
\item Iteración 4. $f(2.003558719)=0.014247540$,
\begin{eqnarray*}
x_5=2.003558719-(0.014247540)\frac{(2.003558719-1.947368421)}{0.014247540-(-0.207756233)}=1.999952593.
\end{eqnarray*}
\item Iteración 5. $f(1.999952593)=-1.89625\times 10^{-4}$, y $x_6=1.999999958$, por lo tanto $x\approx 2.000000000$.
\end{itemize}

Soluci\'on en R:
\begin{verbatim}
# f(x) = x^2 - 4   (x0=1, x1=3)
x0 <- 1.0; x1 <- 3.0
# Iteración 1
f0 <- x0^2 - 4; f1 <- x1^2 - 4
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 2
f0 <- x0^2 - 4; f1 <- x1^2 - 4
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 3
f0 <- x0^2 - 4; f1 <- x1^2 - 4
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 4
f0 <- x0^2 - 4; f1 <- x1^2 - 4
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 5
f0 <- x0^2 - 4; f1 <- x1^2 - 4
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
root_aprox <- x2
print(root_aprox)
\end{verbatim}
\end{Ejem}


\begin{Ejem} $f(x)=x^3-x-2$. Aproximaci\'on inicial: $x_0=1$, $x_1=2$, entonces
\begin{eqnarray*}
f(1)=-2,\qquad f(2)=4.
\end{eqnarray*}
\begin{itemize}
\item Iteración 1.
\begin{eqnarray*}
x_2=2-(4)\frac{(2-1)}{4-(-2)}=1.333333333.
\end{eqnarray*}
\item Iteración 2. $f(1.333333333)=-0.962962963$; $x_3=1.462686567$.

\item Iteración 3. $f(1.462686567)=-0.333338875$; $x_4=1.531169432$.

\item Iteración 4. $f(1.531169432)=0.058626418$; $x_5=1.520926421$.

\item Iteración 5.$f(1.520926421)=-0.002693300$; $x_6=1.521376317$; 
\end{itemize}


Soluci\'on en R
\begin{verbatim}
# f(x) = x^3 - x - 2   (x0=1, x1=2)
x0 <- 1.0; x1 <- 2.0
# Iteración 1
f0 <- x0^3 - x0 - 2; f1 <- x1^3 - x1 - 2
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 2
f0 <- x0^3 - x0 - 2; f1 <- x1^3 - x1 - 2
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 3
f0 <- x0^3 - x0 - 2; f1 <- x1^3 - x1 - 2
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 4
f0 <- x0^3 - x0 - 2; f1 <- x1^3 - x1 - 2
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 5
f0 <- x0^3 - x0 - 2; f1 <- x1^3 - x1 - 2
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
root_aprox <- x2
print(root_aprox)
\end{verbatim}
\end{Ejem}

\begin{Ejem} $f(x)=\cos x - x$. Aproximaci\'on inicial: $x_0=0.5$, $x_1=1.0$, por tanto $f(0.5)=0.37758256$ y $f(1.0)=-0.45969769$.
\begin{itemize}
\item Iteración 1. $x_2=0.725481587$, $f(x_2)\approx 0.022698391$.

\item Iteración 2. $x_3=0.738398620$, $f(x_3)\approx 0.001148782$.

\item Iteración 3. $x_4=0.739087211$, $f(x_4)\approx -3.4771\times 10^{-6}$.

\item Iteración 4. $x_5=0.739085133$; entonces $x\approx 0.739085133$.

\end{itemize}
Soluci\'on en R
\begin{verbatim}
# f(x) = cos(x) - x   (x0=0.5, x1=1.0)
x0 <- 0.5; x1 <- 1.0
# Iteración 1
f0 <- cos(x0) - x0; f1 <- cos(x1) - x1
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 2
f0 <- cos(x0) - x0; f1 <- cos(x1) - x1
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 3
f0 <- cos(x0) - x0; f1 <- cos(x1) - x1
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 4
f0 <- cos(x0) - x0; f1 <- cos(x1) - x1
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
root_aprox <- x2
print(root_aprox)
\end{verbatim}
\end{Ejem}

\begin{Ejem} $f(x)=e^{-x}-x$. Aproximaci\'on inicial: $x_0=0$, $x_1=1$, por tanto  $f(0)=1$, $f(1)=e^{-1}-1\approx -0.632120559$.

\item Iteración 1. $x_2=0.612699837$, $f(x_2)\approx -0.070813948$.

\item Iteración 2. $x_3=0.563838389$, $f(x_3)\approx 0.005182355$.

\item Iteración 3. $x_4=0.567170358$, $f(x_4)\approx -4.2419\times 10^{-5}$.

\item Iteración 4. $x_5=0.567143307$, $f(x_5)\approx -2.5380\times 10^{-8}$.

\item Iteración 5. $x_6=0.567143290$, $x\approx 0.567143290$.


Soluci\'on con R
\begin{verbatim}
# f(x) = exp(-x) - x   (x0=0, x1=1)
x0 <- 0.0; x1 <- 1.0
# Iteración 1
f0 <- exp(-x0) - x0; f1 <- exp(-x1) - x1
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 2
f0 <- exp(-x0) - x0; f1 <- exp(-x1) - x1
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 3
f0 <- exp(-x0) - x0; f1 <- exp(-x1) - x1
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 4
f0 <- exp(-x0) - x0; f1 <- exp(-x1) - x1
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
x0 <- x1; x1 <- x2
# Iteración 5
f0 <- exp(-x0) - x0; f1 <- exp(-x1) - x1
x2 <- x1 - f1*(x1 - x0)/(f1 - f0)
root_aprox <- x2
print(root_aprox)
\end{verbatim}
\end{Ejem}


\begin{Algthm}

Pseudoc\'odigo
\begin{verbatim}
Algoritmo: Método de la Secante

Entrada:
    f(x)        → función
    x0, x1      → valores iniciales
    tol         → tolerancia (por ejemplo, 1e-6)
    iter_max    → número máximo de iteraciones

Proceso:
    iter ← 0
    error ← 1

    Mientras (error > tol) Y (iter < iter_max) Hacer
        fx0 ← f(x0)
        fx1 ← f(x1)

        Si |fx1 - fx0| < 1e-12 Entonces
            Imprimir "Error: división por cero"
            Salir
        FinSi

        x2 ← x1 - fx1 * (x1 - x0) / (fx1 - fx0)
        error ← |x2 - x1|

        x0 ← x1
        x1 ← x2
        iter ← iter + 1
    FinMientras

Salida:
    Imprimir "Raíz aproximada:", x2
    Imprimir "Iteraciones:", iter
FinAlgoritmo

\end{verbatim}

Implememtaci\'on num\'erica

\begin{verbatim}
# Método de la Secante
secante <- function(f, x0, x1, tol = 1e-6, iter_max = 100){
  iter <- 0
  error <- 1
  
  while(error > tol && iter < iter_max){
    fx0 <- f(x0)
    fx1 <- f(x1)
    
    # Evitar división por cero
    if(abs(fx1 - fx0) < 1e-12){
      cat("Error: División por cero (f(x1) - f(x0) approx 0)\n")
      return(NA)
    }
    
    x2 <- x1 - fx1 * (x1 - x0) / (fx1 - fx0)
    error <- abs(x2 - x1)
    
    # Actualizar valores
    x0 <- x1
    x1 <- x2
    iter <- iter + 1
  }
  
  cat("Raíz aproximada:", x2, "\n")
  cat("Iteraciones:", iter, "\n")
  return(x2)
}

# Ejemplo de uso:
# f(x) = x^3 - 2x^2 + 3x - 5
f <- function(x) x^3 - 2*x^2 + 3*x - 5
secante(f, x0 = 1, x1 = 2)

\end{verbatim}
\end{Algthm}

%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\subsection{M\'etodo de Newton-Raphson}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
Se busca una raíz de $f(x)=0$ usando la recta \emph{tangente} en $x_k$. El siguiente punto est\'a dado por
\begin{equation}
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}.
\end{equation}

El criterio de paro es $|x_{k+1}-x_k|<\varepsilon$ o $|f(x_{k+1})|<\varepsilon$.

\begin{Algthm}
\begin{verbatim}
Entrada: f(x), f'(x), x0, tolerancia eps, Nmax
x = x0
Para k = 1..Nmax:
    fx = f(x); dfx = f'(x)
    si dfx = 0: detener (tangente horizontal)
    x1 = x - fx/dfx
    si |x1 - x| < eps o |f(x1)| < eps: retornar x1
    x = x1
Fin
\end{verbatim}
\end{Algthm}

\begin{Ejem} $f(x)=x^2-4$, $f'(x)=2x$.  Aproximaci\'on inicial $x_0=3$

\begin{eqnarray*}
x_1&=&3 - \frac{9-4}{2\cdot 3} = 3 - \frac{5}{6} = 2.166666667.\\
x_2&=&2.166666667 - \frac{(2.166666667)^2-4}{2\cdot 2.166666667}
   =2.166666667 - \frac{0.694444444}{4.333333334}= 2.006410256.\\
x_3&=&2.006410256 - \frac{(2.006410256)^2-4}{2\cdot 2.006410256}\approx 2.000006410.\\
x_4&\approx& 2.000000000.
\end{eqnarray*}
Raíz aproximada $x\approx 2$.

Soluci\'on con R:
\begin{verbatim}
# f(x) = x^2 - 4,  f'(x) = 2*x,  x0 = 3; x <- 3.0
# k=1
fx <- x^2 - 4; dfx <- 2*x
x1 <- x - fx/dfx; err <- abs(x1 - x); x <- x1
# k=2
fx <- x^2 - 4; dfx <- 2*x
x1 <- x - fx/dfx; err <- abs(x1 - x); x <- x1
# k=3
fx <- x^2 - 4; dfx <- 2*x
x1 <- x - fx/dfx; err <- abs(x1 - x); x <- x1
root_aprox <- x
print(root_aprox)
\end{verbatim}
\end{Ejem}

\begin{Ejem} $f(x)=x^3-x-2$, $f'(x)=3x^2-1$.  Valor inicial $x_0=1.5$

\begin{eqnarray*}
f(1.5)&=&-0.125; f'(1.5)=5.75,\\
x_1&=&1.5 - \frac{-0.125}{5.75}=1.521739130.\\
f(1.521739130)&\approx& 0.002137; f'(1.521739130)\approx 5.947070,\\
x_2 &=& 1.521739130 - \frac{0.002137}{5.947070} \approx 1.521380215.\\
f(1.521380215)&\approx& 6.03\times 10^{-7},\;\; f'(1.521380215)\approx 5.943790,\\
x_3 &\approx& 1.521380005,\\
x_4 &\approx& 1.521379706.
\end{eqnarray*}
por tanto $x\approx 1.521379706$. Soluci\'on con R
\begin{verbatim}
# f(x) = x^3 - x - 2,  f'(x) = 3*x^2 - 1,  x0 = 1.5
x <- 1.5

# k=1
fx <- x^3 - x - 2; dfx <- 3*x^2 - 1
x1 <- x - fx/dfx; err <- abs(x1 - x); x <- x1

# k=2
fx <- x^3 - x - 2; dfx <- 3*x^2 - 1
x1 <- x - fx/dfx; err <- abs(x1 - x); x <- x1

# k=3
fx <- x^3 - x - 2; dfx <- 3*x^2 - 1
x1 <- x - fx/dfx; err <- abs(x1 - x); x <- x1

root_aprox <- x
print(root_aprox)
\end{verbatim}
\end{Ejem}

\begin{Ejem} $f(x)=\cos x - x$, $f'(x)=-\sin x - 1$.  Aproximaci\'on inicial $x_0=0.5$

\begin{eqnarray*}
f(0.5)&=&0.37758256,\;\; f'(0.5)=-\sin(0.5)-1\approx -1.47942554,\\
x_1& =& 0.5 - \frac{0.37758256}{-1.47942554} = 0.755222.\\
f(0.755222)&\approx& -0.027103,\;\; f'(0.755222)\approx -1.685451,\\
x_2 &=& 0.755222 - \frac{-0.027103}{-1.685451} \approx 0.739142.\\
f(0.739142)&\approx& -9.46\times 10^{-5},\;\; f'(0.739142)\approx -1.673654,\\
x_3 &\approx& 0.739085;\\
x_4 &\approx& 0.739085133.
\end{eqnarray*}
por lo tanto $x\approx 0.739085133$.
\end{Ejem}

\begin{Ejem} $f(x)=e^{-x}-x$, $f'(x)=-e^{-x}-1$.  Aproximaci\'on inicial $x_0=0$
\begin{eqnarray*}
f(0)&=&1,\;\; f'(0)=-2 \;\Rightarrow\; x_1 = 0 - \frac{1}{-2} = 0.5.\\
f(0.5)&=&e^{-0.5}-0.5\approx 0.10653066,\;\; f'(0.5)=-e^{-0.5}-1\approx -1.60653066,\\
x_2 &=& 0.5 - \frac{0.10653066}{-1.60653066} \approx 0.566311.\\
f(0.566311)&\approx& 0.001157,\;\; f'(0.566311)\approx -1.567468,\\
x_3&\approx& 0.567049,\;\; x_4 \approx 0.56714329.
\end{eqnarray*}
por lo tanto la ra\'iz se encuentra en $x\approx 0.56714329$.
\end{Ejem}


\begin{Algthm}
Pseudoc\'odigo


\begin{verbatim}
Inicio
    Definir f(x) y f'(x)
    Leer x0, tolerancia, iter_max
    iter ← 0
    error ← 1

    Mientras (error > tolerancia) Y (iter < iter_max) Hacer
        x1 ← x0 - f(x0)/f'(x0)
        error ← |x1 - x0|
        x0 ← x1
        iter ← iter + 1
    FinMientras

    Imprimir "Raíz aproximada:", x1
    Imprimir "Iteraciones:", iter
Fin
\end{verbatim}

Implementaci\'on num\'erica

\begin{verbatim}
# Método de Newton-Raphson
newton <- function(f, df, x0, tol = 1e-6, iter_max = 100){
  iter <- 0
  error <- 1
  
  while(error > tol && iter < iter_max){
    x1 <- x0 - f(x0)/df(x0)
    error <- abs(x1 - x0)
    x0 <- x1
    iter <- iter + 1
  }
  
  cat("Raíz aproximada:", x1, "\n")
  cat("Iteraciones:", iter, "\n")
  return(x1)
}

# Ejemplo de uso:
# f(x) = x^2 - 2 
f  <- function(x) x^2 - 2
df <- function(x) 2*x
newton(f, df, x0 = 1)

\end{verbatim}
\end{Algthm}

%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>
\subsection{M\'etodo del punto fijo}
%<>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<><>===<>

Dada una ecuación $f(x)=0$, elegimos una función $g$ tal que
\begin{eqnarray*}
f(x)=0 \quad \textrm{s\'i y s\'olo s\'i }x=g(x).
\end{eqnarray*}
El método de \textbf{punto fijo} construye la sucesión
\begin{eqnarray*}
x_{k+1}=g(x_k),\qquad k=0,1,2,\dots
\end{eqnarray*}

\begin{Teo}Si $g$ es continua en un intervalo $I$ que contiene a la raíz $r$, $g(I)\subset I$ y
$\displaystyle \max_{x\in I}|g'(x)|=L<1$, entonces existe un único punto fijo $r\in I$ y para $x_0\in I$ se cumple $x_k\to r$ con convergencia al menos lineal (factor $\le L$).
\end{Teo}

\begin{Algthm}
\begin{verbatim}
Entrada: g(x), x0, tolerancia eps, Nmax
x = x0
Para k = 1..Nmax:
    x1 = g(x)
    si |x1 - x| < eps  o  |f(x1)| < eps: retornar x1
    x = x1
Fin
\end{verbatim}
\end{Algthm}

\begin{Ejem} $f(x)=x^2-4=0$
\begin{eqnarray*}
g(x)=\frac{1}{2}\Big(x+\frac{4}{x}\Big)\quad\text{(iteración de Herón para }\sqrt{4}\text{)}.
\end{eqnarray*}
Entonces $x_{k+1}=g(x_k)$ y el punto fijo es $r=2$. Tomamos $x_0=3$.

\begin{eqnarray*}
x_1&=&\frac{1}{2}(3+\tfrac{4}{3})=2.166666667.\\
x_2&=&\frac{1}{2}\!\Big(2.166666667+\tfrac{4}{2.166666667}\Big)=2.006410256.\\
x_3&=&\frac{1}{2}\!\Big(2.006410256+\tfrac{4}{2.006410256}\Big)=2.000003626.\\
x_4&=&\frac{1}{2}\!\Big(2.000003626+\tfrac{4}{2.000003626}\Big)=2.000000000.\\
x_5&\approx& 2.000000000.
\end{eqnarray*}
por lo tanto $r\approx 2$. Aquí $|g'(r)|=\frac{1}{2}(1-\tfrac{4}{r^2})=0$, por eso converge muy rápido. Soluci\'on con R
\begin{verbatim}
# g(x) = 0.5*(x + 4/x)   (x0 = 3)
x <- 3.0

# Iteración 1
x1 <- 0.5*(x + 4/x); x <- x1

# Iteración 2
x1 <- 0.5*(x + 4/x); x <- x1

# Iteración 3
x1 <- 0.5*(x + 4/x); x <- x1

# Iteración 4
x1 <- 0.5*(x + 4/x); x <- x1

# Iteración 5
x1 <- 0.5*(x + 4/x); x <- x1

root_aprox <- x
print(root_aprox)
\end{verbatim}
\end{Ejem}


\begin{Ejem} $f(x)=x^3-x-2=0$. Reescribimos $x=\sqrt[3]{x+2}$
\begin{eqnarray*}
g(x)=(x+2)^{1/3}.
\end{eqnarray*}
Cerca de la raíz $r\approx 1.52138$, $|g'(r)|=\frac{1}{3}(r+2)^{-2/3}<1$. Tomamos $x_0=1$.

\begin{eqnarray*}
x_1&=&(1+2)^{1/3}=1.44224957.\\
x_2&=&(1.44224957+2)^{1/3}=1.51571657.\\
x_3&=&(1.51571657+2)^{1/3}=1.52137900.\\
x_4&=&(1.52137900+2)^{1/3}=1.52137966.\\
x_5&=&(1.52137966+2)^{1/3}=1.52137971.\\
\end{eqnarray*}
por lo tanto $r\approx 1.52137971$. Soluci\'on con R
\begin{verbatim}
# g(x) = (x + 2)^(1/3)   (x0 = 1)
x <- 1.0

# Iteración 1
x1 <- (x + 2)^(1/3); x <- x1

# Iteración 2
x1 <- (x + 2)^(1/3); x <- x1

# Iteración 3
x1 <- (x + 2)^(1/3); x <- x1

# Iteración 4
x1 <- (x + 2)^(1/3); x <- x1

# Iteración 5
x1 <- (x + 2)^(1/3); x <- x1

root_aprox <- x
print(root_aprox)
\end{verbatim}
\end{Ejem}

\begin{Ejem} $f(x)=\cos x - x=0$
Se selecciona $x=\cos x$:
\begin{eqnarray*}
g(x)=\cos x.
\end{eqnarray*}
$|g'(r)|=|\!- \sin r|\approx 0.673<1$. Tomamos $x_0=0.5$.

\begin{eqnarray*}
x_1&=&\cos(0.5)=0.87758256.\\
x_2&=&\cos(0.87758256)=0.63901249.\\
x_3&=&\cos(0.63901249)=0.80268510.\\
x_4&=&\cos(0.80268510)=0.69477803.\\
x_5&=&\cos(0.69477803)=0.76819583.
\end{eqnarray*}

Soluci\'on con R
\begin{verbatim}
# g(x) = cos(x)   (x0 = 0.5)
x <- 0.5

# Iteración 1
x1 <- cos(x); x <- x1

# Iteración 2
x1 <- cos(x); x <- x1

# Iteración 3
x1 <- cos(x); x <- x1

# Iteración 4
x1 <- cos(x); x <- x1

# Iteración 5
x1 <- cos(x); x <- x1

root_aprox <- x
print(root_aprox)
\end{verbatim}
\end{Ejem}


\begin{Ejem} $f(x)=e^{-x}-x=0$, se toma
\begin{eqnarray*}
g(x)=e^{-x}.
\end{eqnarray*}
En la raíz $r\approx 0.567143$, $|g'(r)|=e^{-r}\approx 0.567<1$. Tomamos $x_0=1$.

\begin{eqnarray*}
x_1&=&e^{-1}=0.36787944.\\
x_2&=&e^{-0.36787944}=0.69220063.\\
x_3&=&e^{-0.69220063}=0.50047350.\\
x_4&=&e^{-0.50047350}=0.60624354.\\
x_5&=&e^{-0.60624354}=0.54539579.\\
\end{eqnarray*}

Soluci\'on con R
\begin{verbatim}
# g(x) = exp(-x)   (x0 = 1)
x <- 1.0

# Iteración 1
x1 <- exp(-x); x <- x1

# Iteración 2
x1 <- exp(-x); x <- x1

# Iteración 3
x1 <- exp(-x); x <- x1

# Iteración 4
x1 <- exp(-x); x <- x1

# Iteración 5
x1 <- exp(-x); x <- x1

root_aprox <- x
print(root_aprox)
\end{verbatim}
\end{Ejem}


\begin{Algthm}
Pseudoc\'odigo
\begin{verbatim}
Inicio
    Definir f(x) y g(x)
    Leer x0, tolerancia, iter_max
    iter ← 0
    error ← 1

    Mientras (error > tolerancia) Y (iter < iter_max) Hacer
        x1 ← g(x0)
        error ← |x1 - x0|
        x0 ← x1
        iter ← iter + 1
    FinMientras

    Imprimir "Raíz aproximada:", x1
    Imprimir "Iteraciones:", iter
Fin

\end{verbatim}


Implementaci\'on numérica:

\begin{verbatim}
# Método del Punto Fijo
fijo <- function(g, x0, tol = 1e-6, iter_max = 100){
  iter <- 0
  error <- 1
  
  while(error > tol && iter < iter_max){
    x1 <- g(x0)
    error <- abs(x1 - x0)
    x0 <- x1
    iter <- iter + 1
  }
  
  cat("Raíz aproximada:", x1, "\n")
  cat("Iteraciones:", iter, "\n")
  return(x1)
}

# Ejemplo de uso:
# f(x) = cos(x) - x  →  g(x) = cos(x)
g <- function(x) cos(x)
fijo(g, x0 = 0.5)

\end{verbatim}
\end{Algthm}

%=========================================================
\subsection{Ejercicios}
%=========================================================

\subsubsection{Método de Bisección}\label{Secc_Biseccion}

\textbf{Instrucciones:}
\begin{enumerate}
    \item Para cada función \(f(x)\), encuentre un intervalo \([a,b]\) tal que \(f(a)\cdot f(b) < 0\).
    \item Realice al menos 8 iteraciones del método de bisección.
    \item Registre los valores \(a_k, b_k, c_k, f(a_k), f(b_k), f(c_k), \text{signo}(f(a_k)f(c_k))\) y el error \(|b_k-a_k|\).
    \item Grafique la función y los intervalos de reducción en cada paso.
\end{enumerate}


\begin{Ejer} Resuelve los siguientes ejercicioes aplicando el m\'etodo de la bisecci\'on
\begin{enumerate}
\begin{multicols}{2}
\item $f(x) = x^2 - 5$ en $[2,3]$
\item $f(x) = x^3 - 4x + 1$ en $[0,1]$
\item $f(x) = x^4 - 8x + 2$ en $[0,3]$
\item $f(x) = e^{-x} - \frac{x}{2}$ en $[0,2]$
\item $f(x) = \cos(x) - x^2$ en $[0,1]$
\item $f(x)=3x^{2}-7x-2$. $[a,b]=[-1,\,0]$.
\item $f(x)=-4x^{2}+5x+6$.$[a,b]=[-1,\,0]$.
\item $f(x)=2x^{2}+3x-5$. $[a,b]=[0,\,2]$.
\item $f(x)=x^{3}-5x+1$. $[a,b]=[0,\,1]$.
\item $f(x)=x^{4}-6x^{3}+5x^{2}+8x-4$.  $[a,b]=[0,\,1]$.
\item $f(x)=-2x^{4}+4x^{3}+6x^{2}-3x+2$. $[a,b]=[-2,\,-1]$.
\item $f(x)=2x^{4}-x^{3}-7x^{2}+4x+3$.  $[a,b]=[-1,\,0]$.
\item $f(x)=x^{4}+2x^{3}-10x^{2}-x+5$.  $[a,b]=[0,\,1]$.
\item $f(x)=5\,e^{0.6x-1}-3$.  $[a,b]=[0,\,2]$.
\item $f(x)=7-4\,e^{-0.9x+0.2}$.  $[a,b]=[-2,\,0]$.
\item $f(x)=\sin(2x)-\dfrac{x}{2}$. $[a,b]=[1,\,2]$.
\item $f(x)=(2x+1.4)\,\cos\!\bigl(3x+\tfrac{\pi}{4}\bigr)$. $[a,b]=[0.1,\,0.6]$.
\item $f(x)=(-1.5x+0.9)\,\sin\!\bigl(4x-\tfrac{\pi}{6}\bigr)$. $[a,b]=[0.2,\,0.9]$.
\end{multicols}
\end{enumerate}
\end{Ejer}
\noindent\textbf{Formato sugerido de tabla:}

\begin{table}[H]
\centering
\small
\begin{tabular}{c|c|c|c|c|c|c|c|c|}
\hline
$k$ & $a$ & $b$ & $c$ & $f(a)$ & $f(b)$ & $f(c)$ & $signo(f(a)*f(c))$ & $error$ \\
\hline
1 &     &     &     &     &     &     &     \\
2 &     &     &     &     &     &     &     \\
3 &     &     &     &     &     &     &     \\
4 &     &     &     &     &     &     &     \\
5 &     &     &     &     &     &     &     \\
6 &     &     &     &     &     &     &     \\
7 &     &     &     &     &     &     &     \\
8 &     &     &     &     &     &     &     \\
9 &     &     &     &     &     &     &     \\
10 &     &     &     &     &     &     &     \\
... &  &  &  &  &  &  &  \\
\hline
\end{tabular}
\caption{Iteraciones del método de bisección para $f(x)$.}
\end{table}

\bigskip

%===================== SECANTE =====================
\subsubsection{Método de la Secante}\label{Secc_Secante}

\textbf{Instrucciones:}
\begin{enumerate}
    \item Para cada función \(f(x)\), elija dos puntos iniciales \(x_0, x_1\) cercanos a la raíz.
    \item Aplique iterativamente:
    \[
    x_{k+1} = x_k - f(x_k)\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}
    \]
    \item Realice al menos 6 iteraciones o hasta alcanzar una tolerancia de \(10^{-5}\).
    \item Compare el número de iteraciones con el método de bisección.
    \item Grafique las secantes de cada iteración y el punto de intersección con el eje \(x\).
\end{enumerate}

\begin{Ejer}
Resuelva los siguientes ejercicios aplicando el m\'etodo de la secante:

\begin{enumerate}
\begin{multicols}{2}
\item \(f(x) = x^2 - 2x - 3\), con \(x_0=0\), $x_1=3$
\item $f(x)=x^{4}-10x^{2}+9$, $x_0=0$, $x_1=2$
\item $f(x)=2x^2 - 5*x + 3$, $x_0=-1$, $x_1=2$
\item $f(x)=-3x^2+4*x + 2$, $x_0=-1$, $x_1=3$
\item $f(x) = x^3 +4x^2 - x - 1$, con $x_0=-3$, $x_1=1.5$
\item $f(x) = x^4-5x^3 +6x^2 +4x - 8$, con $x_0=0$, $x_1=3$
\item $f(x) = x^4 - 5$, con $x_0=1$, $x_1=2$
\item $f(x) = -2x^4+3x^3+7x^2-5x+1$, con $x_0=2$, $x_1=4$
\item $f(x) = 4e^{-0.8x-0.5} - 7$, con $x_0=0$, $x_1=1.5$
\item $f(x) = -3e^{-1.2x+0.4} +2$, con $x_0=0$, $x_1=1.1$
\item $f(x) = \sin(x) - \frac{x}{3}$, con $x_0=0$, $x_1=1$
\item $f(x) = (3x+3.1)\cos(2x+\pi/6)$, con $x_0=0.3$, $x_1=0.8$
\item $f(x) = (-2x+1.1)\sin(5x-\pi/3)$, con $x_0=0.05$, $x_1=0.35$
\end{multicols}
\end{enumerate}

\end{Ejer}

\noindent\textbf{Formato sugerido de tabla:}
\begin{table}[H]
\centering
\small
\begin{tabular}{c|c|c|c|c|c|}
\hline
$k$ & $x_{k-1}$ & $x_k$ & $f(x_k)$ & $x_{k+1}$ & $e_{k}=\|x_{k+1}-x_{k}\|$\\
\hline
1 &     &     &     &     \\
2 &     &     &     &     \\
3 &     &     &     &     \\
4 &     &     &     &     \\
5 &     &     &     &     \\
6 &     &     &     &     \\
7 &     &     &     &     \\
8 &     &     &     &     \\
9 &     &     &     &     \\
10 &     &     &     &     \\
... &  &  &  &  \\
\hline
\end{tabular}
\caption{Iteraciones del método de la secante para $f(x)$.}
\end{table}

\subsubsection{Método de Newton}\label{Secc_Newton}
\textbf{Instrucciones:}
Resuelve los siguientes ejercicios aplicando el \textbf{método de Newton}.
Para cada función \( f(x) \):
\begin{enumerate}
    \item Verifica que exista un intervalo \([a,b]\) donde \(f(a)\cdot f(b) < 0\).
    \item Realiza al menos 8 iteraciones del método.
    \item Registra los valores \(x_k, f(x_k), f'(x_k)\) y el error \(|x_{k+1}-x_k|\).
    \item Grafica la función y marca el punto donde converge la raíz.
\end{enumerate}


\begin{Ejer}
\begin{multicols}{2}
\begin{enumerate}
\item $f(x)=x^3-7x+6$, \quad $[a,b]=[0,1]$
\item $f(x)=x^3+2x^2-5$, \quad $[a,b]=[1,2]$
\item $f(x)=x^4-3x^3+2$, \quad $[a,b]=[0,1]$
\item $f(x)=\sin(x)-\dfrac{x}{3}$, \quad $[a,b]=[0,2]$
\item $f(x)=e^{-x}-x$, \quad $[a,b]=[0,1]$
\item $f(x)=x^3-2x-5$, \quad $[a,b]=[2,3]$
\item $f(x)=x^5-3x+1$, \quad $[a,b]=[0,1]$
\item $f(x)=\cos(x)-2x$, \quad $[a,b]=[0,1]$
\item $f(x)=\ln(x+2)+x-1$, \quad $[a,b]=[-1,0]$
\item $f(x)=x\,e^{-x}-0.1$, \quad $[a,b]=[0,1]$
\item $f(x)=x^3-4\sin(x)$, \quad $[a,b]=[1,2]$
\item $f(x)=e^x-3x^2$, \quad $[a,b]=[0,1]$
\item $f(x)=x^2-\cos(x)$, \quad $[a,b]=[0,1]$
\item $f(x)=x^3+4x^2-10$, \quad $[a,b]=[1,2]$
\item $f(x)=2x\,\sin(x)-1$, \quad $[a,b]=[0,1]$
\end{enumerate}
\end{multicols}
\end{Ejer}

\noindent\textbf{Formato sugerido de tabla para Newton:}

\begin{table}[H]
\centering
\small
\begin{tabular}{c|c|c|c|c|}
\hline
$k$ & $x_k$ & $f(x_k)$ & $f'(x_k)$ & $|x_{k+1}-x_k|$ \\
\hline
1 &     &     &     &     \\
2 &     &     &     &     \\
3 &     &     &     &     \\
4 &     &     &     &     \\
5 &     &     &     &     \\
6 &     &     &     &     \\
7 &     &     &     &     \\
8 &     &     &     &     \\
9 &     &     &     &     \\
10 &    &     &     &     \\
\hline
\end{tabular}
\caption{Iteraciones del método de Newton para $f(x)$.}
\end{table}

\subsubsection{Método del Punto Fijo}\label{Secc_PtoFijo}

\textbf{Instrucciones:}
Resuelve los siguientes ejercicios aplicando el \textbf{método del punto fijo}.
\begin{enumerate}
    \item Reescriba la ecuación \(f(x)=0\) en forma \(x=g(x)\).
    \item Verifique que la función \(g(x)\) cumpla la condición de convergencia local \(|g'(x)| < 1\) en el intervalo considerado.
    \item Realice al menos 8 iteraciones partiendo de un valor inicial \(x_0\).
    \item Registre los valores \(x_k, g(x_k), |x_{k+1}-x_k|\).
    \item Grafique \(y=g(x)\) y \(y=x\) para visualizar el punto de intersección.
\end{enumerate}


\begin{Ejer}
\begin{enumerate}
\item $f(x)=\cos(x)-x$, Sugerencia: $g(x)=\cos(x)$, \quad $x_0=0.5$
\item $f(x)=x^3+x-1$, Sugerencia: $g(x)=1-x^3$, \quad $x_0=0.6$
\item $f(x)=e^{-x}-x$, Sugerencia: $g(x)=e^{-x}$, \quad $x_0=0.7$
\item $f(x)=x^2-4x+1$, Sugerencia: $g(x)=\dfrac{x^2+1}{4}$, \quad $x_0=1$
\item $f(x)=x^3-2x-5$ , Sugerencia: $g(x)=\sqrt[3]{2x+5}$, \quad $x_0=2$
\item $f(x)=x^2-e^{x}+2$, Sugerencia: $g(x)=\sqrt{e^x-2}$, \quad $x_0=0.5$
\item $f(x)=\ln(x+1)+x-2$, Sugerencia: $g(x)=2-\ln(x+1)$, \quad $x_0=0.5$
\item $f(x)=x^3-3x+1$, Sugerencia: $g(x)=\sqrt[3]{3x-1}$, \quad $x_0=0.8$
\item $f(x)=x-\sin(x)-1$, Sugerencia: $g(x)=\sin(x)+1$, \quad $x_0=1$
\item $f(x)=x^2-3$, Sugerencia: $g(x)=\dfrac{3}{x}$, \quad $x_0=1.5$
\item $f(x)=2x-e^{-x}$, Sugerencia: $g(x)=\dfrac{e^{-x}}{2}$, \quad $x_0=0$
\item $f(x)=x^3+4x^2-10$, Sugerencia: $g(x)=\sqrt{\dfrac{10-x^3}{4}}$, \quad $x_0=1.5$
\item $f(x)=x^2-\cos(x)$, Sugerencia: $g(x)=\sqrt{\cos(x)}$, \quad $x_0=0.5$
\item $f(x)=x-e^{-x^2}$, Sugerencia: $g(x)=e^{-x^2}$, \quad $x_0=0.5$
\item $f(x)=x^3-2$, Sugerencia: $g(x)=\sqrt[3]{2}$, \quad $x_0=1$
\end{enumerate}
\end{Ejer}

\noindent\textbf{Formato sugerido de tabla para el método del punto fijo:}

\begin{table}[H]
\centering
\small
\begin{tabular}{c|c|c|c|}
\hline
$k$ & $x_k$ & $g(x_k)$ & $|x_{k+1}-x_k|$ \\
\hline
1 &     &     &     \\
2 &     &     &     \\
3 &     &     &     \\
4 &     &     &     \\
5 &     &     &     \\
6 &     &     &     \\
7 &     &     &     \\
8 &     &     &     \\
9 &     &     &     \\
10 &    &     &     \\
\hline
\end{tabular}
\caption{Iteraciones del método del punto fijo para $f(x)=0$.}
\end{table}



\newpage


\subsubsection{Para impresión}


\subsubsection{Punto fijo}

Pseudoc\'odigo

\begin{verbatim}
Algoritmo: Método del Punto Fijo

Entrada:
    g(x)        → función de iteración
    x0          → valor inicial
    tol         → tolerancia (por ejemplo, 1e-6)
    iter_max    → número máximo de iteraciones

Proceso:
    iter ← 0
    error ← 1

    Mientras (error > tol) Y (iter < iter_max) Hacer
        x1 ← g(x0)
        error ← |x1 - x0|
        x0 ← x1
        iter ← iter + 1
    FinMientras

Salida:
    Imprimir "Raíz aproximada:", x1
    Imprimir "Iteraciones:", iter
FinAlgoritmo
\end{verbatim}

Implementación numérica

\begin{verbatim}
\begin{lstlisting}[language=R, caption={Método del Punto Fijo en R}]
fijo <- function(g, x0, tol = 1e-6, iter_max = 100){
  iter <- 0
  error <- 1
  
  while(error > tol && iter < iter_max){
    x1 <- g(x0)
    error <- abs(x1 - x0)
    x0 <- x1
    iter <- iter + 1
  }
  
  cat("Raíz aproximada:", x1, "\n")
  cat("Iteraciones:", iter, "\n")
  return(x1)
}

# Ejemplo de uso:
g <- function(x) cos(x)
fijo(g, x0 = 0.5)
\end{lstlisting}

\end{verbatim}

\subsubsection{Newton-Raphson}

Pseudoc\'odigo

\begin{verbatim}

Algoritmo: Método de Newton–Raphson

Entrada:
    f(x)        → función
    f'(x)       → derivada de f(x)
    x0          → valor inicial
    tol         → tolerancia
    iter_max    → número máximo de iteraciones

Proceso:
    iter ← 0
    error ← 1

    Mientras (error > tol) Y (iter < iter_max) Hacer
        x1 ← x0 - f(x0)/f'(x0)
        error ← |x1 - x0|
        x0 ← x1
        iter ← iter + 1
    FinMientras

Salida:
    Imprimir "Raíz aproximada:", x1
    Imprimir "Iteraciones:", iter
FinAlgoritmo
\end{verbatim}

Implementaci\'on num\'erica

\begin{verbatim}

newton <- function(f, df, x0, tol = 1e-6, iter_max = 100){
  iter <- 0
  error <- 1
  
  while(error > tol && iter < iter_max){
    x1 <- x0 - f(x0)/df(x0)
    error <- abs(x1 - x0)
    x0 <- x1
    iter <- iter + 1
  }
  
  cat("Raíz aproximada:", x1, "\n")
  cat("Iteraciones:", iter, "\n")
  return(x1)
}

# Ejemplo:
f  <- function(x) x^2 - 2
df <- function(x) 2*x
newton(f, df, x0 = 1)


\end{verbatim}

\subsubsection{Bisecci\'on}

Pseudoc\'odigo

\begin{verbatim}

Algoritmo: Método de Bisección

Entrada:
    f(x)        → función continua
    a, b        → intervalo [a,b] con f(a)*f(b) < 0
    tol         → tolerancia
    iter_max    → número máximo de iteraciones

Proceso:
    Si f(a)*f(b) > 0 Entonces
        Imprimir "No hay cambio de signo"
        Salir
    FinSi

    iter ← 0
    error ← |b - a|

    Mientras (error > tol) Y (iter < iter_max) Hacer
        c ← (a + b)/2

        Si f(a)*f(c) < 0 Entonces
            b ← c
        Sino
            a ← c
        FinSi

        error ← |b - a|
        iter ← iter + 1
    FinMientras

Salida:
    Imprimir "Raíz aproximada:", c
    Imprimir "Iteraciones:", iter
FinAlgoritmo
\end{verbatim}

Implementaci\'on num\'erica

\begin{verbatim}
biseccion <- function(f, a, b, tol = 1e-6, iter_max = 100){
  if(f(a)*f(b) > 0){
    cat("Error: no hay cambio de signo en [a,b].\n")
    return(NA)
  }
  
  iter <- 0
  error <- abs(b - a)
  
  while(error > tol && iter < iter_max){
    c <- (a + b)/2
    if(f(a)*f(c) < 0){
      b <- c
    } else {
      a <- c
    }
    error <- abs(b - a)
    iter <- iter + 1
  }
  
  cat("Raíz aproximada:", c, "\n")
  cat("Iteraciones:", iter, "\n")
  return(c)
}

# Ejemplo:
f <- function(x) x^3 - x - 2
biseccion(f, a = 1, b = 2)
\end{verbatim}

\subsubsection{Secante}

Pseudoc\'odigo

\begin{verbatim}


Algoritmo: Método de la Secante

Entrada:
    f(x)        → función
    x0, x1      → valores iniciales
    tol         → tolerancia
    iter_max    → número máximo de iteraciones

Proceso:
    iter ← 0
    error ← 1

    Mientras (error > tol) Y (iter < iter_max) Hacer
        fx0 ← f(x0)
        fx1 ← f(x1)

        Si |fx1 - fx0| < 1e-12 Entonces
            Imprimir "Error: división por cero"
            Salir
        FinSi

        x2 ← x1 - fx1 * (x1 - x0) / (fx1 - fx0)
        error ← |x2 - x1|

        x0 ← x1
        x1 ← x2
        iter ← iter + 1
    FinMientras

Salida:
    Imprimir "Raíz aproximada:", x2
    Imprimir "Iteraciones:", iter
FinAlgoritmo
\end{verbatim}

Implementaci\'on num\'erica


\begin{verbatim}
secante <- function(f, x0, x1, tol = 1e-6, iter_max = 100){
  iter <- 0
  error <- 1
  
  while(error > tol && iter < iter_max){
    fx0 <- f(x0)
    fx1 <- f(x1)
    
    if(abs(fx1 - fx0) < 1e-12){
      cat("Error: División por cero (f(x1) - f(x0) approxx 0)\n")
      return(NA)
    }
    
    x2 <- x1 - fx1 * (x1 - x0) / (fx1 - fx0)
    error <- abs(x2 - x1)
    
    x0 <- x1
    x1 <- x2
    iter <- iter + 1
  }
  
  cat("Raíz aproximada:", x2, "\n")
  cat("Iteraciones:", iter, "\n")
  return(x2)
}

# Ejemplo:
f <- function(x) x^3 - 2*x^2 + 3*x - 5
secante(f, x0 = 1, x1 = 2)

\end{verbatim}

\newpage
%=========================================================
\subsection{Tarea para el portafolio}
%=========================================================
\begin{centering} 
\textbf{ A continuaci\'on se presentan las indicaciones sobre los ejercicios que se van a a incluir en el portafolio del curso de M\'etodos Num\'ericos}
\end{centering}

\begin{enumerate}
\item Del m\'etodo de Bisecci\'on,  (subsección \ref{Secc_Biseccion}) del ejercicio 1, resolver de los numerales impares: 3 ejercicios \textit{a mano} y 5 ejercicios con el \textit{c\'odigo automatizado}.

\item Del m\'etodo de la Secante (subsección \ref{Secc_Secante}), del ejercicio 2, resolver de los ejercicios impares 3: \textit{a mano} y de los ejercicios pares 5: con el \textit{c\'odigo automatizado}.


\item De la subsección \ref{Secc_Newton}, Del m\'etodo de Newton (subsección \ref{Secc_Newton}), del ejercicio 3, resolver de los ejercicios pares: 3 \textit{a mano} y de los ejercicios impares 5: con el \textit{c\'odigo automatizado}.

\item De la subsecci\'on \ref{Secc_PtoFijo}, Del m\'etodo del Punto Fijo (subsección \ref{Secc_PtoFijo}), del ejercicio 4, resolver de los:  3 \textit{a mano} y  5 con el \textit{c\'odigo automatizado}.




\end{enumerate}


%<<==>><<==>><<==>><<==>><<==>><<==>>
\section{Interpolaci\'on Numérica}
%<<==>><<==>><<==>><<==>><<==>><<==>>

%<<==>><<==>><<==>><<==>><<==>><<==>>
\subsection{Conceptos Fundamentales}
%<<==>><<==>><<==>><<==>><<==>><<==>>
\begin{Def}
La interpolación es una técnica numérica utilizada para estimar valores desconocidos de una función a partir de un conjunto de puntos conocidos.  A diferencia del ajuste de curvas, la interpolación busca una función que pase exactamente por los puntos dados.   La interpolaci\'on consiste en estimar el valor de una funci\'on dentro del intervalo cubierto por un conjunto de datos conocidos.   
\begin{eqnarray}
(x_0, y_0), \; (x_1, y_1), \; \ldots, \; (x_n, y_n)
\end{eqnarray}
donde se asume que $y_i = f(x_i)$ para $i = 0, 1, \ldots, n$.  
El objetivo es construir una función $P(x)$, generalmente un polinomio, tal que:
\begin{eqnarray}
P(x_i) = f(x_i) = y_i, \quad i=0,1,\ldots,n.
\end{eqnarray}
Dado que existen $n+1$ puntos distintos $(x_i, y_i)$, se puede demostrar que existe un único polinomio de grado $n$ que los interpola:
\begin{eqnarray*}
P_n(x) = a_0 + a_1x + a_2x^2 + \cdots + a_nx^n.
\end{eqnarray*}
La determinación de los coeficientes $a_i$ puede hacerse de diversas maneras, dando origen a los distintos métodos de interpolación.
\end{Def}



\begin{Note}
En la interpolaci\'on, la funci\'on construida pasa exactamente por los puntos dados;  en el ajuste de curvas, se busca una funci\'on que aproxime los datos minimizando un error global. Si el polinomio interpolante se usa para estimar valores de $f(x)$ fuera del intervalo cubierto por los puntos dados, el proceso se denomina \textbf{extrapolación}.  La extrapolaci\'on extiende esta idea a valores fuera del intervalo, con menor confiabilidad, tiende a generar errores grandes, por lo que debe usarse con precaución.
\end{Note}

\begin{center}
\textbf{Comparación entre interpolación y ajuste de curvas:}
\begin{tabular}{|p{5cm}|p{5cm}|}
\hline
\textbf{Interpolación} & \textbf{Ajuste de Curvas} \\ \hline
La curva pasa exactamente por todos los puntos. & La curva se aproxima a los puntos, pero no necesariamente pasa por todos. \\ \hline
Se usa cuando los datos son exactos o tabulados sin error experimental. & Se usa cuando los datos contienen ruido o errores de medición. \\ \hline
Ejemplo: tablas de logaritmos o funciones trigonométricas. & Ejemplo: datos experimentales en laboratorio. \\ \hline
\end{tabular}
\end{center}

\begin{Note}
El error de interpolaci\'on mide la diferencia entre el valor real de la funci\'on y el valor estimado por el polinomio interpolante.  Se expresa mediante la f\'ormula:
\begin{eqnarray}
R_n(x) = f(x) - P_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^{n}(x - x_i),
\end{eqnarray}
para alg\'un $\xi$ en el intervalo de interpolaci\'on.

El error cometido al aproximar $f(x)$ mediante el polinomio interpolante $P_n(x)$ se expresa como:
\begin{eqnarray*}
R_n(x) = f(x) - P_n(x) = \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^{n}(x - x_i),
\end{eqnarray*}
donde $\xi$ es algún valor dentro del intervalo $(x_0, x_n)$.  
El error depende de la derivada de orden $(n+1)$ de la función $f(x)$ y de la distribución de los nodos $x_i$.
\end{Note}

\begin{Note}
\textbf{Problemas mal condicionados (efecto Runge):}  El uso de polinomios de alto grado con nodos equiespaciados puede producir oscilaciones extremas cerca de los extremos del intervalo.
Cuando los nodos $x_i$ están equiespaciados y se incrementa el número de puntos, el polinomio interpolante puede oscilar significativamente, especialmente cerca de los extremos del intervalo.  
Este fenómeno se conoce como \textbf{efecto Runge}.  
Una forma de mitigarlo es usar nodos distribuidos según los \textbf{polinomios de Chebyshev} o recurrir a métodos por tramos como los 
\end{Note}

%<<==>><<==>><<==>><<==>><<==>><<==>>
\subsection{Forma General del Polinomio Interpolante}
%<<==>><<==>><<==>><<==>><<==>><<==>>

Se define el polinomio de grado $n$ que pasa por $n+1$ puntos $(x_0, y_0), (x_1, y_1), \ldots, (x_n, y_n)$.
\begin{eqnarray*}
P_n(x_i) = y_i, \quad i = 0, 1, 2, \ldots, n
\end{eqnarray*}
donde se conoce el valor de la función $ f(x) $ en cada punto, es decir, $ y_i = f(x_i) $. El \textbf{problema de interpolación polinómica} consiste en determinar un polinomio $ P_n(x) $ de grado menor o igual a $ n $,  la forma general del polinomio interpolante es:
\begin{eqnarray*}
P_n(x) = a_0 + a_1x + a_2x^2 + \cdots + a_nx^n,
\end{eqnarray*}


donde los coeficientes $ a_0, a_1, \ldots, a_n $ se determinan imponiendo las condiciones de interpolación:
\begin{eqnarray*}
\begin{cases}
a_0 + a_1x_0 + a_2x_0^2 + \cdots + a_nx_0^n = y_0 \\
a_0 + a_1x_1 + a_2x_1^2 + \cdots + a_nx_1^n = y_1 \\
\quad \vdots \\
a_0 + a_1x_n + a_2x_n^2 + \cdots + a_nx_n^n = y_n
\end{cases}
\end{eqnarray*}

En forma matricial:
\begin{eqnarray*}
\underbrace{
\begin{bmatrix}
1 & x_0 & x_0^2 & \cdots & x_0^n \\
1 & x_1 & x_1^2 & \cdots & x_1^n \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^n
\end{bmatrix}}_{V}
\underbrace{
\begin{bmatrix}
a_0 \\ a_1 \\ \vdots \\ a_n
\end{bmatrix}}_{\vec{a}}
=
\underbrace{
\begin{bmatrix}
y_0 \\ y_1 \\ \vdots \\ y_n
\end{bmatrix}}_{\vec{y}}
\end{eqnarray*}
donde $ V $ es la \textbf{matriz de Vandermonde}.  
La existencia de una solución única depende de que $ V $ sea invertible, lo cual se cumple siempre que todos los $ x_i $ sean distintos. Para demostrar que el polinomio interpolante es único, consideremos dos polinomios $ P_n(x) $ y $ Q_n(x) $ que ambos interpola los mismos puntos:
\begin{eqnarray*}
P_n(x_i) = Q_n(x_i) = y_i, \quad i=0,1,\ldots,n.
\end{eqnarray*}
se define la función:
\begin{eqnarray*}
R(x) = P_n(x) - Q_n(x).
\end{eqnarray*}
Entonces $ R(x) $ es también un polinomio de grado a lo sumo $ n $, y satisface:
\begin{eqnarray*}
R(x_i) = P_n(x_i) - Q_n(x_i) = 0, \quad \text{para todos } i=0,1,\ldots,n.
\end{eqnarray*}
Por lo tanto, $ R(x) $ tiene $ n+1 $ raíces distintas $ x_0, x_1, \ldots, x_n $.  
Sin embargo, un polinomio de grado $ n $ no puede tener más de $ n $ raíces distintas, a menos que todos sus coeficientes sean cero.  
Así,
\begin{eqnarray*}
R(x) \equiv 0 \quad \Longrightarrow \quad P_n(x) = Q_n(x),
\end{eqnarray*}
lo que demuestra que el polinomio interpolante es \textbf{único}.

\begin{Note}
\begin{itemize}
    \item Para $ n+1 $ puntos distintos, siempre existe un polinomio de grado $ n $ que pasa exactamente por ellos.
    \item El cálculo directo mediante la matriz de Vandermonde es exacto teóricamente, pero numéricamente inestable para grandes $ n $ debido a la mala condición de la matriz.
    \item Por esta razón, en la práctica se prefieren formas más estables del polinomio, como las fórmulas de \textbf{Lagrange} o \textbf{Newton}.
\end{itemize}
\end{Note}

\begin{Ejem}
Supóngase que se tienen los puntos:
\begin{eqnarray*}
(1, 2), \quad (2, 3), \quad (4, 7).
\end{eqnarray*}
El polinomio de grado 2 tendrá la forma:
\begin{eqnarray*}
P_2(x) = a_0 + a_1x + a_2x^2.
\end{eqnarray*}
Sustituyendo los puntos:
\begin{eqnarray*}
\begin{cases}
a_0 + a_1(1) + a_2(1)^2 = 2 \\
a_0 + a_1(2) + a_2(2)^2 = 3 \\
a_0 + a_1(4) + a_2(4)^2 = 7
\end{cases}
\end{eqnarray*}
Al resolver el sistema, se obtiene:
\begin{eqnarray*}
a_0 = 1, \quad a_1 = 0.5, \quad a_2 = 0.25.
\end{eqnarray*}
Por tanto,
\begin{eqnarray*}
P_2(x) = 1 + 0.5x + 0.25x^2.
\end{eqnarray*}
Verificando:
\begin{eqnarray*}
P_2(1)=2, \quad P_2(2)=3, \quad P_2(4)=7,
\end{eqnarray*}
lo que confirma que el polinomio interpola correctamente los puntos.
\end{Ejem}

\begin{Ejem}
Interpolar los puntos:
\begin{eqnarray*}
(0, 1), \quad (1, 0), \quad (2, -1), \quad (3, 2).
\end{eqnarray*}

Buscamos un polinomio cúbico:
\begin{eqnarray*}
P_3(x) = a_0 + a_1x + a_2x^2 + a_3x^3.
\end{eqnarray*}

Sustituyendo:
\begin{eqnarray*}
\begin{cases}
a_0 = 1, \\
a_0 + a_1 + a_2 + a_3 = 0, \\
a_0 + 2a_1 + 4a_2 + 8a_3 = -1, \\
a_0 + 3a_1 + 9a_2 + 27a_3 = 2.
\end{cases}
\end{eqnarray*}

De la primera ecuación, $ a_0 = 1 $. Sustituyendo en las demás:
\begin{eqnarray*}
\begin{cases}
a_1 + a_2 + a_3 = -1, \\
2a_1 + 4a_2 + 8a_3 = -2, \\
3a_1 + 9a_2 + 27a_3 = 1.
\end{cases}
\end{eqnarray*}

Resolviendo el sistema:
\begin{eqnarray*}
a_1 = -\tfrac{5}{2}, \quad a_2 = \tfrac{9}{4}, \quad a_3 = -\tfrac{3}{4}.
\end{eqnarray*}

El polinomio interpolante es:
\begin{eqnarray*}
P_3(x) = 1 - \frac{5}{2}x + \frac{9}{4}x^2 - \frac{3}{4}x^3.
\end{eqnarray*}

Verificando:
\begin{eqnarray*}
P_3(0)=1, \quad P_3(1)=0, \quad P_3(2)=-1, \quad P_3(3)=2.
\end{eqnarray*}

\textbf{Interpretación:} este polinomio de grado tres conecta los cuatro puntos de manera exacta.  
Si se grafican los datos y la curva, se observa un comportamiento ondulado típico de polinomios de orden alto, mostrando cómo la interpolación exacta puede oscilar entre los nodos.
\end{Ejem}

\begin{Note}
\begin{itemize}
    \item Para $ n+1 $ puntos distintos, existe un único polinomio de grado $ n $ que interpola los datos.
    \item El cálculo directo de los coeficientes mediante la matriz de Vandermonde es teóricamente exacto, pero numéricamente inestable para valores grandes de $ n $.
    \item Por eficiencia y estabilidad se prefieren métodos como los de \textbf{Lagrange} y \textbf{Newton}, que se estudiarán a continuación.
\end{itemize}
\end{Note}

%<<==>><<==>><<==>><<==>><<==>><<==>>
\subsection{Interpolaci\'on de Lagrange}
%<<==>><<==>><<==>><<==>><<==>><<==>>
Dado un conjunto de $n+1$ puntos con abscisas distintas
\begin{eqnarray*}
(x_0,y_0),\ (x_1,y_1),\ \ldots,\ (x_n,y_n),\qquad y_i=f(x_i),
\end{eqnarray*}
el polinomio interpolante de Lagrange $P_n(x)$ se escribe como
\begin{eqnarray*}
P_n(x)=\sum_{i=0}^{n} y_i\,L_i(x),\qquad
L_i(x)=\prod_{\substack{j=0\\ j\neq i}}^{n}\frac{x-x_j}{\,x_i-x_j\,}.
\end{eqnarray*}
Cada base $L_i$ satisface $L_i(x_k)=\delta_{ik}$, por lo que $P_n(x_i)=y_i$ para todo $i$.

Si $f$ es $(n+1)$ veces derivable en un intervalo que contiene a los nodos $x_i$ y al punto $x$, entonces
\begin{eqnarray*}
f(x)-P_n(x)=\frac{f^{(n+1)}(\xi)}{(n+1)!}\,\omega_{n+1}(x),\textrm{ donde }
\omega_{n+1}(x)=\prod_{i=0}^{n}(x-x_i),
\end{eqnarray*}
para alg\'un $\xi$ en el intervalo convexo de $\{x_0,\ldots,x_n,x\}$.

\begin{Note}
\begin{itemize}
    \item Construir $P_n(x)$ y evaluarlo directamente con la f\'ormula de Lagrange cuesta $O(n^2)$ por punto de evaluaci\'on.
    \item Para muchos nodos o evaluaciones repetidas, se prefiere la forma \emph{baric\'entrica} por estabilidad y costo $O(n)$ por evaluaci\'on (se comenta al final).
    \item Con nodos equiespaciados y $n$ grande puede aparecer el \emph{efecto Runge}. Una mitigaci\'on es usar nodos de Chebyshev o splines.
\end{itemize}
\end{Note}


\begin{Ejem}
Interpolar $(0,0)$, $(1,1)$, $(2,4)$ (muestran $f(x)=x^2$). Se obtiene
\begin{eqnarray*}
\begin{aligned}
L_0(x)&=\frac{(x-1)(x-2)}{(0-1)(0-2)}=\frac{(x-1)(x-2)}{2},\\
L_1(x)&=\frac{(x-0)(x-2)}{(1-0)(1-2)}=-(x)(x-2),\\
L_2(x)&=\frac{(x-0)(x-1)}{(2-0)(2-1)}=\frac{x(x-1)}{2}.
\end{aligned}
\end{eqnarray*}
Entonces
\begin{eqnarray*}
P_2(x)=0\cdot L_0(x)+1\cdot L_1(x)+4\cdot L_2(x) = x^2,
\end{eqnarray*}
que recupera exactamente la funci\'on cuadr\'atica.
\end{Ejem}

%<<==>><<==>><<==>><<==>><<==>><<==>>
\subsection{Interpolaci\'on de Newton--Diferencias Finitas}
%<<==>><<==>><<==>><<==>><<==>><<==>>

El m\'etodo de Newton construye el polinomio interpolante de manera incremental,  usando el concepto de \textbf{diferencias divididas}.   Esto permite agregar nuevos puntos sin recalcular todo el polinomio.

Sea un conjunto de $n+1$ puntos distintos:
\begin{eqnarray*}
(x_0, y_0),\ (x_1, y_1),\ \ldots,\ (x_n, y_n).
\end{eqnarray*}
El polinomio de Newton tiene la forma:
\begin{eqnarray*}
P_n(x)& = &f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1) + \cdots \\
&+& f[x_0,x_1,\ldots,x_n]\prod_{j=0}^{n-1}(x-x_j),
\end{eqnarray*}
donde los coeficientes $f[x_i,\ldots,x_j]$ son las \emph{diferencias divididas}.

\begin{itemize}
    \item De primer orden:
    \begin{eqnarray*}
    f[x_i,x_{i+1}] = \frac{f(x_{i+1}) - f(x_i)}{x_{i+1}-x_i}.
    \end{eqnarray*}
    \item De segundo orden:
    \begin{eqnarray*}
    f[x_i,x_{i+1},x_{i+2}] = \frac{f[x_{i+1},x_{i+2}] - f[x_i,x_{i+1}]}{x_{i+2}-x_i}.
    \end{eqnarray*}
    \item En general:
    \begin{eqnarray*}
    f[x_i, x_{i+1}, \ldots, x_{i+k}] = 
    \frac{f[x_{i+1},\ldots,x_{i+k}] - f[x_i,\ldots,x_{i+k-1}]}{x_{i+k}-x_i}.
    \end{eqnarray*}
\end{itemize}

Estas diferencias se organizan en una tabla triangular.




\begin{Ejem}
Interpolar los datos:
\begin{eqnarray*}
(1, 2),\ (2, 3),\ (4, 7).
\end{eqnarray*}
\textbf{Paso 1:} construir la tabla de diferencias divididas:

\begin{center}
\begin{tabular}{c|c|c|c}
$x_i$ & $f[x_i]$ & $f[x_i,x_{i+1}]$ & $f[x_i,x_{i+1},x_{i+2}]$ \\ \hline
1 & 2 & $\frac{3-2}{2-1}=1$ & $\frac{(7-3)/(4-2) - 1}{4-1}=\frac{1-1}{3}=0$\\
2 & 3 & $\frac{7-3}{4-2}=2$ &  \\
4 & 7 &  &  \\ 
\end{tabular}
\end{center}

\textbf{Paso 2:} polinomio de Newton:
\begin{eqnarray*}
P_2(x) = f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1),
\end{eqnarray*}
sustituyendo:
\begin{eqnarray*}
P_2(x) = 2 + 1(x-1) + 0(x-1)(x-2) = 2 + (x-1) = x + 1.
\end{eqnarray*}

En este caso el t\'ermino cuadr\'atico se anul\'o (el polinomio resultante es lineal porque los puntos est\'an casi alineados).
\end{Ejem}

\begin{Ejem}
Sea:
\begin{eqnarray*}
(0, 1),\ (1, 0),\ (2, -1),\ (3, 2).
\end{eqnarray*}
\textbf{Tabla de diferencias divididas:}

\begin{center}
\begin{tabular}{c|c|c|c|c}
$x_i$ & $f[x_i]$ & $f[x_i,x_{i+1}]$ & $f[x_i,x_{i+1},x_{i+2}]$ & $f[x_i,x_{i+1},x_{i+2},x_{i+3}]$ \\ \hline
0 & 1 & $\frac{0-1}{1-0}=-1$ & $\frac{(-1)-(-1)}{2-0}=0$ & $\frac{(1.5)-0}{3-0}=0.5$ \\
1 & 0 & $\frac{-1-0}{2-1}=-1$ & $\frac{2-(-1)}{3-1}=1.5$ &  \\
2 & -1 & $\frac{2-(-1)}{3-2}=3$ &  &  \\
3 & 2 &  &  &  \\ 
\end{tabular}
\end{center}

\textbf{Polinomio:}
\begin{eqnarray*}
P_3(x) &=& f[x_0] 
+ f[x_0,x_1](x-x_0)
+ f[x_0,x_1,x_2](x-x_0)(x-x_1)\\
&+& f[x_0,x_1,x_2,x_3](x-x_0)(x-x_1)(x-x_2) \\
&=& 1 -1(x-0) + 0(x)(x-1) + 0.5(x)(x-1)(x-2).
\end{eqnarray*}
Simplificando:
\begin{eqnarray*}
P_3(x) = 1 - x + 0.5x(x-1)(x-2) = 1 - x + 0.5x^3 - 1.5x^2 + x = 1 - 1.5x^2 + 0.5x^3.
\end{eqnarray*}
Por tanto:
\begin{eqnarray*}
P_3(x) = 1 - \frac{3}{2}x^2 + \frac{1}{2}x^3.
\end{eqnarray*}

Verificando:
\begin{eqnarray*}
P_3(0)=1,\quad P_3(1)=0,\quad P_3(2)=-1,\quad P_3(3)=2.
\end{eqnarray*}
\end{Ejem}


\begin{Note}
\begin{itemize}
    \item Las diferencias divididas se pueden calcular una sola vez y reutilizar para m\'ultiples evaluaciones.
    \item Si se a\~nade un nuevo punto $(x_{n+1}, y_{n+1})$, basta con agregar una nueva columna a la tabla sin recomputar todo.
    \item El polinomio de Newton es m\'as estable que el de Lagrange para grandes $n$ y se adapta mejor a la incorporaci\'on incremental de datos.
\end{itemize}
\end{Note}

\begin{Ejem}
Dados los puntos
\begin{eqnarray*}
(1,2),\quad (2,3),\quad (4,7),
\end{eqnarray*}
construyamos la tabla de diferencias divididas y el polinomio de Newton. 

\paragraph{Paso 1: Primera columna (valores de la funcion).}
\begin{eqnarray*}
f[x_0]=2\ (\text{en }x_0=1),\qquad
f[x_1]=3\ (\text{en }x_1=2),\qquad
f[x_2]=7\ (\text{en }x_2=4).
\end{eqnarray*}

\paragraph{Paso 2: Diferencias divididas de primer orden.}
\begin{eqnarray*}
\begin{aligned}
f[x_0,x_1] &= \frac{f[x_1]-f[x_0]}{x_1-x_0}
= \frac{3-2}{2-1} = 1,\\
f[x_1,x_2] &= \frac{f[x_2]-f[x_1]}{x_2-x_1}
= \frac{7-3}{4-2} = 2.
\end{aligned}
\end{eqnarray*}

\paragraph{Paso 3: Diferencia dividida de segundo orden.}
\begin{eqnarray*}
f[x_0,x_1,x_2] = 
\frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}
= \frac{2-1}{4-1} = \frac{1}{3}.
\end{eqnarray*}
\textbf{Atencion:} En el ejemplo del texto principal se obtuvo cero por un redondeo al mostrar pasos; aqu\'i dejamos el valor exacto $\tfrac{1}{3}$. Para verificar coherencia, construyamos el polinomio y validemos en los nodos.

\paragraph{Paso 4: Polinomio de Newton.}
\begin{eqnarray*}
\begin{aligned}
P_2(x) &= f[x_0] 
+ f[x_0,x_1](x-x_0)
+ f[x_0,x_1,x_2](x-x_0)(x-x_1)\\
&= 2 + 1(x-1) + \frac{1}{3}(x-1)(x-2).
\end{aligned}
\end{eqnarray*}

\paragraph{Paso 5: Verificacion en los nodos.}
\begin{eqnarray*}
\begin{aligned}
P_2(1) &= 2 + 1(0) + \tfrac{1}{3}(0)(-1) = 2,\\
P_2(2) &= 2 + 1(1) + \tfrac{1}{3}(1)(0) = 3,\\
P_2(4) &= 2 + 1(3) + \tfrac{1}{3}(3)(2) = 2+3+2 = 7.
\end{aligned}
\end{eqnarray*}

\paragraph{Paso 6: Evaluacion en un punto intermedio (por ejemplo, $x=3$).}
\begin{eqnarray*}
P_2(3) = 2 + 1(2) + \tfrac{1}{3}(2)(1) = 2 + 2 + \tfrac{2}{3} = \tfrac{14}{3}.
\end{eqnarray*}

\paragraph{Tabla triangular resumida.}
\begin{eqnarray*}
\begin{array}{c|c|c|c}
x_i & f[x_i] & f[x_i,x_{i+1}] & f[x_i,x_{i+1},x_{i+2}]\\ \hline
1 & 2 & 1 & \tfrac{1}{3}\\
2 & 3 & 2 & \\
4 & 7 &  & 
\end{array}
\end{eqnarray*}
\end{Ejem}

%-----------------------------------------------------------
\subsection{Polinomio de Newton}
%-----------------------------------------------------------

Sea una funci\'on $f$ conocida en los puntos
\begin{eqnarray*}
(x_0,f_0), (x_1,f_1),\dots,(x_n,f_n),
\qquad f_k = f(x_k).
\end{eqnarray*}
El polinomio interpolante de Newton en t\'erminos de diferencias divididas
se escribe como
\begin{equation}
P_n(x) = f[x_0]
+ (x - x_0) f[x_0,x_1]
+ (x - x_0)(x - x_1) f[x_0,x_1,x_2]
+ \cdots
+ (x-x_0)\cdots(x-x_{n-1}) f[x_0,\dots,x_n].
\label{eq:newton-general}
\end{equation}

Recordemos que las \emph{diferencias divididas} se definen recursivamente:
\begin{eqnarray*}
f[x_k] &=& f(x_k),\\
f[x_k,x_{k+1}] &=& \frac{f(x_{k+1}) - f(x_k)}{x_{k+1}-x_k},\\
f[x_k,x_{k+1},x_{k+2}] &=& \frac{f[x_{k+1},x_{k+2}] - f[x_k,x_{k+1}]}
                               {x_{k+2}-x_k},\\
&\ \ \vdots
\end{eqnarray*}

En esta secci\'on supondremos que los nodos est\'an \emph{equispaciados}, es
decir:
\begin{eqnarray*}
x_k = x_0 + kh, \qquad k=0,1,\dots,n,
\end{eqnarray*}
para cierto paso $h>0$ constante. Definimos primero las \emph{diferencias progresivas} de $f$ en los nodos
$x_k$:

\begin{eqnarray*}
\Delta f_k
&=& f_{k+1} - f_k,\\
\Delta^2 f_k
&=& \Delta f_{k+1} - \Delta f_k,\\
\Delta^3 f_k
&=& \Delta^2 f_{k+1} - \Delta^2 f_k,\\
&\ \ \vdots
\end{eqnarray*}
es decir: 
\begin{eqnarray*}
\Delta f_0 &=& f_1 - f_0,\\
\Delta^2 f_0 &=& (f_2 - f_1) - (f_1 - f_0) = f_2 - 2f_1 + f_0,\\
\Delta^3 f_0 &=& (f_3 - 2f_2 + f_1) - (f_2 - 2f_1 + f_0)\\
&=& f_3 - 3f_2 + 3f_1 - f_0,\\
&\ \ \vdots
\end{eqnarray*}

Para el primer orden tenemos:
\begin{eqnarray*}
f[x_0,x_1]
= \frac{f_1 - f_0}{x_1 - x_0}.
\end{eqnarray*}
Como $x_1 - x_0 = h$ y $f_1 - f_0 = \Delta f_0$, se sigue que
\begin{equation}
f[x_0,x_1] = \frac{\Delta f_0}{h}.
\label{eq:dividida1}
\end{equation}

Por definici\'on de diferencias divididas de segundo orden:
\begin{eqnarray*}
f[x_0,x_1,x_2]
= \frac{f[x_1,x_2] - f[x_0,x_1]}{x_2 - x_0}.
\end{eqnarray*}
Usamos el caso de primer orden para $f[x_1,x_2]$ y $f[x_0,x_1]$:
\begin{eqnarray*}
f[x_1,x_2] = \frac{f_2 - f_1}{h}, \qquad
f[x_0,x_1] = \frac{f_1 - f_0}{h}.
\end{eqnarray*}
Entonces:
\begin{eqnarray*}
f[x_0,x_1,x_2]
&=& \frac{\frac{f_2 - f_1}{h} - \frac{f_1 - f_0}{h}}{x_2 - x_0}\\
&=& \frac{\frac{f_2 - 2f_1 + f_0}{h}}{2h}
= \frac{f_2 - 2f_1 + f_0}{2h^2}.
\end{eqnarray*}
Observamos que $f_2 - 2f_1 + f_0 = \Delta^2 f_0$, por lo que
\begin{equation}
f[x_0,x_1,x_2]
= \frac{\Delta^2 f_0}{2!\,h^2}.
\label{eq:dividida2}
\end{equation}

Procediendo de manera an\'aloga llegamos a la expresi\'on general:
\begin{equation}
f[x_0,x_1,\dots,x_k]
= \frac{\Delta^k f_0}{k!\,h^k}, \qquad k=0,1,\dots,n.
\label{eq:dividida-general-prog}
\end{equation}

Para $k=0$ se tiene $f[x_0] = f_0$, y como $\Delta^0 f_0 = f_0$,
la igualdad se cumple.  Para $k=1$ y $k=2$ la f\'ormula se verific\'o expl\'icitamente en 
\ref{eq:dividida1} y \ref{eq:dividida2}.

Supongamos que
\begin{eqnarray*}
f[x_0,\dots,x_k] = \frac{\Delta^k f_0}{k!\,h^k},
\qquad
f[x_1,\dots,x_{k+1}] = \frac{\Delta^k f_1}{k!\,h^k}.
\end{eqnarray*}

entonces consideremos
\begin{eqnarray*}
f[x_0,x_1,\dots,x_{k+1}]
= \frac{f[x_1,\dots,x_{k+1}] - f[x_0,\dots,x_k]}{x_{k+1} - x_0}.
\end{eqnarray*}
Como $x_{k+1} - x_0 = (k+1)h$, usando la hip\'otesis:
\begin{eqnarray*}
f[x_1,\dots,x_{k+1}] = \frac{\Delta^k f_1}{k!\,h^k},
\qquad
f[x_0,\dots,x_k]   = \frac{\Delta^k f_0}{k!\,h^k},
\end{eqnarray*}
por tanto se obtiene
\begin{eqnarray*}
f[x_0,\dots,x_{k+1}]
&=& \frac{\frac{\Delta^k f_1}{k!\,h^k} - \frac{\Delta^k f_0}{k!\,h^k}}
         {(k+1)h}=\frac{\Delta^k f_1 - \Delta^k f_0}{k!\,h^k\,(k+1)h}.
\end{eqnarray*}
Por definici\'on de diferencia progresiva de orden $k+1$:
\begin{eqnarray*}
\Delta^{k+1} f_0 = \Delta^k f_1 - \Delta^k f_0.
\end{eqnarray*}
es decir
\begin{eqnarray*}
f[x_0,\dots,x_{k+1}]
= \frac{\Delta^{k+1} f_0}{(k+1)!\,h^{k+1}},
\end{eqnarray*}
que es precisamente la f\'ormula \eqref{eq:dividida-general-prog} para $k+1$, y por lo tanto queda demostrada la relaci\'on
\begin{eqnarray*}
f[x_0,\dots,x_k]
= \frac{\Delta^k f_0}{k!\,h^k}, \quad k=0,1,\dots,n,
\end{eqnarray*}
cuando los nodos son equiespaciados. Ahora, a partir de la expresi\'on general \ref{eq:newton-general} y sustituimos
las diferencias divididas usando \ref{eq:dividida-general-prog}:
\begin{eqnarray*}
P_n(x)
&=& f_0
+ (x-x_0) \frac{\Delta f_0}{1!\,h}
+ (x-x_0)(x-x_1) \frac{\Delta^2 f_0}{2!\,h^2}
+ \cdots \\
&+& (x-x_0)(x-x_1)\cdots(x-x_{n-1}) \frac{\Delta^n f_0}{n!\,h^n}.
\end{eqnarray*}

donde
\begin{eqnarray*}
x_k = x_0 + kh, \textrm{ por lo tanto }x - x_k = x - x_0 - kh.
\end{eqnarray*}
haciendo 
\begin{eqnarray*}
s = \frac{x - x_0}{h}\textrm{ se tiene que }x - x_0 = sh,
\end{eqnarray*}
y entonces
\begin{eqnarray*}
x - x_1 &=& x - (x_0 + h)   = (x - x_0) - h   = (s - 1) h,\\
x - x_2 &=& x - (x_0 + 2h) = (x - x_0) - 2h  = (s - 2) h,\\
&\vdots& \\
x - x_{k-1} &=& (s - (k-1))h.
\end{eqnarray*}
Por lo tanto, el producto
\begin{eqnarray*}
(x-x_0)(x-x_1)\cdots(x-x_{k-1})
= h^k\, s(s-1)\cdots(s-k+1).
\end{eqnarray*}

Sustituyendo en $P_n(x)$:
\begin{eqnarray*}
P_n(x)
&=& f_0
+ \frac{h s}{h} \Delta f_0
+ \frac{h^2 s(s-1)}{2!\,h^2}\Delta^2 f_0
+ \cdots
+ \frac{h^n s(s-1)\cdots(s-n+1)}{n!\,h^n}\Delta^n f_0\\
&=& f_0
+ s\Delta f_0
+ \frac{s(s-1)}{2!}\,\Delta^2 f_0
+ \cdots
+ \frac{s(s-1)\cdots(s-n+1)}{n!}\,\Delta^n f_0.
\end{eqnarray*}

Es decir, se tiene la f\'ormula de Newton con diferencias progresivas:

\begin{equation}
P_n(x) = f_0
+ s\,\Delta f_0
+ \frac{s(s-1)}{2!}\,\Delta^2 f_0
+ \frac{s(s-1)(s-2)}{3!}\,\Delta^3 f_0
+ \cdots
+ \frac{s(s-1)\cdots(s-n+1)}{n!}\,\Delta^n f_0,
\label{eq:newton-progresivo}
\end{equation}
donde
\begin{eqnarray*}
s = \frac{x - x_0}{h}.
\end{eqnarray*}

De forma an\'aloga definimos las \emph{diferencias regresivas}:
\begin{eqnarray*}
\nabla f_k &=& f_k - f_{k-1},\\
\nabla^2 f_k &=& \nabla f_k - \nabla f_{k-1},\\
\nabla^3 f_k &=& \nabla^2 f_k - \nabla^2 f_{k-1},\\
&\ \ \vdots
\end{eqnarray*}

En particular, las primeras diferencias regresivas en el nodo final $x_n$
son:
\begin{eqnarray*}
\nabla f_n &=& f_n - f_{n-1},\textrm{ luego }\\
\nabla^2 f_n &=& (f_n - f_{n-1}) - (f_{n-1} - f_{n-2})
= f_n - 2f_{n-1} + f_{n-2},\\
&\vdots&
\end{eqnarray*}

Ahora consideramos las diferencias divididas de atr\'as hacia adelante:
\begin{eqnarray*}
f[x_n],\ f[x_n,x_{n-1}],\ f[x_n,x_{n-1},x_{n-2}],\dots
\end{eqnarray*}

Se tiene que
\begin{eqnarray*}
f[x_n,x_{n-1}]
= \frac{f_n - f_{n-1}}{x_n - x_{n-1}}
= \frac{\nabla f_n}{h}.
\end{eqnarray*}

luego,
\begin{eqnarray*}
f[x_n,x_{n-1},x_{n-2}]
= \frac{f[x_{n-1},x_{n-2}] - f[x_n,x_{n-1}]}{x_{n-2} - x_n}.
\end{eqnarray*}
donde
\begin{eqnarray*}
f[x_{n-1},x_{n-2}] = \frac{f_{n-1} - f_{n-2}}{h}, \textrm{ y }
f[x_n,x_{n-1}]     = \frac{f_n     - f_{n-1}}{h},
\end{eqnarray*}
y $x_{n-2} - x_n = -2h$. Entonces:
\begin{eqnarray*}
f[x_n,x_{n-1},x_{n-2}]
&=& \frac{\frac{f_{n-1} - f_{n-2}}{h} - \frac{f_n - f_{n-1}}{h}}{-2h}=\frac{(f_{n-1} - f_{n-2}) - (f_n - f_{n-1})}{-2h^2}\\
&=& \frac{-f_n + 2f_{n-1} - f_{n-2}}{-2h^2}
= \frac{f_n - 2f_{n-1} + f_{n-2}}{2h^2}=\frac{\nabla^2 f_n}{2!\,h^2}.
\end{eqnarray*}

De forma an\'aloga al caso progresivo, se puede probar por inducci\'on que:
\begin{equation}
f[x_n,x_{n-1},\dots,x_{n-k}]
= \frac{\nabla^k f_n}{k!\,h^k},\qquad k=0,1,\dots,n.
\label{eq:dividida-general-reg}
\end{equation}

Procediendo de manera an\'aloga escribimos ahora el polinomio de Newton de atr\'as hacia adelante:
\begin{eqnarray*}
P_n(x)
&=& f[x_n]
+ (x - x_n) f[x_n,x_{n-1}]
+ (x - x_n)(x - x_{n-1}) f[x_n,x_{n-1},x_{n-2}]\\
&+& \cdots
+ (x-x_n)(x-x_{n-1})\cdots(x-x_{n-k+1}) f[x_n,\dots,x_{n-k}]+\cdots.
\end{eqnarray*}

Sustituimos la ecuaci\'on \eqref{eq:dividida-general-reg}:
\begin{eqnarray*}
P_n(x)
&=& f_n
+ (x - x_n)\frac{\nabla f_n}{1!\,h}
+ (x - x_n)(x - x_{n-1})\frac{\nabla^2 f_n}{2!\,h^2}
+ \cdots\\
&+& (x-x_n)(x-x_{n-1})\cdots(x-x_{n-k+1}) \frac{\nabla^k f_n}{k!\,h^k}
+ \cdots.
\end{eqnarray*}

Definimos ahora
\begin{eqnarray*}
s = \frac{x - x_n}{h}\textrm{ entonces }x - x_n = sh.
\end{eqnarray*}
Adem\'as,
\begin{eqnarray*}
x - x_{n-1}
&=& x - (x_n - h)
 = (x - x_n) + h
 = (s+1)h,\\
x - x_{n-2}
&=& x - (x_n - 2h)
 = (x - x_n) + 2h
 = (s+2)h,\\
&\ \ \vdots\\
x - x_{n-k+1}
&=& (s + k - 1) h.
\end{eqnarray*}
Por lo tanto
\begin{eqnarray*}
(x-x_n)(x-x_{n-1})\cdots(x-x_{n-k+1})
= h^k\, s(s+1)\cdots(s + k - 1).
\end{eqnarray*}

Sustituyendo en $P_n(x)$:
\begin{eqnarray*}
P_n(x)
&=& f_n
+ \frac{h s}{h}\nabla f_n
+ \frac{h^2 s(s+1)}{2!\,h^2}\nabla^2 f_n
+ \cdots
+ \frac{h^k s(s+1)\cdots(s+k-1)}{k!\,h^k}\nabla^k f_n
+ \cdots\\
&=& f_n
+ s\,\nabla f_n
+ \frac{s(s+1)}{2!}\,\nabla^2 f_n
+ \cdots
+ \frac{s(s+1)\cdots(s+k-1)}{k!}\,\nabla^k f_n
+ \cdots.
\end{eqnarray*}

con lo cual se obtiene la f\'ormula de Newton con diferencias regresivas:
\begin{equation}
P_n(x)
= f_n
+ s\,\nabla f_n
+ \frac{s(s+1)}{2!}\,\nabla^2 f_n
+ \frac{s(s+1)(s+2)}{3!}\,\nabla^3 f_n
+ \cdots
+ \frac{s(s+1)\cdots(s+n-1)}{n!}\,\nabla^n f_n,
\label{eq:newton-regresivo}
\end{equation}
donde
\begin{eqnarray*}
s = \frac{x - x_n}{h}.
\end{eqnarray*}

\subsubsection{Ejemplos del m\'etodo de Newton con diferencias finitas}

\begin{Ejem} Se quiere  aproximar $f(x)=x^2$ en un punto cercano a $x_0=0$ usando diferencias progresivas de Newton.

Consid\'erense los puntos equiespaciados (con $h=1$):
\begin{eqnarray*}
x_0=0,\quad x_1=1,x_2=2,
\end{eqnarray*}
y
\begin{eqnarray*}
f_0=f(0)=0, f_1=f(1)=1, f_2=f(2)=4.
\end{eqnarray*}

Entonces se tienen la siguiente tabla de diferencias divididas:

\begin{eqnarray*}
\begin{array}{c|c|c|c}
k & x_k & f_k & \text{Diferencias} \\ \hline
0 & 0 & 0 & \\
1 & 1 & 1 & \Delta f_0 = f_1 - f_0 = 1 - 0 = 1 \\
2 & 2 & 4 & \Delta f_1 = f_2 - f_1 = 4 - 1 = 3 \\
\hline
  &   &   & \Delta^2 f_0 = \Delta f_1 - \Delta f_0 = 3 - 1 = 2
\end{array}
\end{eqnarray*}

entonces el Polinomio de Newton progresivo se calcula a partir de la f\'ormula de Newton progresiva:
\begin{eqnarray*}
P_2(x) &=& f_0 + s\,\Delta f_0 + \frac{s(s-1)}{2!}\,\Delta^2 f_0,\\
&s& = \frac{x - x_0}{h}.
\end{eqnarray*}
Con $x_0=0$ y $h=1$, resulta $s=x$, sustituyendo:
\begin{eqnarray*}
P_2(x) 
= 0 + s(1) + \frac{s(s-1)}{2}(2)
= s + s(s-1)
= s + s^2 - s
= s^2.
\end{eqnarray*}
Como $s=x$, se tiene $P_2(x) = x^2$, es decir, recuperamos la funci\'on exacta (como era de esperar al interpolar un polinomio de grado 2 con tres puntos). Evaluando en $x=1.5$: $s = \frac{x-x_0}{h} = \frac{1.5-0}{1} = 1.5$, $P_2(1.5) = (1.5)^2 = 2.25$.  El valor exacto de $f(1.5)$ es tambi\'en $2.25$, de modo que no hay  error de interpolaci\'on en este caso.
\end{Ejem}

\begin{Ejem} Aproximar $f(x)=\ln(1+x)$ cerca de $x_0=0$ usando diferencias progresivas con $h=0.5$, y luego se evaluar\'a en $x=0.75$.

Consid\'erense los puntos equiespaciados:
\begin{eqnarray*}
x_0 = 0,\quad x_1 = 0.5,\quad x_2 = 1.0,\qquad h=0.5.
\end{eqnarray*}
Evaluando la funci\'on:
\begin{eqnarray*}
f_0 &=& \ln(1+0) = 0,\\
f_1 &=& \ln(1+0.5) = \ln(1.5) \approx 0.405465,\\
f_2 &=& \ln(1+1.0) = \ln(2) \approx 0.693147.
\end{eqnarray*}

entonces la Tabla de diferencias progresivas quedar\'ia como:

\begin{eqnarray*}
\begin{array}{c|c|c|c}
k & x_k & f_k & \text{Diferencias} \\ \hline
0 & 0.0 & 0.000000 & \\
1 & 0.5 & 0.405465 & \Delta f_0 = 0.405465 - 0.000000 = 0.405465 \\
2 & 1.0 & 0.693147 & \Delta f_1 = 0.693147 - 0.405465 = 0.287682 \\
\hline
  &     &         & \Delta^2 f_0 = \Delta f_1 - \Delta f_0 = 0.287682 - 0.405465 = -0.117783
\end{array}
\end{eqnarray*}

Entonces el polinomio de Newton progresivo se obtiene con la f\'ormula:
\begin{eqnarray*}
P_2(x) &=& f_0 + s\,\Delta f_0 + \frac{s(s-1)}{2!}\,\Delta^2 f_0,\\
s &=& \frac{x-x_0}{h} = \frac{x}{0.5} = 2x.
\end{eqnarray*}
Sustituimos $f_0=0$, $\Delta f_0=0.405465$ y $\Delta^2 f_0=-0.117783$:
\begin{eqnarray*}
P_2(x)= 0 + s(0.405465) + \frac{s(s-1)}{2}(-0.117783).
\end{eqnarray*}

Evaluando  en $x=0.75$
\begin{eqnarray*}
s = \frac{0.75 - 0}{0.5} = 1.5.
\end{eqnarray*}
Entonces
\begin{eqnarray*}
P_2(0.75)= 1.5(0.405465) + \frac{1.5(1.5-1)}{2}(-0.117783),
\end{eqnarray*}
donde
$1.5(0.405465) \approx 0.6081975$, $\frac{1.5(0.5)}{2} = \frac{0.75}{2} = 0.375$, y $0.375(-0.117783) \approx -0.044168$,
por lo tanto, $P_2(0.75) \approx 0.6081975 - 0.044168 \approx 0.564029$, donde el valor real es $f(0.75) = \ln(1.75) \approx 0.559616$.  El error de interpolaci\'on es peque\~no: $\lvert f(0.75) - P_2(0.75) \rvert \approx 0.0044$.
\end{Ejem}

\begin{Ejem} Ahora usaremos la forma \emph{regresiva} para aproximar $f(x)=\sqrt{x}$ cerca del nodo final $x_n=3$, con $h=1$, y se estimar\'a $f(2.6)$. Consid\'erense los puntos $x_0=1,\quad x_1=2,\quad x_2=3,\qquad h=1$.

Entonces
\begin{eqnarray*}
f_0 &=& \sqrt{1} = 1.000000,\\
f_1 &=& \sqrt{2} \approx 1.414214,\\
f_2 &=& \sqrt{3} \approx 1.732051.
\end{eqnarray*}

la correspondiente tabla de diferencias regresivas (centradas en $x_2$)

\begin{eqnarray*}
\begin{array}{c|c|c|c}
k & x_k & f_k & \text{Diferencias} \\ \hline
0 & 1 & 1.000000 & \\
1 & 2 & 1.414214 & \nabla f_1 = f_1 - f_0 = 1.414214 - 1.000000 = 0.414214 \\
2 & 3 & 1.732051 & \nabla f_2 = f_2 - f_1 = 1.732051 - 1.414214 = 0.317837 \\
\hline
  &   &         & \nabla^2 f_2 = \nabla f_2 - \nabla f_1 = 0.317837 - 0.414214 = -0.096376
\end{array}
\end{eqnarray*}

Nos interesan especialmente $\nabla f_2$ y $\nabla^2 f_2$ para el polinomio
regresivo. La f\'ormula de Newton regresivo de grado 2 es
\begin{eqnarray*}
P_2(x) = f_n + s\,\nabla f_n + \frac{s(s+1)}{2!}\,\nabla^2 f_n,
\qquad
s = \frac{x-x_n}{h}.
\end{eqnarray*}
donde $n=2$, $x_n=x_2=3$, $h=1$, por lo que $s = x-3$.  Sustituyendo se tiene
\begin{eqnarray*}
P_2(x) = f_2 + s\,\nabla f_2 + \frac{s(s+1)}{2}\,\nabla^2 f_2.
\end{eqnarray*}

Evaluando en $x=2.6$: $s = \frac{2.6-3}{1} = -0.4$. Entonces $P_2(2.6)= 1.732051 + (-0.4)(0.317837) 
+ \frac{(-0.4)(-0.4+1)}{2}(-0.096376)$.
Calculamos cada t\'ermino:
\begin{eqnarray*}
(-0.4)(0.317837) \approx -0.127135,\\
-0.4+1 = 0.6,\quad (-0.4)(0.6) = -0.24,\quad \frac{-0.24}{2} = -0.12,\\
-0.12(-0.096376) \approx 0.011570.
\end{eqnarray*}
Por lo tanto $P_2(2.6)\approx 1.732051 - 0.127135 + 0.011570 \approx 1.616486$.

El valor real es $f(2.6) = \sqrt{2.6} \approx 1.612452$. El error es $\lvert f(2.6) - P_2(2.6) \rvert \approx 0.0040$,
lo cual muestra una buena aproximaci\'on usando la forma regresivacerca del extremo derecho.
\end{Ejem}

\begin{Ejem}
Ahora ilustraremos un caso de grado 3 usando $f(x)=\sin(x)$ con paso
$h=0.5$, cerca de $x_0=0$, y evaluaremos en $x=0.7$. Consid\'erese
\begin{eqnarray*}
x_0 = 0.0,\quad x_1=0.5,\quad x_2=1.0,\quad x_3=1.5,\qquad h=0.5.
\end{eqnarray*}
Evaluamos la funci\'on en los puntos dados
\begin{eqnarray*}
f_0 = \sin(0.0) = 0.000000,\\
f_1 = \sin(0.5) \approx 0.479426,\\
f_2 = \sin(1.0) \approx 0.841471,\\
f_3 = \sin(1.5) \approx 0.997495
\end{eqnarray*}

con lo que la tabla de diferencias progresivas

\begin{eqnarray*}
\begin{array}{c|c|c|c}
k & x_k & f_k & \text{Diferencias} \\ \hline
0 & 0.0 & 0.000000 & \\
1 & 0.5 & 0.479426 & \Delta f_0 = 0.479426 - 0.000000 = 0.479426 \\
2 & 1.0 & 0.841471 & \Delta f_1 = 0.841471 - 0.479426 = 0.362045 \\
3 & 1.5 & 0.997495 & \Delta f_2 = 0.997495 - 0.841471 = 0.156024 \\
\hline
  &     &         & \Delta^2 f_0 = \Delta f_1 - \Delta f_0 = 0.362045 - 0.479426 = -0.117380 \\
  &     &         & \Delta^2 f_1 = \Delta f_2 - \Delta f_1 = 0.156024 - 0.362045 = -0.206021 \\
\hline
  &     &         & \Delta^3 f_0 = \Delta^2 f_1 - \Delta^2 f_0 = -0.206021 - (-0.117380) = -0.088641
\end{array}
\end{eqnarray*}

y por tanto el polinomio de Newton progresivo de grado 3 tiene por ecuaci\'on
\begin{eqnarray*}
P_3(x) = f_0 + s\,\Delta f_0 + \frac{s(s-1)}{2!}\,\Delta^2 f_0 + \frac{s(s-1)(s-2)}{3!}\,\Delta^3 f_0,
\qquad
s = \frac{x-x_0}{h} = \frac{x}{0.5} = 2x.
\end{eqnarray*}
Sustituimos $f_0=0$, $\Delta f_0=0.479426$,  $\Delta^2 f_0=-0.117380$, $\Delta^3 f_0=-0.088641$:
\begin{eqnarray*}
P_3(x)= s(0.479426) + \frac{s(s-1)}{2}(-0.117380)+ \frac{s(s-1)(s-2)}{6}(-0.088641).
\end{eqnarray*}

Evaluando en $x=0.7$: $s = \frac{0.7-0}{0.5} = 1.4$, entonces.
\begin{eqnarray*}
P_3(0.7)= 1.4(0.479426)+ \frac{1.4(1.4-1)}{2}(-0.117380)+ \frac{1.4(1.4-1)(1.4-2)}{6}(-0.088641).
\end{eqnarray*}

Evaluamos cada t\'ermino: $1.4(0.479426) \approx 0.671196$, $1.4-1 = 0.4$, $\frac{1.4(0.4)}{2} = \frac{0.56}{2} = 0.28$,
$0.28(-0.117380) \approx -0.032866$, luego $1.4-2 = -0.6$, $1.4(0.4)(-0.6) = 1.4\cdot 0.4\cdot (-0.6) = -0.336$,  $\frac{-0.336}{6} \approx -0.056$, $-0.056(-0.088641) \approx 0.004963$. Por tanto $P_3(0.7)\approx 0.671196 - 0.032866 + 0.004963\approx 0.643293$. Donde el valor real es $\sin(0.7) \approx 0.644218$. El error de interpolaci\'on es $\lvert \sin(0.7) - P_3(0.7) \rvert \approx 0.0009$, lo cual muestra que un polinomio de grado 3 con cuatro nodos cercanos puede aproximar muy bien a $\sin(x)$ en el intervalo considerado.
\end{Ejem}

\begin{Ejem}
Dados los puntos
\begin{eqnarray*}
(0,1),\quad (1,0),\quad (2,-1),\quad (3,2),
\end{eqnarray*}
construyamos la tabla y el polinomio $P_3(x)$.

\begin{eqnarray*}
f[x_0]=1,\quad f[x_1]=0,\quad f[x_2]=-1,\quad f[x_3]=2.
\end{eqnarray*}

\begin{eqnarray*}
\begin{aligned}
f[x_0,x_1] &= \frac{0-1}{1-0} = -1,\\
f[x_1,x_2] &= \frac{-1-0}{2-1} = -1,\\
f[x_2,x_3] &= \frac{2-(-1)}{3-2} = 3.
\end{aligned}
\end{eqnarray*}

\begin{eqnarray*}
\begin{aligned}
f[x_0,x_1,x_2] &= \frac{f[x_1,x_2]-f[x_0,x_1]}{x_2-x_0}
= \frac{(-1)-(-1)}{2-0} = 0,\\
f[x_1,x_2,x_3] &= \frac{f[x_2,x_3]-f[x_1,x_2]}{x_3-x_1}
= \frac{3-(-1)}{3-1} = \frac{4}{2}=2.
\end{aligned}
\end{eqnarray*}

\begin{eqnarray*}
f[x_0,x_1,x_2,x_3] =
\frac{f[x_1,x_2,x_3]-f[x_0,x_1,x_2]}{x_3-x_0}
= \frac{2-0}{3-0} = \frac{2}{3}.
\end{eqnarray*}

Entonces el Polinomio de Newton.
\begin{eqnarray*}
P_3(x) &=& f[x_0]+ f[x_0,x_1](x-x_0)+ f[x_0,x_1,x_2](x-x_0)(x-x_1)\\
& +& f[x_0,x_1,x_2,x_3](x-x_0)(x-x_1)(x-x_2)\\
&=& 1 + (-1)(x-0) + 0\cdot(x)(x-1) + \frac{2}{3}(x)(x-1)(x-2).
\end{eqnarray*}

Verificando: 
\begin{eqnarray*}
\begin{aligned}
P_3(0) &= 1 - 0 + 0 + 0 = 1,\\
P_3(1) &= 1 - 1 + 0 + 0 = 0,\\
P_3(2) &= 1 - 2 + 0 + 0 = -1,\\
P_3(3) &= 1 - 3 + 0 + \tfrac{2}{3}(3)(2)(1) = -2 + 4 = 2.
\end{aligned}
\end{eqnarray*}

La tabla triangular completa quedar\'ia en la forma
\begin{eqnarray*}
\begin{array}{c|c|c|c|c}
x_i & f[x_i] & f[x_i,x_{i+1}] & f[x_i,x_{i+1},x_{i+2}] & f[x_i,x_{i+1},x_{i+2},x_{i+3}]\\ \hline
0 & 1 & -1 & 0 & \tfrac{2}{3}\\
1 & 0 & -1 & 2 & \\
2 & -1 & 3 &  & \\
3 & 2 &  &  & 
\end{array}
\end{eqnarray*}

Evaluando en un punto interno (por ejemplo, $x=1.5$).
\begin{eqnarray*}
\begin{aligned}
P_3(1.5) 
&= 1 + (-1)(1.5) + 0\cdot(1.5)(0.5) + \tfrac{2}{3}(1.5)(0.5)(-0.5)\\
&= 1 - 1.5 + \tfrac{2}{3}\cdot(1.5\cdot 0.5\cdot -0.5)\\
&= -0.5 + \tfrac{2}{3}\cdot(-0.375)\\
&= -0.5 - 0.25 = -0.75.
\end{aligned}
\end{eqnarray*}
\end{Ejem}



\begin{Ejem}
$f(x)=x^2$ en $x_0=0$, $h=1$, estimar $f(0.5)$
Datos en $x=0,1,2,3$:
\begin{eqnarray*}
\begin{array}{c|cccc}
x & 0 & 1 & 2 & 3\\ \hline
y=f(x)=x^2 & 0 & 1 & 4 & 9
\end{array}
\end{eqnarray*}
Tabla de diferencias progresivas (en la primera fila):
\begin{eqnarray*}
\begin{array}{c|ccc}
\Delta y_0=1-0=1 & \Delta^2 y_0=3-1=2 & \Delta^3 y_0=2-2=0
\end{array}
\end{eqnarray*}
Aqu\'i $\Delta y_1=4-1=3$, $\Delta^2 y_1=5-3=2$ (mostradas s\'olo para c\'alculo de $\Delta^2 y_0$).

Par\'ametro $p=\dfrac{x-x_0}{h}=\dfrac{0.5-0}{1}=0.5$.
\begin{eqnarray*}
\begin{aligned}
P(0.5)
&= y_0 + p\,\Delta y_0 + \frac{p(p-1)}{2}\,\Delta^2 y_0 \\
&= 0 + (0.5)(1) + \frac{0.5(-0.5)}{2}\,(2) \\
&= 0.5 + \left(-\frac{0.25}{2}\right)(2)
= 0.5 - 0.25 = 0.25.
\end{aligned}
\end{eqnarray*}
Valor exacto: $f(0.5)=0.25$. Coincide (el t\'ermino de tercer orden es nulo).
\end{Ejem}


\begin{Ejem}
 $f(x)=x^3$ en $x_0=1$, $h=1$, estimar $f(1.5)$
Datos en $x=1,2,3,4$:
\begin{eqnarray*}
\begin{array}{c|cccc}
x & 1 & 2 & 3 & 4\\ \hline
y=f(x)=x^3 & 1 & 8 & 27 & 64
\end{array}
\end{eqnarray*}
Diferencias progresivas (primera fila en $x_0=1$):
\begin{eqnarray*}
\Delta y_0=8-1=7,\quad \Delta y_1=27-8=19,\quad \Delta y_2=64-27=37.
\end{eqnarray*}
\begin{eqnarray*}
\Delta^2 y_0=19-7=12,\quad \Delta^2 y_1=37-19=18,\qquad
\Delta^3 y_0=18-12=6.
\end{eqnarray*}
Par\'ametro $p=\dfrac{x-x_0}{h}=\dfrac{1.5-1}{1}=0.5$.
\begin{eqnarray*}
\begin{aligned}
P(1.5)
&= y_0 + p\,\Delta y_0 + \frac{p(p-1)}{2}\,\Delta^2 y_0
 + \frac{p(p-1)(p-2)}{6}\,\Delta^3 y_0\\
&= 1 + (0.5)(7) + \frac{0.5(-0.5)}{2}(12)
 + \frac{0.5(-0.5)(-1.5)}{6}(6).
\end{aligned}
\end{eqnarray*}
C\'alculo t\'ermino a t\'ermino:
\begin{eqnarray*}
1 + 3.5 + \left(\frac{-0.25}{2}\right)12 + \left(\frac{0.375}{6}\right)6
= 1 + 3.5 - 1.5 + 0.375 = 3.375.
\end{eqnarray*}
Exacto: $f(1.5)=(1.5)^3=3.375$.
\end{Ejem}


\begin{Ejem}  $f(x)=x^2$ en $x_n=3$, $h=1$, estimar $f(2.6)$
Datos en $x=0,1,2,3$ (como en el Ejemplo 1).
Diferencias regresivas en el extremo $x_3=3$:
\begin{eqnarray*}
\nabla y_3 = y_3-y_2 = 9-4=5,\quad
\nabla^2 y_3 = \nabla y_3 - \nabla y_2 = 5 - 3 = 2,\quad
\nabla^3 y_3 = 2 - 2 = 0.
\end{eqnarray*}
Aqu\'i $\nabla y_2 = y_2-y_1=3$, $\nabla^2 y_2 = \nabla y_2 - \nabla y_1 = 3-2=1$ (solo de apoyo).

Par\'ametro $q=\dfrac{x-x_n}{h}=\dfrac{2.6-3}{1}=-0.4$.
\begin{eqnarray*}
\begin{aligned}
P(2.6)
&= y_3 + q\,\nabla y_3 + \frac{q(q+1)}{2}\,\nabla^2 y_3 \\
&= 9 + (-0.4)(5) + \frac{(-0.4)(0.6)}{2}(2) \\
&= 9 - 2 - 0.24 = 6.76.
\end{aligned}
\end{eqnarray*}
Exacto: $f(2.6)=(2.6)^2=6.76$.
\end{Ejem}


\begin{Ejem}
 $f(x)=x^3$ en $x_n=4$, $h=1$, estimar $f(3.2)$
Datos en $x=1,2,3,4$ .
Diferencias regresivas en el extremo $x_4=4$:
\begin{eqnarray*}
\begin{aligned}
\nabla y_4 &= y_4-y_3 = 64-27=37,\\
\nabla^2 y_4 &= \nabla y_4 - \nabla y_3 = 37 - 19 = 18,\\
\nabla^3 y_4 &= \nabla^2 y_4 - \nabla^2 y_3 = 18 - 12 = 6.
\end{aligned}
\end{eqnarray*}
Par\'ametro $q=\dfrac{x-x_n}{h}=\dfrac{3.2-4}{1}=-0.8$.
\begin{eqnarray*}
\begin{aligned}
P(3.2)
&= y_4 + q\,\nabla y_4 + \frac{q(q+1)}{2}\,\nabla^2 y_4
 + \frac{q(q+1)(q+2)}{6}\,\nabla^3 y_4\\
&= 64 + (-0.8)(37) + \frac{(-0.8)(0.2)}{2}(18)
 + \frac{(-0.8)(0.2)(1.2)}{6}(6).
\end{aligned}
\end{eqnarray*}
C\'alculo:
\begin{eqnarray*}
64 - 29.6 + (-0.08)(18) + \left(\frac{-0.192}{6}\right)6
= 64 - 29.6 - 1.44 - 0.192 = 32.768.
\end{eqnarray*}
Exacto: $f(3.2)=(3.2)^3=32.768$.
\end{Ejem}


\begin{Note}
\begin{itemize}
    \item Use Newton hacia adelante si $x$ est\'a pr\'oximo a $x_0$; use Newton hacia atr\'as si $x$ est\'a pr\'oximo a $x_n$.
    \item Para funciones polin\'omicas de grado $m$, las diferencias de orden $>m$ son cero y la f\'ormula se vuelve exacta con $m+1$ nodos.
    \item En datos reales con ruido, truncar la serie a pocas diferencias suele mejorar estabilidad.
\end{itemize}
\end{Note}

\begin{Ejem}

\begin{eqnarray*}
\begin{array}{c|cccc}
x & 0 & 1 & 2 & 3\\ \hline
f(x) & 0 & 1 & 4 & 9
\end{array}
\end{eqnarray*}

\begin{eqnarray*}
\begin{array}{c|cccc}
x & y & \Delta y & \Delta^2 y & \Delta^3 y\\ \hline
0 & 0 & 1 & 2 & 0\\
1 & 1 & 3 & 2 &  \\
2 & 4 & 5 &   &  \\
3 & 9 &   &   &  
\end{array}
\end{eqnarray*}

Se quiere estimar:$f(0.5)$, con $x_0=0$, $h=1$ y $p=0.5$.

\textbf{Sustituci\'on:}
\begin{eqnarray*}
\begin{aligned}
P(0.5) &= y_0 + p\,\Delta y_0 + \frac{p(p-1)}{2!}\Delta^2 y_0 \\
&= 0 + (0.5)(1) + \frac{(0.5)(-0.5)}{2}(2) \\
&= 0.5 - 0.25 = 0.25.
\end{aligned}
\end{eqnarray*}

Verificando: $f(0.5) = (0.5)^2 = 0.25.$ $\Rightarrow$ Coincide exactamente.
\end{Ejem}

\begin{Ejem}

\begin{eqnarray*}
\begin{array}{c|cccc}
x & 1 & 2 & 3 & 4\\ \hline
f(x) & 1 & 8 & 27 & 64
\end{array}
\end{eqnarray*}

La tabla de diferencias queda:
\begin{eqnarray*}
\begin{array}{c|cccc}
x & y & \Delta y & \Delta^2 y & \Delta^3 y\\ \hline
1 & 1 & 7 & 12 & 6\\
2 & 8 & 19 & 18 &  \\
3 & 27 & 37 &   &  \\
4 & 64 &   &   &  
\end{array}
\end{eqnarray*}

Estimemos $f(1.5)$.  
$x_0=1,\; h=1,\; p=0.5.$

\begin{eqnarray*}
\begin{aligned}
P(1.5)
&= y_0 + p\,\Delta y_0 + \frac{p(p-1)}{2!}\Delta^2 y_0
+ \frac{p(p-1)(p-2)}{3!}\Delta^3 y_0\\
&= 1 + (0.5)(7) + \frac{(0.5)(-0.5)}{2}(12)
+ \frac{(0.5)(-0.5)(-1.5)}{6}(6)\\
&= 1 + 3.5 - 1.5 + 0.375 = 3.375
\end{aligned}
\end{eqnarray*}

verificando $f(1.5) = (1.5)^3 = 3.375$, es decir el m\'etodo reproduce perfectamente el valor porque $f$ es un polinomio de grado 3 y se usaron 4 nodos.
\end{Ejem}


\begin{Ejem}

\begin{eqnarray*}
\begin{array}{c|cccc}
x & 0 & 1 & 2 & 3\\ \hline
f(x) & 0 & 1 & 4 & 9
\end{array}
\end{eqnarray*}

Las Diferencias regresivas (desde el final):
\begin{eqnarray*}
\begin{array}{c|cccc}
x & y & \nabla y & \nabla^2 y & \nabla^3 y\\ \hline
0 & 0 &   &   &  \\
1 & 1 & 1 &   &  \\
2 & 4 & 3 & 2 &  \\
3 & 9 & 5 & 2 & 0
\end{array}
\end{eqnarray*}

Estimando $f(2.6)$: $x_n=3,\; h=1,\; q = \dfrac{2.6 - 3}{1} = -0.4.$

\begin{eqnarray*}
\begin{aligned}
P(2.6)
&= y_3 + q\,\nabla y_3 + \frac{q(q+1)}{2!}\nabla^2 y_3\\
&= 9 + (-0.4)(5) + \frac{(-0.4)(0.6)}{2}(2)\\
&= 9 - 2 - 0.24 = 6.76.
\end{aligned}
\end{eqnarray*}

donde $f(2.6) = (2.6)^2 = 6.76.$

\end{Ejem}

\begin{Ejem}
\begin{eqnarray*}
\begin{array}{c|cccc}
x & 1 & 2 & 3 & 4\\ \hline
f(x) & 1 & 8 & 27 & 64
\end{array}
\end{eqnarray*}

donde las Diferencias regresivas:
\begin{eqnarray*}
\begin{array}{c|cccc}
x & y & \nabla y & \nabla^2 y & \nabla^3 y\\ \hline
1 & 1 &   &   &  \\
2 & 8 & 7 &   &  \\
3 & 27 & 19 & 12 &  \\
4 & 64 & 37 & 18 & 6
\end{array}
\end{eqnarray*}

Estimando a $f(3.2)$:   $x_n=4,\; h=1,\; q = \dfrac{3.2 - 4}{1} = -0.8.$

\begin{eqnarray*}
\begin{aligned}
P(3.2)
&= y_4 + q\,\nabla y_4 + \frac{q(q+1)}{2!}\nabla^2 y_4
+ \frac{q(q+1)(q+2)}{3!}\nabla^3 y_4\\
&= 64 + (-0.8)(37) + \frac{(-0.8)(0.2)}{2}(18)
+ \frac{(-0.8)(0.2)(1.2)}{6}(6).
\end{aligned}
\end{eqnarray*}

donde $64 - 29.6 - 1.44 - 0.192 = 32.768$, vefrificando $f(3.2) = (3.2)^3 = 32.768.$
\end{Ejem}


\begin{Ejem}
Implementación numérica:
\begin{verbatim}
is_equispaced <- function(x, tol = 1e-9) {
  if (length(x) < 2) return(TRUE)
  h <- diff(x)
  max(abs(h - h[1])) < tol
}

falling_prod <- function(p, k) {
  if (k == 0) return(1)
  prod(p - 0:(k-1))
}

rising_prod <- function(q, k) {
  if (k == 0) return(1)
  prod(q + 0:(k-1))
}

# =========================================================
# Tablas de diferencias
# =========================================================
forward_diff_table <- function(x, y, check_equispaced = TRUE) {
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  if (check_equispaced && !is_equispaced(x)) stop("Los nodos no son equiespaciados.")
  T <- matrix(0, nrow = n, ncol = n)
  T[,1] <- y
  if (n >= 2) {
    for (j in 2:n) {
      for (i in 1:(n - j + 1)) {
        T[i,j] <- T[i+1,j-1] - T[i,j-1]   # Delta^j y_i
      }
    }
  }
  T
}

backward_diff_table <- function(x, y, check_equispaced = TRUE) {
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  if (check_equispaced && !is_equispaced(x)) stop("Los nodos no son equiespaciados.")
  T <- matrix(0, nrow = n, ncol = n)
  T[,1] <- y
  if (n >= 2) {
    for (j in 2:n) {
      for (i in n:j) {
        T[i,j] <- T[i,j-1] - T[i-1,j-1]   # Nabla^j y_i
      }
    }
  }
  T
}

# =========================================================
# Evaluadores Newton
# - max_order: orden maximo a usar (por defecto, n-1). Truncar ayuda con ruido.
# =========================================================
newton_forward_eval <- function(x, T, xq, max_order = NULL) {
  n <- length(x)
  h <- x[2] - x[1]
  p <- (xq - x[1]) / h
  if (is.null(max_order)) max_order <- n - 1
  max_order <- max(0, min(max_order, n - 1))
  P <- T[1,1]
  for (k in 1:max_order) {
    coef <- falling_prod(p, k) / factorial(k)
    P <- P + coef * T[1, k + 1]
  }
  # Estimacion de error de truncamiento (termino siguiente) si existe:
  err <- NA_real_
  next_k <- max_order + 1
  if (next_k <= n - 1) {
    coef_next <- falling_prod(p, next_k) / factorial(next_k)
    err <- abs(coef_next * T[1, next_k + 1])
  }
  list(value = P, est_trunc_error = err, p = p)
}

newton_backward_eval <- function(x, T, xq, max_order = NULL) {
  n <- length(x)
  h <- x[2] - x[1]
  q <- (xq - x[n]) / h
  if (is.null(max_order)) max_order <- n - 1
  max_order <- max(0, min(max_order, n - 1))
  P <- T[n,1]
  for (k in 1:max_order) {
    coef <- rising_prod(q, k) / factorial(k)
    P <- P + coef * T[n, k + 1]
  }
  # Estimacion de error (termino siguiente) si existe:
  err <- NA_real_
  next_k <- max_order + 1
  if (next_k <= n - 1) {
    coef_next <- rising_prod(q, next_k) / factorial(next_k)
    err <- abs(coef_next * T[n, next_k + 1])
  }
  list(value = P, est_trunc_error = err, q = q)
}

# =========================================================
# Seleccion automatica de metodo y evaluacion vectorizada
# - method = "auto" | "forward" | "backward"
# - max_order: truncamiento del orden
# Devuelve data.frame con resultados por cada xq
# =========================================================
newton_interp_equispaced <- function(x, y, xq, method = "auto", max_order = NULL) {
  if (!is_equispaced(x)) stop("Los nodos x deben ser equiespaciados.")
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  # Precomputo de tablas
  Tforw <- forward_diff_table(x, y, check_equispaced = FALSE)
  Tback <- backward_diff_table(x, y, check_equispaced = FALSE)
  # Funcion auxiliar para elegir metodo segun cercania
  choose_method <- function(xq_single) {
    if (method == "forward") return("forward")
    if (method == "backward") return("backward")
    # auto: decide por cercania
    d0 <- abs(xq_single - x[1])
    dn <- abs(xq_single - x[n])
    if (d0 <= dn) "forward" else "backward"
  }
  # Evaluacion (vectorizada)
  vals <- numeric(length(xq))
  errs <- rep(NA_real_, length(xq))
  meth <- character(length(xq))
  par1 <- numeric(length(xq))  # p o q usado
  for (i in seq_along(xq)) {
    m <- choose_method(xq[i])
    meth[i] <- m
    if (m == "forward") {
      res <- newton_forward_eval(x, Tforw, xq[i], max_order = max_order)
      vals[i] <- res$value
      errs[i] <- res$est_trunc_error
      par1[i] <- res$p
    } else {
      res <- newton_backward_eval(x, Tback, xq[i], max_order = max_order)
      vals[i] <- res$value
      errs[i] <- res$est_trunc_error
      par1[i] <- res$q
    }
  }\newtheorem{Ses}{Sesi\'on}[section]
  data.frame(
    xq = xq,
    estimate = vals,
    est_trunc_error = errs,
    method_used = meth,
    p_or_q = par1
  )
}

# =========================================================
# Ejemplos rapidos
# =========================================================
# Ejemplo 1 (adelante, auto): f(x)=x^2 en x=0:3, evaluar en xq=seq(0,2,by=0.5)
# x <- 0:3; y <- x^2
# newton_interp_equispaced(x, y, seq(0, 2, by = 0.5))

# Ejemplo 2 (atras, auto): f(x)=x^3 en x=1:4, evaluar en xq=c(3.2, 3.6)
# x <- 1:4; y <- x^3
# newton_interp_equispaced(x, y, c(3.2, 3.6))
\end{verbatim}
\end{Ejem}


%<<==>><<==>><<==>><<==>><<==>><<==>>
\subsection{Splines}
%<<==>><<==>><<==>><<==>><<==>><<==>>

Los \textbf{splines} son funciones polinómicas por tramos utilizadas para interpolar un conjunto de puntos de forma suave y estable.  
A diferencia de un único polinomio de grado alto (que puede oscilar violentamente entre nodos, efecto de Runge), los splines utilizan polinomios de bajo grado en cada intervalo, garantizando continuidad en los nodos y sus derivadas. Le idea es que en cada subintervalo $[x_i, x_{i+1}]$ se construye un polinomio cúbico $S_i(x)$, y se imponen condiciones para que toda la función $S(x)$ sea continua, con derivadas suaves.


\begin{Def}

Sea un conjunto de nodos: $x_0 < x_1 < \cdots < x_n$, y valores conocidos: $y_i = f(x_i), \quad i = 0, 1, \ldots, n.$

El \textbf{spline cúbico} $S(x)$ se define por tramos:
\begin{eqnarray}
S(x) =
\begin{cases}
S_0(x), & x_0 \le x < x_1,\\
S_1(x), & x_1 \le x < x_2,\\
\vdots & \\
S_{n-1}(x), & x_{n-1} \le x \le x_n,
\end{cases}
\end{eqnarray}
donde cada tramo es un polinomio cúbico:
\begin{eqnarray}
S_i(x) = a_i + b_i(x - x_i) + c_i(x - x_i)^2 + d_i(x - x_i)^3.
\end{eqnarray}
\end{Def}

El spline cúbico debe cumplir:
\begin{enumerate}
    \item Interpolación: $S_i(x_i) = y_i$, \quad $S_i(x_{i+1}) = y_{i+1}$.
    \item Continuidad de la primera derivada: $S_i'(x_{i+1}) = S_{i+1}'(x_{i+1})$.
    \item Continuidad de la segunda derivada: $S_i''(x_{i+1}) = S_{i+1}''(x_{i+1})$.
\end{enumerate}

Estas condiciones producen $4(n-1)$ ecuaciones, que junto con dos condiciones adicionales en los extremos definen completamente los $4n$ coeficientes.


\begin{itemize}
    \item \textbf{Spline natural:}  
    Se impone $S''(x_0)=S''(x_n)=0$.  
    Esto genera una forma “suave” en los extremos.
    
    \item \textbf{Spline completo (o clamped):}  
    Se imponen las derivadas primeras conocidas:  
    $S'(x_0)=f'(x_0)$, $S'(x_n)=f'(x_n)$.
    
    \item \textbf{Spline “not-a-knot”:}  
    Obliga continuidad de la tercera derivada en los nodos interiores $x_1$ y $x_{n-1}$.
\end{itemize}

\subsubsection*{Sistema tridiagonal de las segundas derivadas}

Definimos:
\begin{eqnarray}
h_i = x_{i+1}-x_i, \qquad m_i = \frac{y_{i+1}-y_i}{h_i}, \quad i=0,\ldots,n-1.
\end{eqnarray}

El sistema para las segundas derivadas $M_i=S''(x_i)$ es:
\begin{eqnarray}
h_{i-1}M_{i-1}+2(h_{i-1}+h_i)M_i+h_iM_{i+1}=6(m_i-m_{i-1}),\quad i=1,\ldots,n-1,
\end{eqnarray}
con las condiciones de frontera:
\begin{eqnarray}
\text{Spline natural: } M_0=M_n=0.
\end{eqnarray}

\textbf{Una vez calculadas las $M_i$,} cada tramo $S_i(x)$ se obtiene como:
\begin{eqnarray}
S_i(x)=
\frac{M_{i+1}(x-x_i)^3}{6h_i}
-\frac{M_i(x_{i+1}-x)^3}{6h_i}
+\left( \frac{y_{i+1}}{h_i}-\frac{M_{i+1}h_i}{6} \right)(x-x_i)
+\left( \frac{y_i}{h_i}-\frac{M_i h_i}{6} \right)(x_{i+1}-x).
\end{eqnarray}


\begin{Algthm}
El sistema tridiagonal se resuelve eficientemente con el \textbf{método de Thomas} (algoritmo especializado para matrices tridiagonales).  
El proceso computacional sigue estos pasos:
\begin{enumerate}
    \item Calcular los $h_i$ y $m_i$.
    \item Formar el sistema tridiagonal en términos de $M_i$.
    \item Resolver para obtener $M_1,\ldots,M_{n-1}$.
    \item Construir los polinomios $S_i(x)$ en cada subintervalo.
\end{enumerate}
\end{Algthm}

\begin{Ejem}
Interpolar los puntos:
\begin{eqnarray*}
(0,0), \quad (1,1), \quad (2,0).
\end{eqnarray*}

Se tiene  $n=2$, $h_0=h_1=1$, 
$m_0=1$, $m_1=-1$.

Ecuación central:
\begin{eqnarray*}
h_0 M_0 + 2(h_0 + h_1) M_1 + h_1 M_2 = 6(m_1 - m_0),
\end{eqnarray*}

\begin{eqnarray*}
1(0) + 4M_1 + 1(0) = 6(-1 - 1) = -12 \quad \Rightarrow \quad M_1 = -3.
\end{eqnarray*}
Y $M_0=M_2=0$.

Sustituyendo en la fórmula del tramo $[0,1]$:
\begin{eqnarray*}
S_0(x) = -\frac{M_1}{6}(x-1)^3 + x.
\end{eqnarray*}
En $[1,2]$:
\begin{eqnarray*}
S_1(x) = -\frac{M_1}{6}(2-x)^3 + (2-x).
\end{eqnarray*}

Graficar ambos tramos muestra una curva suave en forma de “loma”.
\end{Ejem}


\begin{Ejem}
Puntos:
\begin{eqnarray*}
(0,0), \quad (1,1), \quad (2,0), \quad (3,1).
\end{eqnarray*}
Datos: $h_i=1$, 
$m_0=1$, $m_1=-1$, $m_2=1$.

Sistema:
\begin{eqnarray*}
\begin{cases}
4M_1 + M_2 = 6(-1 - 1) = -12,\\
M_1 + 4M_2 = 6(1 - (-1)) = 12.
\end{cases}
\end{eqnarray*}
Resolviendo:
\begin{eqnarray*}
M_1 = -4, \quad M_2 = 4, \quad M_0 = M_3 = 0.
\end{eqnarray*}

Cada tramo se obtiene sustituyendo los valores de $M_i$ en la fórmula general.
\end{Ejem}


\begin{Ejem}
Puntos:
\begin{eqnarray*}
(0,1), \quad (1,2), \quad (2,0),
\end{eqnarray*}
con derivadas conocidas $S'(0)=0$ y $S'(2)=-1$.

Se aplican las condiciones en los extremos:
\begin{eqnarray*}
2h_0 M_0 + h_0 M_1 = 6\left(\frac{y_1 - y_0}{h_0} - S'(0)\right),
\end{eqnarray*}

\begin{eqnarray*}
h_0 M_0 + 2h_0 M_1 = 6\left(S'(2) - \frac{y_2 - y_1}{h_0}\right).
\end{eqnarray*}
Resolviendo se obtienen $M_0$ y $M_1$, y luego los coeficientes $a_i,b_i,c_i,d_i$ de cada tramo.
\end{Ejem}


\begin{Ejem}
Mediciones de temperatura ($^\circ$C) a distintas horas:
\begin{eqnarray*}
\begin{array}{c|cccc}
\text{Hora (h)} & 0 & 3 & 6 & 9\\ \hline
\text{Temperatura} & 15 & 18 & 14 & 10
\end{array}
\end{eqnarray*}
Interpolar con un spline cúbico natural y estimar $T(4.5)$.

\begin{Algthm}
\begin{enumerate}
    \item Calcular $h_i=3$, $m_i=(y_{i+1}-y_i)/h_i$.
    \item Formar el sistema para $M_1, M_2$.
    \item Resolverlo con el método de Thomas.
    \item Evaluar $S_1(4.5)$ usando la fórmula de $S_i(x)$ en el intervalo $[3,6]$.
\end{enumerate}
\end{Algthm}

\textbf{Resultado esperado:} $T(4.5)\approx 15.5^\circ$C.  
La curva obtenida describe de forma suave la variación de temperatura entre mediciones.
\end{Ejem}


\begin{Note}
\begin{itemize}
    \item Los splines cúbicos son continuos en $S$, $S'$, y $S''$, lo que garantiza suavidad.
    \item Reducen el error sin introducir oscilaciones (efecto Runge).
    \item El error máximo en un spline cúbico natural es proporcional a $O(h^4)$.
    \item Son ampliamente usados en gráficos por computadora, CAD, animación y análisis de datos experimentales.
\end{itemize}
\end{Note}


%<<==>><<==>><<==>><<==>><<==>><<==>>
\subsection{Error de Interpolaci\'on}
%<<==>><<==>><<==>><<==>><<==>><<==>>

La interpolación polinómica busca aproximar una función $f(x)$ mediante un polinomio $P_n(x)$ que coincide con $f(x)$ en $n{+}1$ puntos conocidos.  
Sin embargo, entre los nodos de interpolación puede existir una diferencia o \textbf{error de interpolación}, denotado por $R_n(x)$.

\subsubsection*{Fórmula general del error}

Sea $f\in C^{(n+1)}[a,b]$ y $P_n(x)$ el polinomio interpolante de grado $n$ para los puntos $(x_0,y_0),\ldots,(x_n,y_n)$ con $y_i=f(x_i)$.  
Entonces, para todo $x\in[a,b]$, existe un número $\xi\in(a,b)$ tal que:

\begin{eqnarray*}
R_n(x) = f(x) - P_n(x)
= \frac{f^{(n+1)}(\xi)}{(n+1)!}\prod_{i=0}^{n}(x - x_i).
\end{eqnarray*}

\textbf{Interpretación:}  
El error depende de dos factores:
\begin{itemize}
    \item El término $\dfrac{f^{(n+1)}(\xi)}{(n+1)!}$, relacionado con la suavidad de $f(x)$.
    \item El producto $\displaystyle\prod_{i=0}^{n}(x - x_i)$, que refleja la posición de los nodos de interpolación.
\end{itemize}

\subsubsection*{Cota del error}

Si se conoce una cota $|f^{(n+1)}(x)| \le M$ para $x \in [a,b]$, entonces:

\begin{eqnarray*}
|R_n(x)| \le \frac{M}{(n+1)!}\,\max_{x\in[a,b]}\left|\prod_{i=0}^{n}(x - x_i)\right|.
\end{eqnarray*}

Esta expresión indica que el error aumenta:
\begin{itemize}
    \item Si $f^{(n+1)}(x)$ es grande (función muy curvada o con derivadas altas).
    \item Si los nodos $x_i$ están mal distribuidos (especialmente equiespaciados para $n$ grande).
\end{itemize}

\subsubsection*{Comportamiento del error y efecto de Runge}

Cuando los nodos son equiespaciados, el término
\begin{eqnarray*}
\prod_{i=0}^{n}(x - x_i)
\end{eqnarray*}
crece rápidamente en los extremos del intervalo, provocando oscilaciones en el polinomio interpolante — fenómeno conocido como \textbf{efecto de Runge}.  
Este efecto se agrava con $n$ grande y funciones con alta curvatura, como $f(x) = \dfrac{1}{1+x^2}$.

\textit{Conclusión: aumentar el grado $n$ no garantiza una mejor aproximación.}


\subsubsection*{Nodos de Chebyshev (minimizan el error máximo)}

Para reducir las oscilaciones, se eligen nodos no equiespaciados, conocidos como \textbf{nodos de Chebyshev}.  
En el intervalo $[-1,1]$ se definen como:

\begin{eqnarray*}
x_i = \cos\left(\frac{2i+1}{2(n+1)}\pi\right), \quad i=0,1,\ldots,n.
\end{eqnarray*}

Estos nodos distribuyen más puntos cerca de los extremos del intervalo, donde el error tiende a crecer más.

\textbf{Propiedad fundamental:}
Los nodos de Chebyshev minimizan la cantidad
\begin{eqnarray*}
\max_{x\in[-1,1]}\left|\prod_{i=0}^{n}(x - x_i)\right|,
\end{eqnarray*}
y, por tanto, minimizan el error máximo de interpolación en el sentido de la norma infinita.

\begin{Ejem}: comparación cualitativa

Sea $f(x)=\dfrac{1}{1+x^2}$ en $[-5,5]$.

\begin{itemize}
    \item Con $n=10$ y nodos \textbf{equiespaciados}: el polinomio presenta grandes oscilaciones en los extremos (efecto de Runge).
    \item Con $n=10$ y nodos \textbf{de Chebyshev}: la interpolación es estable y el error máximo es mucho menor.
\end{itemize}
\end{Ejem}

\begin{Ejem} Estimación del error
Sea $f(x)=e^x$ en $[0,1]$, con nodos equiespaciados $x_0=0$, $x_1=0.5$, $x_2=1$.
Entonces $f^{(3)}(x)=e^x \le e$.

Usando la fórmula del error para $x=0.25$:
\begin{eqnarray*}
|R_2(0.25)| \le \frac{e}{3!}|(0.25-0)(0.25-0.5)(0.25-1)| = \frac{e}{6}(0.25\cdot 0.25\cdot 0.75) \approx 0.0085.
\end{eqnarray*}
La cota muestra que el error es pequeño, y decrece proporcionalmente a $h^{n+1}$.
\end{Ejem}

\begin{Note}
\begin{itemize}
    \item La magnitud de $R_n(x)$ depende tanto de la función como de la elección de nodos.
    \item Nodos de Chebyshev distribuyen mejor el error, especialmente para grados altos.
    \item En la práctica, cuando los datos son experimentales o $f(x)$ es desconocida, se prefiere reducir $n$ y usar \textbf{splines}, que mantienen continuidad y baja oscilación.
\end{itemize}
\end{Note}


%<<==>><<==>><<==>><<==>><<==>><<==>>
\subsection{Ejercicios }
%<<==>><<==>><<==>><<==>><<==>><<==>>
Esta secci\'on presenta ejercicios orientados a consolidar la comprensi\'on de los m\'etodos 
de interpolaci\'on estudiados: Lagrange, Newton (diferencias divididas) 
y Newton hacia adelante/atr\'as (diferencias finitas).  

Se recomienda resolver los primeros ejercicios de forma manual 
y posteriormente verificar los resultados con el c\'odigo en R proporcionado.

\begin{Ejer}
\begin{enumerate}
    \item Dado el conjunto de puntos
    \begin{eqnarray*}
    (0,1),\ (1,3),\ (2,11),
    \end{eqnarray*}
    \begin{enumerate}[a)]
        \item Determine el polinomio interpolante mediante el \textbf{m\'etodo de Lagrange}.  
        \item Verifique el resultado construyendo la \textbf{tabla de diferencias divididas} 
              y aplicando el \textbf{m\'etodo de Newton}.  
        \item Eval\'ue el polinomio en $x=1.5$ y compare con la funci\'on cuadr\'atica
              ajustada visualmente a los puntos.
    \end{enumerate}
    \vspace{1em}

    \item Con los puntos
    \begin{eqnarray*}
    (1,2),\ (2,5),\ (3,10),\ (4,17),
    \end{eqnarray*}
    \begin{enumerate}[a)]
        \item Obtenga el polinomio interpolante de grado 3 usando el \textbf{m\'etodo de Newton}.  
        \item Muestre la tabla completa de diferencias divididas.  
        \item Eval\'ue el polinomio en $x=2.5$.  
        \item Verifique el resultado calculando directamente $f(x)=x^2+1$ y compare.
    \end{enumerate}
    \vspace{1em}

    \item Dados los puntos
    \begin{eqnarray*}
    (-1,4),\ (0,1),\ (2,3),
    \end{eqnarray*}
    \begin{enumerate}[a)]
        \item Construya el polinomio de Lagrange.  
        \item Expanda y simplifique el resultado hasta la forma general $P_2(x)=a_0+a_1x+a_2x^2.$
        \item Verifique los valores de $P_2(-1)$, $P_2(0)$ y $P_2(2)$.
    \end{enumerate}
\end{enumerate}
\end{Ejer}


\begin{Ejer}

\begin{enumerate}
    \item Para la funci\'on tabulada
    \begin{eqnarray*}
    \begin{array}{c|ccccc}
    x & 0 & 1 & 2 & 3 & 4\\ \hline
    f(x) & 1 & 2 & 4 & 8 & 16
    \end{array}
    \end{eqnarray*}
    \begin{enumerate}[a)]
        \item Construya la \textbf{tabla de diferencias progresivas} $\Delta y_i$.  
        \item Determine el polinomio interpolante con Newton hacia adelante.  
        \item Calcule $f(2.5)$ y comp\'arelo con $2^{2.5}$.
    \end{enumerate}
    \vspace{1em}

    \item Para los valores:
    \begin{eqnarray*}
    \begin{array}{c|cccc}
    x & 1 & 2 & 3 & 4\\ \hline
    f(x) & 1 & 8 & 27 & 64
    \end{array}
    \end{eqnarray*}
    \begin{enumerate}[a)]
        \item Construya la tabla de \textbf{diferencias regresivas} $\nabla y_i$.  
        \item Aplique la f\'ormula de Newton hacia atr\'as para estimar $f(3.4)$.  
        \item Compare con el valor exacto de $f(x)=x^3$.
    \end{enumerate}
    \vspace{1em}

    \item Para los datos experimentales:
    \begin{eqnarray*}
    \begin{array}{c|ccccc}
    x & 10 & 20 & 30 & 40 & 50\\ \hline
    y & 20.5 & 24.0 & 32.0 & 42.5 & 51.0
    \end{array}
    \end{eqnarray*}
    \begin{enumerate}[a)]
        \item Verifique que los nodos sean equiespaciados.  
        \item Calcule las diferencias progresivas hasta el tercer orden.  
        \item Estime $y(25)$ usando Newton hacia adelante.  
        \item Comente si el uso de un polinomio de orden m\'as alto podr\'ia 
              mejorar o empeorar la estabilidad num\'erica.
    \end{enumerate}
\end{enumerate}

\end{Ejer}



\begin{Ejer}
Implemente los siguientes ejercicios utilizando las funciones 
\texttt{newton\_interp\_equispaced()}, \texttt{forward\_diff\_table()} 
y \texttt{backward\_diff\_table()}.

\begin{enumerate}
    \item Replique los ejemplos manuales anteriores en R y 
          compare los resultados obtenidos manualmente con los valores calculados por el script.
          Incluya en su reporte:
          \begin{itemize}
              \item La tabla de diferencias generada.  
              \item El valor estimado y el error de truncamiento.  
              \item Una gr\'afica de los puntos originales y el polinomio interpolante.
          \end{itemize}
    \vspace{1em}

    \item Genere aleatoriamente 5 puntos de una funci\'on cuadr\'atica y aplique:
          \begin{itemize}
              \item Interpolaci\'on de Lagrange (con la f\'ormula cl\'asica).  
              \item Interpolaci\'on de Newton (diferencias divididas).  
              \item Interpolaci\'on hacia adelante (diferencias finitas).  
          \end{itemize}
          Compare los tres resultados gr\'aficamente y analice diferencias num\'ericas.

    \vspace{1em}

    \item Usando los datos:
    \begin{eqnarray*}
    x = [0,1,2,3,4], \quad
    y = [2.0, 2.7, 4.8, 8.9, 16.0],
    \end{eqnarray*}
    \begin{enumerate}[a)]
        \item Aplique el m\'etodo autom\'atico \texttt{newton\_interp\_equispaced()} 
              para evaluar la funci\'on en $x = 1.5, 2.5, 3.5$.  
        \item Reporte el m\'etodo seleccionado (adelante o atr\'as) y el error estimado.  
        \item Compare con una funci\'on de referencia $f(x) = 2^{x}$.
    \end{enumerate}
\end{enumerate}
\end{Ejer}


\begin{Ejer}
Puntos: $(0,1),(1,3),(2,11)$.
\begin{itemize}
\item Lagrange / Forma general: Sea $P_2(x)=a_0+a_1x+a_2x^2$. Con $x=0\Rightarrow a_0=1$; $x=1\Rightarrow 1+a_1+a_2=3$; $x=2\Rightarrow 1+2a_1+4a_2=11$. De $a_1+a_2=2$ y $a_1+2a_2=5\Rightarrow a_2=3,\ a_1=-1$.\begin{eqnarray*}
P_2(x)=1 - x + 3x^2.
\end{eqnarray*}
\item Evaluación: $P_2(1.5)=1-1.5+3(2.25)=6.25$.
\item Newton: tabla de diferencias divididas produce los mismos coeficientes.
\end{itemize}
\end{Ejer}

\begin{Ejer}
Puntos: $(1,2),(2,5),(3,10),(4,17)$.
\begin{itemize}
\item Observa que $y=x^2+1$ calza exactamente con los datos.
\item \textbf{Newton}: diferencias
\begin{eqnarray*}
\Delta y=\{3,5,7\},\quad \Delta^2 y=\{2,2\},\quad \Delta^3 y=\{0\}.
\end{eqnarray*}
\item \textbf{Polinomio}: de grado $2$ (término cúbico nulo), equivale a $P(x)=x^2+1$.

\item \textbf{Evaluación}: $P(2.5)=2.5^2+1=7.25$.
\end{itemize}
\end{Ejer}

\begin{Ejer}
Puntos: $(-1,4),(0,1),(2,3)$.
\begin{itemize}
\item Forma general $P_2(x)=a_0+a_1x+a_2x^2$. De $x=0\Rightarrow a_0=1$.

\item Sistema: $-a_1+a_2=3$ y $a_1+2a_2=1$. Solución: $a_2=\tfrac{4}{3}$, $a_1=-\tfrac{5}{3}$.

\begin{eqnarray*}
P_2(x)=1 - \frac{5}{3}x + \frac{4}{3}x^2.
\end{eqnarray*}

\item Verifica: $P_2(-1)=4,\ P_2(0)=1,\ P_2(2)=3$.
\end{itemize}
\end{Ejer}

\begin{Ejer}
Tabla $2^x$ en $x=0,1,2,3,4$: $y=\{1,2,4,8,16\}$.
\begin{itemize}
\item Diferencias progresivas:$\Delta y_0=1,\ \Delta^2 y_0=1,\ \Delta^3 y_0=1,\ \Delta^4 y_0=1$.
\item Newton adelante (grado $4$), $x_0=0,h=1$, $p=2.5$:
\begin{eqnarray*}
P(2.5) &=& y_0 + p\Delta y_0 + \frac{p(p-1)}{2}\Delta^2 y_0+ \frac{p(p-1)(p-2)}{6}\Delta^3 y_0+ \frac{p(p-1)(p-2)(p-3)}{24}\Delta^4 y_0\\
&=& 1 + 2.5 + 1.875 + 0.3125 - 0.0390625\\
&= &5.6484375.
\end{eqnarray*}

\item Comparación: $2^{2.5}=\sqrt{32}\approx 5.65685425$. Error $\approx -8.4168\times 10^{-3}$ (el grado $4$ no puede reproducir una función exponencial).
\end{itemize}
\end{Ejer}

\begin{Ejer}
$x=1,2,3,4$ con $f(x)=x^3=\{1,8,27,64\}$; estimar $f(3.4)$ por \textbf{atrás}.
\begin{itemize}
\item \textbf{Regresivas en $x_4=4$:} $\nabla y_4=37,\ \nabla^2 y_4=18,\ \nabla^3 y_4=6$.
\item $h=1,\ q=(3.4-4)=-0.6$.
\begin{eqnarray*}
P(3.4) &=& y_4 + q\nabla y_4 + \frac{q(q+1)}{2}\nabla^2 y_4+ \frac{q(q+1)(q+2)}{6}\nabla^3 y_4\\
&=& 64 - 22.2 - 2.16 - 0.336\\
&= &39.304. 
\end{eqnarray*}

\item \textbf{Exacto}: $3.4^3=39.304$. (Para polinomio cúbico con $4$ nodos, el método reproduce exactamente.)
\end{itemize}
\end{Ejer}

\begin{Ejer}
Datos experimentales equiespaciados ($h=10$):
\begin{eqnarray*}
\begin{array}{c|ccccc}
x & 10 & 20 & 30 & 40 & 50\\ \hline
y & 20.5 & 24.0 & 32.0 & 42.5 & 51.0
\end{array}
\end{eqnarray*}
\begin{itemize}
\item \textbf{Progresivas en $x_0=10$:} 
\begin{eqnarray*}
    \Delta y_0=3.5,\quad \Delta^2 y_0=4.5,\quad \Delta^3 y_0=-2.0,\quad \Delta^4 y_0=-2.5.
\end{eqnarray*}

\item \textbf{Estimar $y(25)$} (adelante, hasta orden $3$): $p=\tfrac{25-10}{10}=1.5$.

\begin{eqnarray*}
\begin{aligned}
P(25) &= y_0 + p\Delta y_0 + \frac{p(p-1)}{2}\Delta^2 y_0+ \frac{p(p-1)(p-2)}{6}\Delta^3 y_0\\
&= 20.5 + (1.5)(3.5) + \frac{1.5\cdot 0.5}{2}(4.5)+ \frac{1.5\cdot 0.5\cdot (-0.5)}{6}(-2.0)\\
&= 27.5625.
\end{aligned}
\end{eqnarray*}

\item \textbf{Término siguiente (cota pragmática)}:

\begin{eqnarray*}
\frac{p(p-1)(p-2)(p-3)}{24}\,\Delta^4 y_0
= \frac{1.5\cdot 0.5\cdot (-0.5)\cdot (-1.5)}{24}(-2.5)
\approx \boxed{-0.0586}.
\end{eqnarray*}
(Magnitud $\approx 5.86\times 10^{-2}$: sugiere el orden del error de truncamiento.)
\end{itemize}
\end{Ejer}

\begin{Ejer}
Para $5$ puntos de una cuadrática simulada $y=ax^2+bx+c$ con ruido $N(0,\sigma)$:
\begin{itemize}
\item Lagrange (clásico) y Newton (divididas) deben coincidir en precisión con datos sin ruido.
\item Con ruido, truncar el orden (p.ej. usar $3$--$4$ nodos o limitar \texttt{max\_order})  reduce oscilaciones y mejora estabilidad.
\end{itemize}
\end{Ejer}

\begin{Ejer}
Con $x=\{0,1,2,3,4\}$, $y=\{2.0,2.7,4.8,8.9,16.0\}$:
\begin{itemize}
\item \texttt{newton\_interp\_equispaced} seleccionará \emph{adelante} cerca de $x_0$  y \emph{atrás} cerca de $x_4$ (o el que quede más cercano).

\item La columna \texttt{est\_trunc\_error} reportará el tamaño del término siguiente (guía de error).
\item Al comparar con $2^x$, se observarán discrepancias crecientes fuera del rango central (interpolación no reproduce exponenciales con grado bajo de forma exacta).
\end{itemize}
\end{Ejer}

\begin{Ejem}
Datos:
\begin{eqnarray*}
(x_i,y_i)=(0,1),\ (1,2.2),\ (2,2.8),\ (3,3.6).
\end{eqnarray*}
Pendientes por tramo
\begin{eqnarray*}
b_0=\frac{2.2-1}{1-0}=1.2,\quad
b_1=\frac{2.8-2.2}{2-1}=0.6,\quad
b_2=\frac{3.6-2.8}{3-2}=0.8.
\end{eqnarray*}
Por tramos:
\begin{eqnarray*}
S(x)=
\begin{cases}
1+1.2(x-0), & 0\le x\le 1,\\
2.2+0.6(x-1), & 1\le x\le 2,\\
2.8+0.8(x-2), & 2\le x\le 3.
\end{cases}
\end{eqnarray*}
Evaluaciones:
\begin{eqnarray*}
S(1.7)=2.2+0.6(0.7)=2.62,\qquad
S(2.4)=2.8+0.8(0.4)=3.12.
\end{eqnarray*}
Verificaciones:
\begin{eqnarray*}
S(1^-)=2.2,\ S(1^+)=2.2;\quad
S(2^-)=2.8,\ S(2^+)=2.8.
\end{eqnarray*}
(S\'olo continuidad de la funci\'on; derivadas cambian en los nudos.)
\end{Ejem}


\begin{Ejem}
Datos:
\begin{eqnarray*}
(x_0,y_0)=(0,0),\ (x_1,y_1)=(1,1),\ (x_2,y_2)=(2,0),\ (x_3,y_3)=(3,1).
\end{eqnarray*}
Paso $h_i=1$. Pendientes locales:
\begin{eqnarray*}
m_0=\frac{1-0}{1}=1,\quad m_1=\frac{0-1}{1}=-1,\quad m_2=\frac{1-0}{1}=1.
\end{eqnarray*}
Spline \emph{natural}: $M_0=M_3=0$. Sistema para $M_1,M_2$ (tridiagonal cl\'asico):
\begin{eqnarray*}
\begin{cases}
M_0+4M_1+M_2=6(m_1-m_0)=6(-1-1)=-12,\\
M_1+4M_2+M_3=6(m_2-m_1)=6(1-(-1))=12.
\end{cases}
\end{eqnarray*}
Con $M_0=M_3=0$:
\begin{eqnarray*}
4M_1+M_2=-12,\qquad M_1+4M_2=12.
\end{eqnarray*}
Resolviendo:
\begin{eqnarray*}
M_2=4,\quad M_1=-4,\quad (M_0=0,\ M_3=0).
\end{eqnarray*}
Coeficientes por tramo ($h_i=1$):
\begin{eqnarray*}
\begin{aligned}
a_i&=y_i,\\
b_i&=m_i-\frac{(2M_i+M_{i+1})h_i}{6},\\
c_i&=\frac{M_i}{2},\\
d_i&=\frac{M_{i+1}-M_i}{6h_i}.
\end{aligned}
\end{eqnarray*}
Tramo $[0,1]$ ($i=0$): $m_0=1,\ M_0=0,\ M_1=-4$:
\begin{eqnarray*}
a_0=0,\ b_0=1-\tfrac{(0-4)}{6}=\tfrac{5}{3},\ c_0=0,\ d_0=\tfrac{-4-0}{6}=-\tfrac{2}{3}.
\end{eqnarray*}
\begin{eqnarray*}
S_0(x)=\tfrac{5}{3}x-\tfrac{2}{3}x^3.
\end{eqnarray*}
Tramo $[1,2]$ ($i=1$): $m_1=-1,\ M_1=-4,\ M_2=4$:
\begin{eqnarray*}
a_1=1,\ b_1=-1-\tfrac{(2(-4)+4)}{6}=-\tfrac{4}{3},\ c_1=-2,\ d_1=\tfrac{4-(-4)}{6}=\tfrac{4}{3},
\end{eqnarray*}
\begin{eqnarray*}
S_1(x)=1-\tfrac{4}{3}(x-1)-2(x-1)^2+\tfrac{4}{3}(x-1)^3.
\end{eqnarray*}
Tramo $[2,3]$ ($i=2$): $m_2=1,\ M_2=4,\ M_3=0$:
\begin{eqnarray*}
a_2=0,\ b_2=1-\tfrac{(8+0)}{6}=-\tfrac{1}{3},\ c_2=2,\ d_2=\tfrac{0-4}{6}=-\tfrac{2}{3},
\end{eqnarray*}
\begin{eqnarray*}
S_2(x)=-\tfrac{1}{3}(x-2)+2(x-2)^2-\tfrac{2}{3}(x-2)^3+0\ (\text{y desplazar por }y_2=0).
\end{eqnarray*}
Evaluaciones:
\begin{eqnarray*}
S(0)=0,\ S(1)=1,\ S(2)=0,\ S(3)=1,\quad S'(0)=\tfrac{5}{3},\ S''(0)=0,\ S''(3)=0.
\end{eqnarray*}
(Estructura suave: continuidad de $S,S',S''$ en $x=1,2$.)
\end{Ejem}



\begin{Ejem}
Datos:
\begin{eqnarray*}
(0,0),\ (1,2),\ (2,3),\qquad S'(0)=1,\ S'(2)=0.
\end{eqnarray*}
$h_0=h_1=1$, $m_0=2$, $m_1=1$.
Frontera clamped y ecuaciones interiores:
\begin{eqnarray*}
\begin{cases}
2h_0M_0+h_0M_1=6(m_0-S'(0))=6,\\
h_0M_0+2(h_0+h_1)M_1+h_1M_2=6(m_1-m_0)=-6,\\
h_1M_1+2h_1M_2=6(S'(2)-m_1)=-6.
\end{cases}
\end{eqnarray*}
Resoluci\'on:
\begin{eqnarray*}
M_0=4,\quad M_1=-2,\quad M_2=-2.
\end{eqnarray*}
Coeficientes (con $h=1$):
\begin{eqnarray*}
\begin{aligned}
a_0&=0,\quad b_0=m_0-\tfrac{2M_0+M_1}{6}=2-\tfrac{8-2}{6}=1,\\
c_0&=\tfrac{M_0}{2}=2,\quad d_0=\tfrac{M_1-M_0}{6}=-1,\\begin{eqnarray*}4pt]
a_1&=2,\quad b_1=m_1-\tfrac{2M_1+M_2}{6}=1-\tfrac{-4-2}{6}=2,\\
c_1&=\tfrac{M_1}{2}=-1,\quad d_1=\tfrac{M_2-M_1}{6}=0.
\end{aligned}
\end{eqnarray*}
Por tramos:
\begin{eqnarray*}
S(x)=
\begin{cases}
x+2x^2-x^3, & 0\le x\le 1,\\
2+2(x-1)-(x-1)^2, & 1\le x\le 2.
\end{cases}
\end{eqnarray*}
Verif.: $S(0)=0,\ S(1)=2,\ S(2)=3$; $S'(0)=1$ y $S'(2)=0$ (clamped cumplido).
\end{Ejem}

\begin{Ejem}
\begin{enumerate}
\item \textbf{Spline lineal:} Con $(0,1.2),(1,2.0),(3,3.1),(4,4.0)$:

\begin{enumerate}[a)]
\item Escriba $S_i(x)$ por tramos y eval\'ue $S(2.5)$.
\item Grafique a mano los segmentos y comente la falta de suavidad en los nudos.
\end{enumerate}

\item \textbf{Spline c\'ubico natural:} Con $(0,0),(1,1),(2,0),(3,1)$:

\begin{enumerate}[a)]
\item Arme el sistema tridiagonal para $M_1,M_2$ y resu\'elvalo.
\item Obtenga los coeficientes $(a_i,b_i,c_i,d_i)$ y eval\'ue $S(1.5)$ y $S(2.3)$.
\end{enumerate}


\item \textbf{Spline c\'ubico completo:} Con $(0,0),(1,2),(2,3)$ y $S'(0)=1,\ S'(2)=0$:
\begin{enumerate}[a)]
\item Arme el sistema para $M_0,M_1,M_2$. 
\item Calcule $S(0.5)$ y $S(1.8)$.
\end{enumerate}
\end{enumerate}
\end{Ejem}

%<<==>><<==>><<==>><<==>><<==>><<==>>
\subsection{Implementaciones Computacionales}
%<<==>><<==>><<==>><<==>><<==>><<==>>

\subsubsection*{Pseudoc\'odigo (forma cl\'asica)}
Sea \texttt{x[0..n]}, \texttt{y[0..n]} los datos, y \texttt{xq} el punto a evaluar.
\begin{verbatim}
ALGORITMO LagrangeEval(x[0..n], y[0..n], xq):
    p := 0
    para i := 0..n hacer
        Li := 1
        para j := 0..n hacer
            si j != i entonces
                Li := Li * (xq - x[j]) / (x[i] - x[j])
            fin si
        fin para
        p := p + y[i] * Li
    fin para
    retornar p
FIN
\end{verbatim}

\subsubsection*{Pseudoc\'odigo (baric\'entrico, estable)}
Precompute los pesos baric\'entricos $w_i=\displaystyle\frac{1}{\prod_{j\neq i}(x_i-x_j)}$ una sola vez.
\begin{verbatim}
ALGORITMO BarycentricPrecompute(x[0..n]):
    para i := 0..n hacer
        wi := 1
        para j := 0..n hacer
            si j != i entonces
                wi := wi * 1.0 / (x[i] - x[j])
            fin si
        fin para
        w[i] := wi
    fin para
    retornar w
FIN

ALGORITMO BarycentricEval(x[0..n], y[0..n], w[0..n], xq):
    // Si xq coincide con un nodo, devuelve su y exacta
    para i := 0..n hacer
        si xq == x[i] entonces retornar y[i]
    fin para
    num := 0 ; den := 0
    para i := 0..n hacer
        term := w[i] / (xq - x[i])
        num := num + term * y[i]
        den := den + term
    fin para
    retornar num / den
FIN
\end{verbatim}

\subsubsection*{Implementaci\'on en R (forma cl\'asica y baric\'entrica)}
\begin{verbatim}
# --- Forma clasica O(n^2) por evaluacion ---
lagrange_eval <- function(x, y, xq) {
  # x, y: vectores de longitud n+1
  # xq: escalar o vector
  sapply(xq, function(xx) {
    n <- length(x) - 1
    p <- 0
    for (i in 0:n) {
      Li <- 1
      for (j in 0:n) {
        if (j != i) {
          Li <- Li * (xx - x[j+1]) / (x[i+1] - x[j+1])
        }
      }
      p <- p + y[i+1] * Li
    }
    p
  })
}

# --- Pesos baricentricos (precomputo O(n^2)) ---
barycentric_weights <- function(x) {
  n <- length(x) - 1
  w <- rep(1, n+1)
  for (i in 0:n) {
    for (j in 0:n) {
      if (j != i) w[i+1] <- w[i+1] / (x[i+1] - x[j+1])
    }
  }
  w
}

# --- Evaluacion baricentrica O(n) por punto ---
barycentric_eval <- function(x, y, w, xq) {
  sapply(xq, function(xx) {
    # Coincidencia exacta con un nodo
    idx <- which(xx == x)
    if (length(idx) > 0) return(y[idx[1]])
    terms <- w / (xx - x)
    sum(terms * y) / sum(terms)
  })
}

# --- Ejemplo de uso ---
# Datos: f(x)=x^2 en nodos 0,1,2
x <- c(0,1,2); y <- x^2

# Evaluar en una malla
xx <- seq(0,2,length.out=21)
yy_lagr <- lagrange_eval(x,y,xx)

w  <- barycentric_weights(x)
yy_bar <- barycentric_eval(x,y,w,xx)

# yy_lagr y yy_bar deben coincidir (salvo error redondeo) con xx^2
# plot(xx, yy_lagr, type="l"); points(x,y)
\end{verbatim}


\subsubsection*{Pseudoc\'odigo}
\begin{verbatim}
ALGORITMO NewtonDiffDiv(x[0..n], y[0..n]):
    para i := 0..n hacer
        F[i,0] := y[i]
    fin para
    para j := 1..n hacer
        para i := 0..n-j hacer
            F[i,j] := (F[i+1,j-1] - F[i,j-1]) / (x[i+j] - x[i])
        fin para
    fin para
    retornar F[0,0..n] // Coeficientes f[x0], f[x0,x1], ...
FIN

ALGORITMO NewtonEval(x[0..n], F[0,0..n], xq):
    p := F[0,0]
    prod := 1
    para j := 1..n hacer
        prod := prod * (xq - x[j-1])
        p := p + F[0,j]*prod
    fin para
    retornar p
FIN
\end{verbatim}

\subsubsection*{Implementaci\'on en R}
\begin{verbatim}
# --- Tabla de diferencias divididas ---
newton_diffdiv <- function(x, y) {
  n <- length(x)
  F <- matrix(0, n, n)
  F[,1] <- y
  for (j in 2:n) {
    for (i in 1:(n-j+1)) {
      F[i,j] <- (F[i+1,j-1] - F[i,j-1]) / (x[i+j-1] - x[i])
    }
  }
  F
}

# --- Evaluacion del polinomio de Newton ---
newton_eval <- function(x, F, xq) {
  n <- length(x)
  sapply(xq, function(xx) {
    p <- F[1,1]
    prod <- 1
    for (j in 2:n) {
      prod <- prod * (xx - x[j-1])
      p <- p + F[1,j]*prod
    }
    p
  })
}

# --- Ejemplo ---
x <- c(0,1,2,3)
y <- c(1,0,-1,2)
F <- newton_diffdiv(x,y)
xx <- seq(0,3,length.out=41)
yy <- newton_eval(x,F,xx)

# plot(xx,yy,type="l",col="blue"); points(x,y,col="red",pch=19)
\end{verbatim}

\begin{verbatim}

ALGORITMO TablaDiferenciasProgresivas(x[0..n], y[0..n]):
    // Requiere nodos equiespaciados: x[i] = x[0] + i*h
    VerificarEquiespaciado(x)
    T := matriz (n+1) x (n+1) llena de 0
    para i := 0..n hacer
        T[i,0] := y[i]
    fin para
    para j := 1..n hacer
        para i := 0..n-j hacer
            T[i,j] := T[i+1,j-1] - T[i,j-1]   // 
        fin para
    fin para
    retornar T
FIN



ALGORITMO NewtonAdelanteEval(x[0..n], T, xq):
    // T es la tabla de diferencias progresivas 
    h := x[1] - x[0]
    p := (xq - x[0]) / h
    P := T[0,0]
    prod := 1
    para k := 1..n hacer
        prod := prod * (p - (k-1))          // p(p-1)(p-2)...
        P := P + (prod / k!) * T[0,k]       // usa factorial de k
    fin para
    retornar P
FIN



ALGORITMO TablaDiferenciasRegresivas(x[0..n], y[0..n]):
    VerificarEquiespaciado(x)
    T := matriz (n+1) x (n+1) llena de 0
    para i := 0..n hacer
        T[i,0] := y[i]
    fin para
    para j := 1..n hacer
        para i := n..j hacer
            T[i,j] := T[i,j-1] - T[i-1,j-1]  //
        fin para
    fin para
    retornar T
FIN


ALGORITMO NewtonAtrasEval(x[0..n], T, xq):
    h := x[1] - x[0]
    q := (xq - x[n]) / h
    P := T[n,0]
    prod := 1
    para k := 1..n hacer
        prod := prod * (q + (k-1))          // q(q+1)(q+2)...
        P := P + (prod / k!) * T[n,k]
    fin para
    retornar P
FIN


# =========================================================
# Utilidades
# =========================================================

is_equispaced <- function(x, tol = 1e-9) {
  if (length(x) < 2) return(TRUE)
  h <- diff(x)
  max(abs(h - h[1])) < tol
}

falling_prod <- function(p, k) {
  # p(p-1)(p-2)...(p-k+1), falling factorial
  if (k == 0) return(1)
  prod(p - 0:(k-1))
}

rising_prod <- function(q, k) {
  # q(q+1)(q+2)...(q+k-1), rising factorial
  if (k == 0) return(1)
  prod(q + 0:(k-1))
}

# =========================================================
# 1) Tabla de diferencias progresivas (Newton hacia adelante)
# =========================================================
forward_diff_table <- function(x, y, check_equispaced = TRUE) {
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  if (check_equispaced && !is_equispaced(x)) {
    stop("Los nodos no son equiespaciados.")
  }
  T <- matrix(0, nrow = n, ncol = n)
  T[, 1] <- y
  if (n >= 2) {
    for (j in 2:n) {
      for (i in 1:(n - j + 1)) {
        T[i, j] <- T[i + 1, j - 1] - T[i, j - 1]  # Delta^j y_i
      }
    }
  }
  T
}

# =========================================================
# 2) Evaluacion Newton hacia adelante
#    P(x) = y0 + p y0 + p(p-1)/2! ^2 y0 + ...
# =========================================================
newton_forward_eval <- function(x, T, xq) {
  n <- length(x)
  h <- x[2] - x[1]
  p <- (xq - x[1]) / h
  # T[1, k] contiene ^(k-1) y_0, con k empezando en 1
  P <- T[1, 1]
  for (k in 2:n) {
    coef <- falling_prod(p, k - 1) / factorial(k - 1)
    P <- P + coef * T[1, k]
  }
  P
}

# =========================================================
# 3) Tabla de diferencias regresivas (Newton hacia atras)
# =========================================================
backward_diff_table <- function(x, y, check_equispaced = TRUE) {
  n <- length(x)
  if (length(y) != n) stop("x e y deben tener la misma longitud.")
  if (check_equispaced && !is_equispaced(x)) {
    stop("Los nodos no son equiespaciados.")
  }
  T <- matrix(0, nrow = n, ncol = n)
  T[, 1] <- y
  if (n >= 2) {
    for (j in 2:n) {
      for (i in n:j) {
        T[i, j] <- T[i, j - 1] - T[i - 1, j - 1]  # Nabla^j y_i
      }
    }
  }
  T
}

# =========================================================
# 4) Evaluacion Newton hacia atras
#    P(x) = y_n + q y_n + q(q+1)/2! ^2 y_n + ...
# =========================================================
newton_backward_eval <- function(x, T, xq) {
  n <- length(x)
  h <- x[2] - x[1]
  q <- (xq - x[n]) / h
  P <- T[n, 1]
  for (k in 2:n) {
    coef <- rising_prod(q, k - 1) / factorial(k - 1)
    P <- P + coef * T[n, k]
  }
  P
}

# =========================================================
# 5) Helpers para tabla "bonita" (opcional)
# =========================================================
format_diff_table <- function(x, T, type = c("forward", "backward")) {
  type <- match.arg(type)
  n <- length(x)
  df <- data.frame(x = x, y = T[, 1])
  colnames(df) <- c("x", "y")
  for (j in 2:n) {
    colname <- if (type == "forward") paste0("Delta^", j - 1)
               else paste0("Nabla^", j - 1)
    df[[colname]] <- T[, j]
  }
  df
}

# =========================================================
# 6) Ejemplos de uso
# =========================================================

## Ejemplo A: f(x) = x^2, x = 0,1,2,3; evaluar en 0.5 (adelante)
xA <- 0:3
yA <- xA^2
TA <- forward_diff_table(xA, yA)           # tabla 
PA <- newton_forward_eval(xA, TA, 0.5)     # ~ 0.25
dfA <- format_diff_table(xA, TA, "forward")
# print(dfA); cat("P(0.5) =", PA, "\n")

## Ejemplo B: f(x) = x^3, x = 1,2,3,4; evaluar en 1.5 (adelante)
xB <- 1:4
yB <- xB^3
TB <- forward_diff_table(xB, yB)
PB <- newton_forward_eval(xB, TB, 1.5)     # ~ 3.375
dfB <- format_diff_table(xB, TB, "forward")

## Ejemplo C: f(x) = x^2, x = 0,1,2,3; evaluar en 2.6 (atras)
xC <- 0:3
yC <- xC^2
TC <- backward_diff_table(xC, yC)
PC <- newton_backward_eval(xC, TC, 2.6)    # ~ 6.76
dfC <- format_diff_table(xC, TC, "backward")

## Ejemplo D: f(x) = x^3, x = 1,2,3,4; evaluar en 3.2 (atras)
xD <- 1:4
yD <- xD^3
TD <- backward_diff_table(xD, yD)
PD <- newton_backward_eval(xD, TD, 3.2)    # ~ 32.768
dfD <- format_diff_table(xD, TD, "backward")


\end{verbatim}


\section{Soluci\'on num\'erica de Ecuaciones Diferenciales}

Muchas ecuaciones diferenciales ordinarias (EDO) que aparecen en ingeniería, física, química, biología, economía y otras ciencias no tienen solución analítica cerrada. En tales casos recurrimos a \textbf{métodos numéricos} para aproximar la solución con una precisión controlada.

Consideraremos problemas de valor inicial (PVI) de la forma
\[
y' = f(t,y), \qquad y(t_0) = y_0.
\]

La idea general es aproximar la solución $y(t)$ en un conjunto discreto de puntos
\[
t_n = t_0 + n h, \qquad n = 0, 1, 2, \dots, N,
\]
donde $h>0$ es el \textbf{tamaño de paso}, y construir una sucesión $\{y_n\}$ que aproxime
\[
y_n \approx y(t_n).
\]

Un $h$ más pequeño suele producir mayor precisión pero requiere más cómputo; un $h$ demasiado grande puede llevar a errores grandes o incluso inestabilidades numéricas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Método de Euler (explícito)}



Partimos del PVI
\[
y' = f(t,y), \qquad y(t_0) = y_0.
\]

La derivada se interpreta como razón de cambio:
\[
y'(t) \approx \frac{y(t+h)-y(t)}{h}.
\]

Aproximando $y'(t_n)$ por $f(t_n,y_n)$, se obtiene
\[
\frac{y_{n+1} - y_n}{h} \approx f(t_n,y_n),
\]
es decir,
\[
\boxed{
y_{n+1} = y_n + h\,f(t_n,y_n)
}
\]

Este es el \textbf{método de Euler explícito}.

\textbf{Ejemplo 1 (Euler): cálculo a mano}

Resolver numéricamente el PVI
\[
y' = y - t^2 + 1,\qquad y(0) = 0.5,
\]
en el intervalo $[0,1]$ con tamaño de paso $h = 0.2$.

Definimos
\[
f(t,y) = y - t^2 + 1.
\]

Los puntos de malla son:
\[
t_0 = 0.0,\quad t_1 = 0.2,\quad t_2 = 0.4,\quad t_3 = 0.6,\quad t_4 = 0.8,\quad t_5 = 1.0.
\]

\paragraph{Paso 0. Condición inicial.}
\[
t_0 = 0,\qquad y_0 = 0.5.
\]

\paragraph{Paso 1: de $t_0=0$ a $t_1=0.2$.}
\[
f(t_0,y_0) = f(0,0.5) = 0.5 - 0^2 + 1 = 1.5.
\]
\[
y_1 = y_0 + h f(t_0,y_0) = 0.5 + 0.2(1.5) = 0.5 + 0.3 = 0.8.
\]

\paragraph{Paso 2: de $t_1=0.2$ a $t_2=0.4$.}
\[
f(t_1,y_1) = f(0.2,0.8) = 0.8 - 0.2^2 + 1 = 0.8 - 0.04 + 1 = 1.76.
\]
\[
y_2 = y_1 + 0.2 f(t_1,y_1) = 0.8 + 0.2(1.76) = 0.8 + 0.352 = 1.152.
\]

\paragraph{Paso 3: de $t_2=0.4$ a $t_3=0.6$.}
\[
f(t_2,y_2) = f(0.4,1.152) = 1.152 - 0.4^2 + 1 = 1.152 - 0.16 + 1 = 1.992.
\]
\[
y_3 = 1.152 + 0.2(1.992) = 1.152 + 0.3984 = 1.5504.
\]

\paragraph{Paso 4: de $t_3=0.6$ a $t_4=0.8$.}
\[
f(t_3,y_3) = f(0.6,1.5504) = 1.5504 - 0.6^2 + 1 = 1.5504 - 0.36 + 1 = 2.1904.
\]
\[
y_4 = 1.5504 + 0.2(2.1904) = 1.5504 + 0.43808 = 1.98848.
\]

\paragraph{Paso 5: de $t_4=0.8$ a $t_5=1.0$.}
\[
f(t_4,y_4) = f(0.8,1.98848) = 1.98848 - 0.8^2 + 1 = 1.98848 - 0.64 + 1 = 2.34848.
\]
\[
y_5 = 1.98848 + 0.2(2.34848) = 1.98848 + 0.469696 = 2.458176.
\]

\paragraph{Tabla resumen (Euler).}
\[
\begin{array}{c|c|c|c}
n & t_n & f(t_n,y_n) & y_n \\ \hline
0 & 0.0 & 1.50000 & 0.50000 \\
1 & 0.2 & 1.76000 & 0.80000 \\
2 & 0.4 & 1.99200 & 1.15200 \\
3 & 0.6 & 2.19040 & 1.55040 \\
4 & 0.8 & 2.34848 & 1.98848 \\
5 & 1.0 &   -     & 2.45818 \\
\end{array}
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Implementación en R (Euler, versión manual)}

\begin{verbatim}
# Método de Euler para y' = y - t^2 + 1, y(0) = 0.5

f <- function(t, y) {
  y - t^2 + 1
}

t0 <- 0
y0 <- 0.5
T  <- 1
h  <- 0.2

t_vals <- seq(t0, T, by = h)
N <- length(t_vals)

y_vals <- numeric(N)
y_vals[1] <- y0  # condición inicial

for (n in 1:(N-1)) {
  t_n <- t_vals[n]
  y_n <- y_vals[n]
  y_vals[n+1] <- y_n + h * f(t_n, y_n)
}

data.frame(n = 0:(N-1),
           t = t_vals,
           y_aprox = y_vals)
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Implementación en R (Euler, versión automatizada)}

\begin{verbatim}
# Implementación genérica del método de Euler

euler <- function(f, t0, y0, T, h) {
  t_vals <- seq(t0, T, by = h)
  N <- length(t_vals)
  y_vals <- numeric(N)
  y_vals[1] <- y0
  
  for (n in 1:(N-1)) {
    y_vals[n+1] <- y_vals[n] + h * f(t_vals[n], y_vals[n])
  }
  
  data.frame(t = t_vals, y = y_vals)
}

# Ejemplo de uso:
f <- function(t, y) y - t^2 + 1
sol_euler <- euler(f = f, t0 = 0, y0 = 0.5, T = 1, h = 0.2)
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Método de Euler Mejorado (Heun)}


El método de Euler mejorado (Heun) utiliza dos evaluaciones de $f$ por paso.

\[
k_1 = f(t_n, y_n),
\]
\[
\tilde{y}_{n+1} = y_n + h k_1,
\]
\[
k_2 = f(t_n + h, \tilde{y}_{n+1}),
\]
\[
\boxed{
y_{n+1} = y_n + \frac{h}{2}(k_1 + k_2)
}
\]

Este método es de orden 2 en el error global, por lo que proporciona mayor precisión que Euler con el mismo tamaño de paso.

\textbf{Ejemplo 2 (Heun): cálculo a mano}

Resolvámos el PVI
\[
y' = y + t, \qquad y(0) = 1,
\]
en el intervalo $[0,0.3]$ con $h=0.1$.

Definimos
\[
f(t,y) = y + t.
\]

Los puntos de malla:
\[
t_0 = 0.0,\quad t_1 = 0.1,\quad t_2 = 0.2,\quad t_3 = 0.3.
\]

\paragraph{Paso 0. Condición inicial.}
\[
t_0 = 0,\qquad y_0 = 1.
\]

\paragraph{Paso 1: de $t_0=0$ a $t_1=0.1$.}
\[
k_1 = f(0, 1) = 1 + 0 = 1.
\]
\[
\tilde{y}_1 = y_0 + h k_1 = 1 + 0.1(1) = 1.1.
\]
\[
k_2 = f(0.1, 1.1) = 1.1 + 0.1 = 1.2.
\]
\[
y_1 = y_0 + \frac{h}{2}(k_1 + k_2)
    = 1 + 0.05(1 + 1.2)
    = 1 + 0.05(2.2)
    = 1.11.
\]

Los pasos siguientes se calculan de forma análoga.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Implementación en R (Heun, versión manual)}

\begin{verbatim}
# Método de Heun para y' = y + t, y(0) = 1

f <- function(t, y) {
  y + t
}

t0 <- 0
y0 <- 1
T  <- 0.3
h  <- 0.1

t_vals <- seq(t0, T, by = h)
N <- length(t_vals)
y_vals <- numeric(N)
y_vals[1] <- y0

for (n in 1:(N-1)) {
  t_n <- t_vals[n]
  y_n <- y_vals[n]
  
  k1 <- f(t_n, y_n)
  y_tilde <- y_n + h * k1
  k2 <- f(t_n + h, y_tilde)
  
  y_vals[n+1] <- y_n + (h/2) * (k1 + k2)
}

data.frame(n = 0:(N-1),
           t = t_vals,
           y_aprox = y_vals)
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Implementación automatizada en R (Heun)}

\begin{verbatim}
# Implementación genérica del método de Heun

heun <- function(f, t0, y0, T, h) {
  t_vals <- seq(t0, T, by = h)
  N <- length(t_vals)
  y_vals <- numeric(N)
  y_vals[1] <- y0
  
  for (n in 1:(N-1)) {
    t_n <- t_vals[n]
    y_n <- y_vals[n]
    
    k1 <- f(t_n, y_n)
    y_tilde <- y_n + h * k1
    k2 <- f(t_n + h, y_tilde)
    
    y_vals[n+1] <- y_n + (h/2) * (k1 + k2)
  }
  
  data.frame(t = t_vals, y = y_vals)
}

# Ejemplo de uso
f <- function(t, y) y + t
sol_heun <- heun(f = f, t0 = 0, y0 = 1, T = 0.3, h = 0.1)
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Método de Runge--Kutta de cuarto orden (RK4)}


El método de Runge--Kutta de cuarto orden (RK4) calcula cuatro pendientes:

\[
\begin{aligned}
k_1 &= f(t_n, y_n),\\
k_2 &= f\!\left(t_n+\frac{h}{2},\, y_n+\frac{h}{2}k_1\right),\\
k_3 &= f\!\left(t_n+\frac{h}{2},\, y_n+\frac{h}{2}k_2\right),\\
k_4 &= f\!\left(t_n+h,\, y_n+h k_3\right).
\end{aligned}
\]

La actualización es
\[
\boxed{
y_{n+1} = y_n + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4).
}
\]

Este método es de orden 4, con error global $O(h^4)$.

\textbf{Ejemplo 3 (RK4): cálculo a mano}

Consideremos
\[
y' = -3y, \qquad y(0) = 5,
\]
con $h=0.2$. El primer paso (de $t_0=0$ a $t_1=0.2$) se calcula como:

\[
k_1 = f(0,5) = -3\cdot 5 = -15,
\]
\[
k_2 = f\left(0.1, 5 + \frac{0.2}{2}(-15)\right)
    = f(0.1, 3.5) = -3\cdot 3.5 = -10.5,
\]
\[
k_3 = f\left(0.1, 5 + \frac{0.2}{2}(-10.5)\right)
    = f(0.1,3.95) = -3\cdot 3.95 = -11.85,
\]
\[
k_4 = f\left(0.2, 5 + 0.2(-11.85)\right)
    = f(0.2,2.63) = -3\cdot 2.63 = -7.89.
\]

\[
y_1 = 5 + \frac{0.2}{6}\bigl(k_1 + 2k_2 + 2k_3 + k_4\bigr).
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Implementación en R (RK4 manual)}

\begin{verbatim}
# Método RK4 para y' = -3y, y(0) = 5

f <- function(t,y) {
  -3*y
}

t0 <- 0
y0 <- 5
T  <- 0.6
h  <- 0.2

t_vals <- seq(t0, T, by=h)
N <- length(t_vals)
y_vals <- numeric(N)
y_vals[1] <- y0

for (n in 1:(N-1)) {
  t_n <- t_vals[n]
  y_n <- y_vals[n]
  
  k1 <- f(t_n, y_n)
  k2 <- f(t_n + h/2, y_n + h*k1/2)
  k3 <- f(t_n + h/2, y_n + h*k2/2)
  k4 <- f(t_n + h,   y_n + h*k3)
  
  y_vals[n+1] <- y_n + (h/6)*(k1 + 2*k2 + 2*k3 + k4)
}

data.frame(n = 0:(N-1),
           t = t_vals,
           y_aprox = y_vals)
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Implementación automatizada en R (RK4)}

\begin{verbatim}
# Implementación genérica del método RK4

rk4 <- function(f, t0, y0, T, h) {
  t_vals <- seq(t0, T, by=h)
  N <- length(t_vals)
  y_vals <- numeric(N)
  y_vals[1] <- y0
  
  for (n in 1:(N-1)) {
    t_n <- t_vals[n]
    y_n <- y_vals[n]
    
    k1 <- f(t_n, y_n)
    k2 <- f(t_n + h/2, y_n + h*k1/2)
    k3 <- f(t_n + h/2, y_n + h*k2/2)
    k4 <- f(t_n + h,   y_n + h*k3)
    
    y_vals[n+1] <- y_n + (h/6)*(k1 + 2*k2 + 2*k3 + k4)
  }
  
  data.frame(t = t_vals, y = y_vals)
}

# Ejemplo de uso
f <- function(t, y) -3*y
sol_rk4 <- rk4(f = f, t0 = 0, y0 = 5, T = 0.6, h = 0.2)
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Método de Euler implícito}


Para el PVI
\[
y' = f(t,y), \qquad y(t_0) = y_0,
\]
el método de Euler implícito define
\[
\boxed{
y_{n+1} = y_n + h\,f(t_{n+1}, y_{n+1}).
}
\]

Aquí $y_{n+1}$ aparece en ambos lados. En general, para $f$ no lineal en $y$, se debe resolver en cada paso la ecuación
\[
G(y_{n+1}) := y_{n+1} - h\,f(t_{n+1}, y_{n+1}) - y_n = 0
\]
mediante un método numérico (por ejemplo, Newton).

\textbf{Caso lineal: $f(t,y) = a(t)y + b(t)$}

Si
\[
f(t,y) = a(t)y + b(t),
\]
entonces
\[
y_{n+1} = y_n + h\,[a(t_{n+1}) y_{n+1} + b(t_{n+1})].
\]
Reordenando:
\[
y_{n+1}\bigl(1 - h a(t_{n+1})\bigr) = y_n + h b(t_{n+1}),
\]
\[
\boxed{
y_{n+1} = \frac{y_n + h b(t_{n+1})}{1 - h a(t_{n+1})}.
}
\]

\textbf{Ejemplo 4 (Euler implícito): cálculo a mano}

Consideremos la ecuación rígida
\[
y' = -50y + 10,\qquad y(0)=0,
\]
en el intervalo $[0,0.2]$ con $h=0.05$.

Aquí
\[
a(t) = -50,\qquad b(t) = 10.
\]

Entonces
\[
y_{n+1} = \frac{y_n + h\cdot 10}{1 - h(-50)}
= \frac{y_n + 0.5}{1 + 2.5}
= \frac{y_n + 0.5}{3.5}.
\]

Puntos de malla:
\[
t_0 = 0.00,\quad t_1 = 0.05,\quad t_2 = 0.10,\quad t_3 = 0.15,\quad t_4 = 0.20.
\]

\paragraph{Paso 0. Condición inicial.}
\[
y_0 = 0.
\]

\paragraph{Paso 1.}
\[
y_1 = \frac{0 + 0.5}{3.5} \approx 0.142857.
\]

\paragraph{Paso 2.}
\[
y_2 = \frac{y_1 + 0.5}{3.5}
    = \frac{0.142857 + 0.5}{3.5}
    \approx 0.183673.
\]

\paragraph{Paso 3.}
\[
y_3 = \frac{0.183673 + 0.5}{3.5}
    \approx 0.195335.
\]

\paragraph{Paso 4.}
\[
y_4 = \frac{0.195335 + 0.5}{3.5}
    \approx 0.198667.
\]

\paragraph{Tabla resumen (Euler implícito).}
\[
\begin{array}{c|c|c}
n & t_n & y_n \\ \hline
0 & 0.00 & 0.000000 \\
1 & 0.05 & 0.142857 \\
2 & 0.10 & 0.183673 \\
3 & 0.15 & 0.195335 \\
4 & 0.20 & 0.198667 \\
\end{array}
\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Implementación en R (Euler implícito, versión manual)}

\begin{verbatim}
# Euler implícito para y' = -50y + 10, y(0) = 0, h = 0.05

h <- 0.05
t_vals <- seq(0, 0.2, by = h)
N <- length(t_vals)

y_vals <- numeric(N)
y_vals[1] <- 0  # y0

for (n in 1:(N-1)) {
  y_vals[n+1] <- (y_vals[n] + 0.5) / 3.5
}

data.frame(n = 0:(N-1),
           t = t_vals,
           y_aprox = y_vals)
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Implementación automatizada en R (Euler implícito lineal)}

\begin{verbatim}
# Euler implícito para y' = a(t)*y + b(t)

euler_imp_lineal <- function(a_fun, b_fun, t0, y0, T, h) {
  t_vals <- seq(t0, T, by = h)
  N <- length(t_vals)
  y_vals <- numeric(N)
  y_vals[1] <- y0
  
  for (n in 1:(N-1)) {
    t_next <- t_vals[n+1]
    a_next <- a_fun(t_next)
    b_next <- b_fun(t_next)
    
    # y_{n+1} = (y_n + h*b_next) / (1 - h*a_next)
    y_vals[n+1] <- (y_vals[n] + h * b_next) / (1 - h * a_next)
  }
  
  data.frame(t = t_vals, y = y_vals)
}

# Ejemplo: y' = -50y + 10
a_fun <- function(t) -50
b_fun <- function(t) 10

sol_imp <- euler_imp_lineal(a_fun = a_fun,
                            b_fun = b_fun,
                            t0 = 0, y0 = 0,
                            T = 0.2, h = 0.05)
\end{verbatim}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicios propuestos}


Resuelva numéricamente con Euler, construyendo la tabla de iteraciones y, si es posible, comparando con la solución exacta.

\begin{enumerate}
  \item $y' = y - t$, \quad $y(0) = 1$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = 3t^2 - y$, \quad $y(0) = 2$, \quad $h = 0.05$, \quad intervalo $[0,0.5]$.
  \item $y' = \sin(t) - y^2$, \quad $y(0) = 0$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = t y$, \quad $y(0) = 1$, \quad $h = 0.2$, \quad intervalo $[0,1]$.
  \item $y' = e^{-t} - y$, \quad $y(0) = 3$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = y(1-y)$ (modelo logístico), \quad $y(0) = 0.2$, \quad $h = 0.1$, \quad intervalo $[0,2]$.
  \item $y' = 4y + t$, \quad $y(0) = 1$, \quad $h = 0.05$, \quad intervalo $[0,0.5]$.
  \item $y' = \cos(t) + y$, \quad $y(0) = 0$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = -5y$, \quad $y(0) = 2$, \quad $h = 0.2$, \quad intervalo $[0,1]$.
  \item $y' = t^3 - y^2$, \quad $y(0) = 1$, \quad $h = 0.05$, \quad intervalo $[0,0.5]$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Aplique el método de Euler mejorado (Heun), mostrando los valores de $k_1$ y $k_2$ en cada paso.

\begin{enumerate}
  \item $y' = y + t^2$, \quad $y(0) = 1$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = \cos(t) - y$, \quad $y(0) = 0$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = t - y^2$, \quad $y(0) = 0.5$, \quad $h = 0.05$, \quad intervalo $[0,0.5]$.
  \item $y' = 2y + e^t$, \quad $y(0) = 1$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = y(1+t)$, \quad $y(0) = 1$, \quad $h = 0.2$, \quad intervalo $[0,1]$.
  \item $y' = -3y + t$, \quad $y(0) = 2$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = t^2\sqrt{y}$ (suponer $y\ge 0$), \quad $y(0) = 1$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = y^2 - y$, \quad $y(0) = 0.1$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = 1 - t y$, \quad $y(0) = 1$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = 5 - 4y$, \quad $y(0) = 0$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Use el método de Runge--Kutta de cuarto orden, calculando explícitamente $k_1, k_2, k_3, k_4$.

\begin{enumerate}
  \item $y' = y - t^2 + 1$, \quad $y(0) = 0.5$, \quad $h = 0.2$, \quad intervalo $[0,1]$.
  \item $y' = -2y + \sin(t)$, \quad $y(0) = 1$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = y^2 + t$, \quad $y(0) = 0$, \quad $h = 0.05$, \quad intervalo $[0,0.5]$.
  \item $y' = e^{-t} - y$, \quad $y(0) = 2$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = 3y(1-y)$ (modelo logístico), \quad $y(0) = 0.1$, \quad $h = 0.1$, \quad intervalo $[0,2]$.
  \item $y' = t\cos(y)$, \quad $y(0) = 0$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = -5y + 5$, \quad $y(0) = 0$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = \sqrt{t + y}$ (suponer $t+y \ge 0$), \quad $y(0) = 1$, \quad $h = 0.1$, \quad intervalo $[0,1]$.
  \item $y' = y\ln(t+1)$, \quad $y(0) = 1$, \quad $h = 0.2$, \quad intervalo $[0,2]$.
  \item $y' = -50y + \cos(t)$, \quad $y(0) = 0$, \quad $h = 0.02$, \quad intervalo $[0,0.4]$.
\end{enumerate}


\section{Tareas del curso}

\begin{Ejer}
Convertir los siguientes números de base 10 a base 2.
\begin{enumerate}
\item $324$
\item $27$
\item $1423$
\item $235.25$
\item $41.596$
\end{enumerate}
\end{Ejer}


\begin{Ejer}
\begin{enumerate}
\item Realizar una revisión de la historia de los m\'etodos num\'ericos, elaborar un documento de hasta dos cuartillas.
\item Realiza las siguientes conversiones de base $10$ a base $2$:
\begin{enumerate}
\item 246
\item 345.68
\item 4586632.2846
\item 984365.27463
\item 79905523
\end{enumerate}
\item Elabora el c\'odigo en R para realizar la conversi\'on de base $10$ a base $2$.

\item Describe exhaustivamente los tipos de errores que existen

\end{enumerate}


\end{Ejer}


\begin{Note}
En los siguientes ejercicios se indicar\'a cuales ejercicios pueden realizarse sin el apoyo de R, se deben de resolver al menos dos ejercicios de cada serie sin el apoyo de R, es decir, se tienen que resolver manualmente.
\end{Note}

\begin{Ejer}
Resolver por eliminación Gaussiana Simple los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
x_1-2x_2+0.5x_3&=&-5\\
-2x_1+5x_{2}-1.5x_3&=&0\\
-0.2x_1+1.75x_2-x_3&=&10
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-x_2+6x_4&=&2.3\\
4x_1+2x_2-x_3-5x_4&=&6.9\\
-5x_1+x_2-3x_3&=&-36\\
10x_2-4x_3+7x_4&=&-36
\end{eqnarray*}

\item \begin{eqnarray*}
0.003000x_1+59.14x_2&=&59.17\\
5.291x_1-6.130x_2&=&46.78
\end{eqnarray*}
utilizar redondeo a 4 cifras significativas.

\item \begin{eqnarray*}
4x_1+2x_2&=&2\\
2x_1+3x_2+x_3&=&-1\\
x_2+\frac{5}{2}x_3&=&3
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.1x_2-0.2x_3&=&7.85\\
0.1x_{1}+7x_2-0.3x_3=-19.3\\
0.3x_1-0.2x_2+10x_3&=&71.4
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
8x_1+2x_2-2x_3&=&-2\\
10x_1+2x_2+4x_3&=&4\\
12x_1+2x_2+2x_3&=&6
\end{eqnarray*}

\item\textbf{*} \begin{eqnarray*}
5x_1 + 2x_2 + x_3 - x_4 = 1\\
2x_1 - x_2 + 3x_3 + 2x_4 = 12\\
4x_1 + x_2 - 2x_3 + 3x_4 = 5\\
-2x_1 + 2x_2 + x_3 + x_4 = 2
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
2x_1+3x_2+2x_3+4x_4&=&4\\
4x_1+10x_2-4x_3&=&-8\\
-3x_1-2x_2-5x_3-2x_4&=&-4\\
-2x_1+4x_2+4x_3-7x_4&=&-1
\end{eqnarray*}

\item \begin{eqnarray*}
1.133x_1+5.281x_2-2.454x_3&=&6.414\\
24.14x_1-1.21x_2+5.281x_3&=&113.8\\
-10.123x_1+6.387x_2-x_3&=&1
\end{eqnarray*}

\item \begin{eqnarray*}A=\left(\begin{array}{ccccc}
2 & 3 & 2 & 4\\
4 & 10 &-4 & 0\\
-3 & -2 & -5 &-2\\
-2 & 4 & 4 &-7\\
\end{array}\right)\end{eqnarray*} y \begin{eqnarray*}b=\left(\begin{array}{c}\\
9 \\
-15\\
6 \\
2\\
\end{array}\right)\end{eqnarray*}

\end{enumerate}
\end{Ejer}

\begin{Ejer}
Resolver por eliminación gaussiana con pivoteo parcial los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
0.4x_1-1.5x_2+0.75x_3&=&-20\\
-0.5x_1-15x_2+10x_3&=&-10\\
-10x_1-9x_2+2.5x_3&=&30
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
5x_1-8x_2+x_3&=&-71\\
-2x_1+6x_2-9x_3&=&134\\
3x_1-5x_2+2x_3&=&-58
\end{eqnarray*}

\item \begin{eqnarray*}
0.003000x_1+59.14x_2&=&59.17\\
5.291x_1-6.130x_2&=&46.78
\end{eqnarray*}
utilizar redondeo a 4 cifras significativas.


\item \textbf{*}\begin{eqnarray*}
-x_2+4x_3-x_4&=&-1\\
-x_1+4x_2-x_3&=&2\\
-x_1-x_3+4x_4&=&4\\
4x_1-x_2&=&10
\end{eqnarray*}

\item \begin{eqnarray*}
0.00031000x_1+1.000000x_2&=&3.000000\\
1.00045534x_1+1.00034333x_2&=&7.000
\end{eqnarray*}


\item \textbf{*} Resolver para $A=\left(\begin{array}{ccccc}\\
14 & 14 & -9 & 3 & -5\\
14 & 52 & -15 & 2 & -32\\
-9 & -15 & 36 &-5 & 16\\
3 &2 &-5&47 & 49\\
-5 & 32 & 16 &49 & 79\end{array}\right)$,  $b=\left(\begin{array}{c}\\
-15\\
-100\\
106\\
329\\
463\end{array}\right)$ y  $X=\left(\begin{array}{c}\\
x_1\\
x_2\\
x_3\\
x_4\\
x_5\end{array}\right)$


\item Resolver para $A=\left(\begin{array}{ccccc}
\frac{1}{4} &\frac{1}{5} &\frac{1}{6}\\
 \frac{1}{3} & \frac{1}{4} & \frac{1}{5} \\  
  \frac{1}{2} &  1& 2
\end{array}\right)$,  $b=\left(\begin{array}{c}
9\\
8\\
8\\\end{array}\right)$ y  $X=\left(\begin{array}{c}
x_1\\
x_2\\
x_3\\
\end{array}\right)$

\item  Resolver para $A=\left(\begin{array}{ccccc}
1 & \frac{1}{2} & \frac{1}{3} & \frac{1}{4}  \\
 \frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \frac{1}{5}\\
 \frac{1}{3} & \frac{1}{4} & \frac{1}{5} & \frac{1}{6}\\
 \frac{1}{4} & \frac{1}{5} & \frac{1}{6} & \frac{1}{7}
 \end{array}\right)$,  $b=\left(\begin{array}{c}
 \frac{1}{6} \\
 \frac{1}{7} \\
  \frac{1}{8}\\ 
 \frac{1}{9} \\\end{array}\right)$ y  $X=\left(\begin{array}{c}
x_1\\
x_2\\
x_3\\
x_4
\end{array}\right)$

\item Resolver el sistema
\begin{eqnarray*}
2x_1+x_2-x_3+x_4-3x_5&=&7\\
x_1+2x_3-x_4+x_5&=&2\\
-2x_2-x_3+x_4-x_5&=&-5\\
3x_1+x_2-4x_3+5x_5&=&6\\
x_1-x_2-x_3-x_4+x_5&=&3
\end{eqnarray*}



\item Resolver el sistema
\begin{eqnarray*}
3.333x_1+15920x_2-10.333x_3&=&15913\\
2.222x_1+16.71x_29.612x_2&=&28.544\\
1.5611x_1+5.1791x_2+1.6852x_3&=&8.4254
\end{eqnarray*}

\end{enumerate}
\end{Ejer}


\begin{Ejer}
Resolver por el método de Gauss-Jordan los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
x_1+2x_2+3x_3&=&1\\
-0.4x_1+2x_2-x_3&=&10\\
0.5x_1-3x_2+x_3&=&15
\end{eqnarray*}

\item \begin{eqnarray*}
2x_1-0.9x_2+3x_3&=&-3.61\\
-0.5x_1+0.1x_2-x_3&=&2.035\\
x_1-6.35x_2-0.45x_3&=&15.401
\end{eqnarray*}

\item \begin{eqnarray*}
0.7x_1+2.7x_2-6x_3+0.7x_4&=&1.6487\\
2x_1-0.8x_2+3x_3-x_4&=&-2.342\\
-x_1-1.5x_2+1.4x_3+3x_4&=&-4.189\\
7x_2-1.56x_3+x_4=15.792
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.1x_2-0.2x_3&=&7.85\\
0.1x_1+7x_2-0.3x_3&=&-19.3\\
0.3x_1-0.2x_2+10x_3&=&71.4
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
10x_1+2x_2-x_3&=&27\\
-3x_1-6x_2+2x3&=&-61.5\\
x_1+x_2+5x_3&=&-21.5
\end{eqnarray*}

\item $A=\left(\begin{array}{ccccc}\\
1 &3 & -2 & 1\\
1  &3 & -1 & 2\\
0  &1 & -1 & 4\\
2  &6 & 1 & 2\\
\end{array}\right)$ y $b=\left(\begin{array}{c}
4 \\
1 \\
5\\ 
2\\\end{array}\right)$

\item \begin{eqnarray*}
6x_1-x_2-x_3+4x_4&=&17\\
x_1-10x_2+2x_3-x_4&=&-17\\
3x_1-2x_2+8x_3-x_4&=&19\\
x_1+x_2+x_3-5x_4&=&-14
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
x+2y+3z+4w&=&1\\
x-4y+z+11w&=&2\\
-x+8y+7z+6w&=&-2\\
16x+8y-5z+6w&=&11
\end{eqnarray*}


\item \textbf{*}\begin{eqnarray*}
x_1+x_2&=&3\\
x_1+2x_2+x_3&=&-1\\
x_2+3x_3+x_4&=&2\\
x_3+4x_4+x_5&=&1\\
x_4+5x_5&=&3
\end{eqnarray*}

\item \begin{eqnarray*}
15x_1-18x_2+15x_3-3x_4&=&11\\
-18x_1+24_2-18x_3+4x_4&=&10\\
15x_1-18x_2+18x_3-3x_4&=&11\\
-3x_1+4x_2-3x_3+x_4&=&13
\end{eqnarray*}

\end{enumerate}
\end{Ejer}

\begin{Ejer}
Resolver por el método de Gauss-Seidel los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
3x_1-0.2x_2-0.5x_3&=&8\\
0.1x_1+7x_2+0.4x_3&=&-19.5\\
0.4x_1-0.1x_2+10x_3&=&72.4
\end{eqnarray*}

\item \begin{eqnarray*}
-5x_1+1.4x_2-2.7x_3&=&94.2\\
0.7x_1-2.5x_2+15x_3&=&-6\\
3.3x_1-11x_2+4.4x_3&=&-27.5
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.5x_2+0.6x_3&=&5.24\\
0.3x_1-4x_2-x_3&=&-0.387\\
-0.7x_1+2x_2+7x_3&=&14.803
\end{eqnarray*}

\item \begin{eqnarray*}
5x_1-0.2x_2+x_3&=&1.5\\
0.1x_1+3x_2-0.5x_3&=&-2.7\\
-0.3x_1+x_2-7x_3&=&9.5
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
-3x_2+7x_3&=&2\\
x_1+2x_2-x_3&=&3\\
12x_1+2x_2+2x_3&=&6
\end{eqnarray*}

\item \begin{eqnarray*}
0.15_1+2.11x_2+30.75x_3&=&-26.38\\
0.64x_1+1.21x_2+2.05x_3&=&1.01\\
3.21x_1+1.53x_2+1.04x_3&=&5.23
\end{eqnarray*}


\item \textbf{*}\begin{eqnarray*}
x_1+x_2-x_3&=&-3\\
6x_1+2x_2+2x_3&=&2\\
-3x_1+4x_2+x_3&=&1
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
2x_1+x_2-x3&=&1\\
5x_1+2x_2+2x_3&=&-4\\
3x_1+x_2+x_3&=&5
\end{eqnarray*}

\item \begin{eqnarray*}
3x-0.1y-0.2z&=&7.85\\
0.1x+7y-0.3z&=&-19.3\\
0.3x_1-0.2x_2+10x_3=71.4
\end{eqnarray*}


\item \begin{eqnarray*}
17x_1-2x_2-3x_3=500\\
-5x_1+21x_2-2x_3&=&200\\
-5x_1-5x_2+22x_3&=&30
\end{eqnarray*}

\end{enumerate}
\end{Ejer}



\begin{Ejer}
Aplicar el método de Jacobi para resolver los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item $A=\left(\begin{array}{cccc|c}\\
10 & 2 &  -1 &  0 &  26\\
1 & 20 & -2 & 3 & -15\\
-2 & 1 & 30 & 0 & 53\\
1 & 2 & 3 & 20 & 47
\end{array}\right)$


\item \textbf{*}$A=\left(\begin{array}{ccc|c}\\
-1 & 2 & 10 & 11\\ 
11 & -1 & 2 & 12\\
1 & 5 & 2 & 8
\end{array}\right)$

\item $A=\left(\begin{array}{ccc|c}\\
8 & 2 & 3 & 51\\
2 & 5 & 1 & 23\\
-3 & 1 & 6 & 20
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
2 & -1 & 1 & 3 & 10\\
2 & 2 & 2 & 2 & 1\\
-1 & -1 & 2 & 2 & -5\\
3 & 1 & -1 & 4 & 6
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
3 & 1 & 1 & -1 & 5\\
0 & 2 & 1 & 4 & 0\\
1 & 1 & -1 & 9 & 1\\
2 & 4 & 6 & 3 & 0
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
10 & -1 & 2 & 0 & 6\\
-1 & 11 & -1 & 3 & 25\\
2 & -1 & 10 & -1 & -11\\
0 & 2 & -1 & 8 & 15
\end{array}\right)$

\item \textbf{*}\begin{eqnarray*}
x_1+2x_2-2x_3&=&7\\
x_1+x_2+x_3&=&2\\
2x_1+2x_2+x_3&=&5
\end{eqnarray*}

\item \begin{eqnarray*}
-4x_1+14x_2=10\\
-5x_1+13x_2&=8\\
-x_1+2x_3&=&1
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
x+y+2z&=&1\\
x+2y+z&=&1\\
2x+y+z&=&1
\end{eqnarray*}

\item \begin{eqnarray*}
6x_1-2x_2+2x_3+4x_4&=&10\\
12x_1-8x_2+6x_3+10x_4&=&20\\
3x_1-13x_2+9x_3+3x_4&=&2\\
-6x_1+4x_2+x_3-18x_4&=&-19
\end{eqnarray*}


\end{enumerate}
\end{Ejer}

\begin{Ejer} La siguiente serie de ejercicios hay que resolverlos con el apoyo de R, excepto los indicados por un \textbf{*}

\begin{enumerate}
  \item \textbf{*} Resuelva el sistema
  \begin{eqnarray*}
  \begin{cases}
  2x+y-z=1,\\
  -x+3y+2z=12,\\
  x+2y+3z=7
  \end{cases}
  \end{eqnarray*}
  mediante eliminación gaussiana simple (sin pivoteo).

  \item Resuelva el mismo sistema anterior pero ahora aplicando eliminación gaussiana con pivoteo parcial. Compare los pasos con el ejercicio anterior.

  \item Aplique eliminación gaussiana con pivoteo y escalamiento al sistema
  \begin{eqnarray*}
  \begin{cases}
  10x+2y+z=7,\\
  2x+20y+2z=9,\\
  x+2y+30z=12
  \end{cases}
  \end{eqnarray*}
  y analice la importancia del escalamiento.

  \item \textbf{*}Utilice el método de Gauss--Jordan para calcular la inversa de
  \begin{eqnarray*}
  A=\begin{bmatrix}
  1 & 2 & 1\\
  0 & 1 & -1\\
  2 & 3 & 4
  \end{bmatrix}.
  \end{eqnarray*}

  \item \textbf{*}Resuelva $Ax=b$ con $A$ y $b$ dados por
  \begin{eqnarray*}
  A=\begin{bmatrix}
  4 & -2 & 1\\
  -2 & 4 & -2\\
  1 & -2 & 3
  \end{bmatrix},\qquad
  b=\begin{bmatrix}1\\4\\2\end{bmatrix},
  \end{eqnarray*}
  utilizando factorización $LU$ y sustitución hacia adelante y hacia atrás.

  \item Calcule la factorización de Cholesky de
  \begin{eqnarray*}
  A=\begin{bmatrix}
  25 & 15 & -5\\
  15 & 18 & 0\\
  -5 & 0 & 11
  \end{bmatrix}
  \end{eqnarray*}
  y resuelva $Ax=b$ con $b=(35,33,6)^\top$.
\end{enumerate}


\begin{enumerate}\setcounter{enumi}{6}
  \item \textbf{*}Resuelva mediante sustitución hacia atrás el sistema triangular superior:
  \begin{eqnarray*}
  \begin{cases}
  2x+3y-z=5,\\
  -y+2z=4,\\
  3z=6.
  \end{cases}
  \end{eqnarray*}

  \item \textbf{*}Resuelva mediante sustitución hacia adelante el sistema triangular inferior:
  \begin{eqnarray*}
  \begin{cases}
  x=3,\\
  2y+x=5,\\
  z-y+2x=10.
  \end{cases}
  \end{eqnarray*}

\end{enumerate}


\begin{enumerate}\setcounter{enumi}{9}
  \item Aplique el método de Jacobi para resolver
  \begin{eqnarray*}
  \begin{cases}
  10x-y+2z=6,\\
  -x+11y-z+3w=25,\\
  2x-y+10z-w=-11,\\
  3y-z+8w=15,
  \end{cases}
  \end{eqnarray*}
  realizando 3 iteraciones con $x^{(0)}=\mathbf{0}$.

  \item Repita el ejercicio anterior con el método de Gauss--Seidel. Compare la velocidad de convergencia con Jacobi.

  \item Aplique el método SOR con $\omega=1.25$ al mismo sistema y compare las tres trayectorias de convergencia.

  \item Escriba la matriz de iteración $T_J$ y el vector $c_J$ del método de Jacobi para el sistema
  \begin{eqnarray*}
  \begin{cases}
  4x+y=9,\\
  x+3y=7.
  \end{cases}
  \end{eqnarray*}
  Verifique si $\rho(T_J)<1$.

  \item Investigue experimentalmente en R cuál es el valor óptimo aproximado de $\omega$ para el método SOR en el sistema $4\times 4$ del ejercicio 10.
\end{enumerate}


\begin{enumerate}\setcounter{enumi}{14}
  \item Genere una matriz aleatoria simétrica definida positiva $5\times 5$ en R y resuelva $Ax=b$:
 \begin{itemize}
    \item[(a)] Usando factorización $LU$.  
    \item[(b)] Usando factorización de Cholesky.  
    \item[(c)] Usando Gauss--Seidel con 20 iteraciones.  
  \end{itemize}
  Compare el tiempo de cómputo y la precisión de cada método.
\end{enumerate}


\end{Ejer}



\begin{Ejer}
Genera un archivo tipo Rmd con las prácticas realizadas en el laboratorio, en las que deberá de desplegarse una barra lateral izquierda con un nivel de profundidad de 4: seccion, subseccion, subsubseccion y subsubsubseccion.
\end{Ejer}




\begin{Ejer}

Revisa en R los códigos para resolver sistemas de ecuaciones lineales, esto en un archivo tipo Rmd, mismo que deberás de entregar.



\end{Ejer}

\newpage



\section{Apendice A: Breve historia de los M\'etodos Num\'ericos}


Un \textit{m\'etodo num\'erico} es un proceso matem\'atico \textit{iterativo} cuyo objetivo es encontrar la aproximaci\'on a una soluci\'on espec\'ifica con un cierto error previamente determinado. Los m\'etodos num\'ericos requieren de una aproximaci\'on a la soluci\'on real al problema, misma que es corregida a trav\'es de la repetici\'on de un cierto proceso que debe arrojar soluciones cada vez m\'as cercanas al valor real. Cada correcci\'on de un valor inicial se conoce como \textit{iteraci\'on}. El proceso es controlado por medio de la medici\'on de una cantidad de error predefinido entre dos aproximaciones sucesivas.

La historia de los m\'etodos num\'ericos es la colecci\'on de acontecimientos matem\'aticos en los que se resuelven problemas sin el uso de la matem\'atica anal\'itica. Algunos de los m\'etodos m\'as utilizados en la actualidad fueron creados mucho antes de la invenci\'on de la computadora; su aplicaci\'on era extenuante y complicada porque cada iteraci\'on requer\'ia de una diversidad de operaciones aritm\'eticas que se realizaban por grupos enteros de calculistas, evidentemente, de forma manual. La historia de los m\'etodos num\'ericos es paralela, al menos desde la mitad del siglo XIX, a la historia de la computaci\'on. Las contribuciones m\'as actuales radican en la creaci\'on de software que minimiza los errores y mejora las aproximaciones de los resultados \cite{Isaacson}.

\begin{itemize}
    \item 1650 a.C. Se crean los Papiros de Rhynd en los que se escribe un m\'etodo para resolver expresiones matem\'aticas sin \'algebra.
    \item 250 a.C. Euclides crea el M\'etodo de Exhausti\'on, que consiste en aproximar figuras geom\'etricas (tri\'angulos, cuadrados, pent\'agonos, etc.) consecutivamente dentro de un c\'irculo para obtener una aproximaci\'on a $\pi$.
    \item Siglo IX d.C. Al Juarismi crea los \textit{algoritmos}.
    \item 1623. John Napier inventa los \textit{huesos de Napier}, que son arreglos pr\'acticos de logaritmos en tablas.
    \item Siglo XVII. Isaac Newton crea los procesos de interpolaci\'on polinomial.
    \item Siglo XVIII. Leibnitz crea el C\'alculo diferencial.
    \item 1768. Euler crea soluciones aproximadas a ecuaciones diferenciales con el principio de la integraci\'on num\'erica. Jacob Stirling y Brook Taylor presentan el C\'alculo de diferencias finitas.
    \item 1822. Charles Babbage inventa la \textit{M\'aquina diferencial}.
    \item 1843. Ada, condesa de Lovelace, publica sus notas sobre la m\'aquina anal\'itica de Charles Babbage.
    \item 1890. (IBM) Tabula el censo estadounidense empleando las m\'aquinas de tarjetas perforadas de Herman Hollerith.
    \item 1931. Vannebar Bush dise\~na el analizador diferencial, un computador anal\'ogico electromec\'anico. En 1945 publicar\'a el art\'iculo \textit{C\'omo podremos pensar} en el que describe la computaci\'on personal.    \item 1937. Alan Turing publica \textit{Sobre los n\'umeros computables}, en el que describe un computador universal. En este mismo a\~no, Howard Aiken propone la construcci\'on de un gran computador y descubre partes de la m\'aquina diferencial de Babbage en Harvard; tambi\'en John Vincent Atanasoff conceptualiza el computador electr\'onico (la cual completar\'a en 1939).
    
    \item 1938. William Hewlett y David Packard crean su empresa en Palo Alto, California, Estados Unidos.
    
    \item 1939. Turing comienza a descifrar los c\'odigos secretos alemanes.
    
    \item 1944. John Von Neumann redacta el primer informe sobre EDVAC. En distintas universidades de Estados Unidos se desarrollan proyectos sobre computadoras cuya aplicaci\'on (secreta) ser\'a apoyar a la milicia en c\'alculos bal\'isticos (ecuaciones diferenciales).
    
    \item 1950. Turing crea su famosa prueba sobre la inteligencia artificial; se suicidar\'a en 1954. J.H. Wilkinson acudi\'o al Laboratorio Nacional de F\'isica de Reino Unido para construir una versi\'on m\'as simple de la m\'aquina de Turing; construy\'o la \textit{ACE (Automatic Computing Engine)} para resolver c\'alculos con matrices.
    
    \item 1953. John W. Backus, empleado de IBM, desarrolla \textit{FORTRAN (Formulae Translating)}, como una alternativa al uso del lenguaje ensamblador; se us\'o por primera vez en una IBM 704.
    
    \item 1958. Se anuncia la creaci\'on de la Agencia de Proyectos de Investigaci\'on Avanzada (ARPA).
    
    \item 1962. Doug Engelbart publica \textit{Aumentar el intelecto humano}; en 1963, junto con Bill English inventar\'a el rat\'on.
    
    \item 1968. Noyce y Moore fundan \textit{INTEL}.
    
    \item 1969. Misi\'on Apolo 11. Katherine Johnson calcula la trayectoria del cohete Mercurio. Dorothy Vaughan se convierte en la supervisora de IBM dentro de la NASA. Mary Jackson es la primera ingeniera aeroespacial en Estados Unidos. Margaret Hamilton escribe el c\'odigo del programa que control\'o la nave. Todas ellas tuvieron una participaci\'on fundamental para que la misi\'on fuera un \'exito.
    
    \item 1970. Investigadores visitantes en el \textit{Argone National Laboratory} de Estados Unidos traducen c\'odigos de \textit{ALGOL} para obtener eigenvalores planteados por Wilkinson para incluirlos en \textit{FORTRAN}. De esta labor nace \textit{EISPACK} en 1976 y posteriormente \textit{LINPACK} en 1976.
    
    \item 1973. Vint Cerf y Bob Kahn completan los protocolos TCP/IP.
    
    \item 1975. Bill Gates y Paul Allen desarrollan el lenguaje de programaci\'on \textit{BASIC}; fundan \textit{Microsoft}. Steve Jobs y Steve Wozniak lanzan el \textit{Apple I}.
    
    \item 1983. Richard Stallman empieza a desarrollar el proyecto \textit{GNU}.
    
    \item 1986. Cleve Moler, a partir de \textit{EISPACK} y \textit{LINPACK}, crea \textit{MATLAB}; funda la empresa \textit{MathWorks}.
    
    \item 1991. Linus Torvalds lanza la primera versi\'on de \textit{Linux}. Tim Berbers-Lee anuncia la \textit{World Wide Web}.
\end{itemize}



%===========================================
\section{Introducción al uso de R}
%===========================================
\textbf{Programas y rutinas en R}

%-------------------------------------------
\subsection{Sesiones en RStudio}
%-------------------------------------------


Al utilizar R, existen varios entornos que facilitan la gestión y ejecución de rutinas,  \textit{archivos con extensión .R}, el más popular es \textit{RStudio} o bien directamente desde la terminal o ejecutando simplemente \textit{R}, la ventaja de \textit{RStudio} es que permite que en una pantalla podamos visualizar: Consola (lugar donde se ejecutan los comandos directamente), History (el histórico de las variables y funciones definidas mismo que puede guardarse para ser invocado posteriormente), Plots (ventana en la que se muestran los gráficos generados), Help (la ayuda sobre comandos, funciones, sintaxis en R), Files (lugar donde se manejan los archivos, es decir leer, guardar, mover o renombrar archivos), y Packages (espacio para instalar o cargar paquetes de manera gráfica), todo esto para facilitar el manejo y ejecución de rutinas compatibles con R. Por otra parte \textbf{Workspace} es un entorno en el que se incluyen todos los objetos definidos,  al final de una sesión de R,  este entorno puede guardarse una imagen del mismo para ser cargada posteriormente. \bigskip


Durante el uso de R en ocasiones se requiere limpiar la consola, para esto al presionar \textbf{Ctrl+L}.  


%-------------------------------------------
\subsection{Uso de R}
%-------------------------------------------

\begin{itemize}
\item \textbf{Constantes}: $\pi$, $exp(1)$

\item Las constantes pueden ser de tipo \textit{integer},  \textit{double} o \textit{complex}, el tipo de constante se puede consultar con la función \textbf{typeof()}

\begin{verbatim}
> typeof('mi constante')
[1] "character"
\end{verbatim}

\item Operadores: $<,>,>=,>=,!=$ ,$\!$ (Not),  $\|$ (OR), $\&$ (And),  $==$ (comparar)

\item Operadores aritméticos: $+$, $-$, $*$, \verb|^| potencia,  \verb|%%| resto de la división entera,  \verb|%/%| división entera.

\item Logaritmos y exponenciales: \verb|log| logaritmo natural,  \verb|log(x,b)| ($log_{b}x$) y \verb|exp(x)| ($e^x$).

\item Funciones trigonométricas\verb|cos(x)|,\verb|sin(x)|, \verb|tan(x)|, \verb|acos(x)|, \verb|asin(x)|, \verb|atan(x)|.

\item Funciones misceláneas \verb|abs(x)|,  \verb|sqrt(x)|,  \verb|floor(x)|,  \verb|ceiling(x)|, \verb|max(x)|,  \verb|sign(x)|.

\item Comando \verb|options(digits=k)|:

\begin{verbatim}
> 1/3.0
[1] 0.3333333
> options(digits=3)
> 1/3
[1] 0.333
> 1/17.0
[1] 0.0588
> options(digits=3)
> 1/17.0
[1] 0.0588
> options(digits=5)
> 1/17.0
[1] 0.058824
> options(digits=9)
> 1/17.0
[1] 0.0588235294
\end{verbatim}

\item Comando \verb|round(x,n)| redondea $x$ a $n$ decimales, el valor por defecto es $n=6$.

\item Comando \verb|cat('caracter1','caracter2')| concatena dos cadenas o valores y el resultado lo convierte a un objeto tipo \textit{string}.

\end{itemize}

%-------------------------------------------
\subsection{Funciones}
%-------------------------------------------

\begin{verbatim}
nombrefun = function(a1,a2,...,an) {
# código ...
instruccion-1
instruccion-2
# ...
return( ... ) #valor que retorna (o también la última instrucción, si ésta retorna algo)
}
\end{verbatim}

\subsection{Clase en Laboratorio de Cómputo}

\begin{Ejer}
\begin{enumerate}
\item Generar un archivo tipo Rmd, personalizar de manera básica
\item Realizar las siguientes operaciones

\begin{verbatim}
x = c(1.1, 1.2, 1.3, 1.4, 1.5)
x = 1:5 
x = seq(1,3, by =0.5) 
x = seq(5,1, by =-1) 
x = rep(1, times = 5) 
length(x) 
rep(x, 2) 
set.seed(123) 
x = sample(1:10, 5) 
\end{verbatim}

\item 
\begin{verbatim}
x = 1:5 
y = rep(0.5, times = length(x)) 
x+y
x*y
x^y
1/(1:5)
\end{verbatim}

\item 
\begin{verbatim}
x = 1:5
2*x
1/x^2
x+3

\end{verbatim}

\item 
\begin{verbatim}
A = matrix(rep(0,9), nrow = 3, ncol= 3); 
B = matrix(c(1,2,3,5,6,7), nrow = 2, byrow=T); 
x = 1:3; y = seq(1,2, by = 0.5); z = rep(8, 3) ; x; y; z
C = matrix(c(x,y,z), nrow = length(x)); C # ncol no es necesario declararlo
xi = seq(1,2, by 0.1); yi = seq(5,10, by = 0.5)
rbind(xi,yi)
cbind(xi,yi)
\end{verbatim}

\item 
\begin{verbatim}

A = diag(c(3,1,3)); 
diag(A)
n = 3
I = diag(1, n);
D = diag(diag(A))
J = diag(1, 3, 4);
\end{verbatim}

\item 
\begin{verbatim}

B = matrix(c( 1, 1 ,8,
              2, 0, 8,
              3, 2, 8), nrow = 3, byrow=TRUE); B

B[2, 3]
B[3,]
B[,2]
B[1:2,c(2,3)]

\end{verbatim}

\item 
\begin{verbatim}
A = matrix(c( 1, 1 ,8,
              2, 0, 8,
              3, 2, 8), nrow = 3, byrow=TRUE); A
A[c(1,3), ] = A[c(3,1), ] 
A[2, ] = A[2, ] - A[2,1]/A[1,1]*A[1, ]

\end{verbatim}

\item 
\begin{verbatim}
x = c(2, -6, 7, 8, 0.1,-8.5, 3, -7, 3)
which.max(x)
which.max(abs(x)) 

\end{verbatim}

\item 
\begin{verbatim}

A = matrix(1:9, nrow=3); A # Por columnas
B = matrix(rep(1,9), nrow=3); B

A+B
A*B
A%*%B
A^2
A-2
3*A
t(A)
det(A)
C <- A-diag(1,3); det(C)
\end{verbatim}

\item 
\begin{verbatim}


notas = matrix(c(80, 40, 70, 30, 90, 67, 90,
                 40, 40, 30, 90, 100, 67, 90,
                 100,100,100, 100, 70, 76, 95), nrow=3, byrow=TRUE); notas
# crear columa con la suma de los renglones
#agregar la columna al final de la matriz
\end{verbatim}

\item 
\begin{verbatim}

u=c(1,2)
v=c(-2,3)
w=c(3,-5)
norma=function(u){sqrt(sum(u^2))}
\end{verbatim}

\item 
\begin{verbatim}

t(u-2*v)%*%w
norma(u+v+w)
norma(u)+norma(v)+norma(w)
t(u-v)%*%(v-w)

\end{verbatim}

\item 
\begin{verbatim}

u=c(8,3);a=c(4,-5)
ProyOrto=function(u,a){(t(u)%*%a)*a/norma(a)}
ProyOrto(u,a)
ProyOrto(c(2,1,-4),c(-5,3,11))

\end{verbatim}

\item 
\begin{verbatim}

A=matrix(c(1,0,0,1/3,4,0,1/2,3,2),ncol=3,byrow=TRUE)
B=matrix(c(9,0,0,0,1,8,0,0,0,-2,7,0,0,0,-3,6),ncol=4)
solve(A)  # Inversa de A
det(A)
solve(B);det(B)

\end{verbatim}

\item 
\begin{verbatim}

A=matrix(c(4,4.001,4.001,4.002),ncol=2)
B=A;B[2,2]=4.002001
solve(A)
solve(B)

\end{verbatim}

\item 
\begin{verbatim}

X=matrix(runif(12),ncol=3)
u=runif(4)
A=t(X)%*%X
B=u%*%t(u)
DA=eigen(A)$values
TA=eigen(A)$vectors
t(TA)%*%TA
prod(DA);det(A)
qr(A)$rank  # Rango de una matriz
sum(diag(DA)!=0)
\end{verbatim}

\item 
\begin{verbatim}

DB=eigen(B)$values
TB=eigen(B)$vectors
t(TB)%*%TB
prod(DB);det(B)
qr(B)$rank
sum(diag(DB)!=0)

\end{verbatim}

\item 
\begin{verbatim}

A=matrix(c(3,2,0,2,3,0,0,0,3),ncol=3)
eig_A=eigen(A)
eig_A$values
eigen(A%*%A)$values
eigen(solve(A))$values
eig_A$vectors%*%diag(eigen(A%*%A)$values)%*%t(eig_A$vectors);A%*%A

\end{verbatim}

\item 
\begin{verbatim}

A=matrix(c(6,10,1,10,6,5),ncol=2)
ginvMP=function(A){
        res=svd(A)
        res$v%*%diag(1/res$d)%*%t(res$u)}
B=ginvMP(A)
A%*%B%*%A

\end{verbatim}

\item 
\begin{verbatim}

es.positiva=function(A){
 if (ncol(A)!=nrow(A)) stop("Esto se hace para matrices cuadradas")
 v=eigen(A)$values
 tol=ncol(A)*max(abs(v))* .Machine$double.eps
 if (sum(v>tol)==length(v)) return(TRUE) else return(FALSE)}
 
A=matrix(c(2,-3/2,-3/2,3),ncol=2);es.positiva(A)
A=matrix(c(1,0.8,0.5,0.8,0.6,0.4,0.5,0.4,0.25),ncol=3);es.positiva(A)

\end{verbatim}

\item 
\begin{verbatim}

matrixA=function(m){
A=matrix(c(1,-2,-2,m),ncol=2)
return(A)}

dmatrixA=function(m){det(matrixA(m))}

m=seq(-4,10,len=101)
plot(m,mapply(dmatrixA,m=m),type="l") # Dibuja el determinante en funci?n de m
abline(h=0)
A=matrixA(-2)
print(z<-eigen(A))


\end{verbatim}

\item 
\begin{verbatim}

A=cbind(c(3,1,0),c(1,3,0),c(0,0,3))
B=matrix(0,ncol=3,nrow=3);B[3,3]=2
eigen(A)
eigen(B)

eigen(A+B) # Trampilla

\end{verbatim}

\end{enumerate}



\end{Ejer}



\paragraph{Sesión 1}


\subsection{Operadores lógicos}

\begin{verbatim}
17<5
17>5
17<=5
17>=5
17!=5
17==5
\end{verbatim}

\subsection{OPERADORES ARITMETICOS}

\subsubsection{SUMA, RESTA, MULTIPLICACION, DIVISION, POTENCIA, MODULO, DIVISION ENTERA}
\begin{verbatim}
17+5
17*5
17*5
17^5
17%/%5
17%%5
\end{verbatim}

\subsubsection{LOGARITMOS Y EXPONENCIALES}
\begin{verbatim}
log(1)
log(12)
log(12,2)
exp(12)
exp(1)
\end{verbatim}
\subsubsection{FUNCIONES TRIGONOMETRICAS}
\begin{verbatim}
sin(45)
cos(45)
tan(45)
asin(0.96)
acos(0.97)
atan(0.45)
\end{verbatim}
\subsubsection{FUNCIONES VARIAS}
\begin{verbatim}
abs(-34)
sqrt(8)
floor(1.56)
ceiling(1.56)
max(4,7,2,12)
min(4,7,2,12)
sign(-45)
\end{verbatim}
\subsubsection{EJERCICIOS DE PRACTICA}
\begin{verbatim}
# calcular la expresion cos(pi/6+pi/2)+e^2
# calcular la expresion cos(pi/6+pi/2)+e^2*log(5)+arc cos(1/raiz(2))
# introducir las siguientes expresiones: 
# a) 1/7
# b) options(digits=3); 1/7
# c) options(digits=6); 1/7
# d) round(67.45)
# e) round(75.324568,2)
# f) options(digits=7);
# g) signif(56.345458234234,2)
# h) signif(56.345458234234)
# i) exp(-30)
# j) options(scipen= 999)
# k) exp(-30)
# l) options(scipen=0)
\end{verbatim}

\paragraph{Sesión 2}

\subsection{EJERCICIOS DE PRACTICA}

\subsubsection{DEFINICION DE CONSTANTES}
\begin{verbatim}
e = exp(1); 
x = 0.0034
e <- exp(1)
x <- 0.034;
x0 = e^(2*x)
\end{verbatim}
\subsubsection{CONCATENAR Y PEGAR EXPRESIONES}
\begin{verbatim}
txt = "El valor de x0 es _"
cat(txt, x0)
paste(txt,x0)
paste0(txt,x0)
\end{verbatim}
\subsubsection{ASIGNACION E IMPRESION}
\begin{verbatim}
x0 <- 1
x1 <- x0 - pi*x0 + 1 
(x1 <- x0 - pi*x0 + 1 ) 
print(x1)
\end{verbatim}
\subsubsection{LISTADO DE OBJETOS DEFINIDOS}
\begin{verbatim}
ls()
# Eliminar todos los objetos
rm(list= ls())
ls()
\end{verbatim}
\subsubsection{IMPRIMIR PEGAR AVANZADO}
\begin{verbatim}
x0 <- 1
x1 <- x0 - pi*x0 + 1
cat("x0 =", x0, "\n","x1 =", x1) 
\end{verbatim}
\subsubsection{EJERCICIOS DE PRACTICA}



\paragraph{Sesión 3}


\subsubsection{DEFINICION DE FUNCIONES}
\begin{verbatim}
# nombre_funcion <- function(param1,param2,param3,...,paramn){
# instruccion 1
# instruccion 2
# return(valor_de_retorno)
#}
\end{verbatim}

\subsubsection{Ejemplo 1}
\begin{verbatim}
fun1 <- function(x,a,b,h,k){
  res <- a+b*cos(hx+k)
  return(res)
}
\end{verbatim}
\subsubsection{Ejemplo 2}
\begin{verbatim}
Discriminante <- function(a,b,c){
  res <- b^2-4*a*c
  return(res)
}
\end{verbatim}
\subsubsection{GRAFICAS}
\begin{verbatim}
fun2 <- function(x,h,k){
  res <- 1/h*sin(k*x)
  return(res)
}

f2 <- fun2(1:100,2,3)
plot(f2,type="l", col= "red", lwd=2,
     main= "Grafico de la funcion f2",
     xlab= "x",
     ylab="f(x)=1/h*sin(k*x)",
     axes= TRUE)
\end{verbatim}
\subsubsection{EJEMPLOS DE PRACTICA}
Graficar: rectas, parabolas, cubicas, polinomios, exponenciales, logaritmos


\subsection{Introducción a Factorización LU}


\subsubsection{Inversa de una matriz}
\begin{verbatim}

```{r}
A=matrix(c(1,3,-2,1,1,4,-1,2,0,1,-1,4,2,6,1,2),nrow = 4,byrow = TRUE)
(A)
Aorig<- A
```
\end{verbatim}
Paso 1)

\begin{verbatim}

```{r}
I = diag(4);(I)
```
\end{verbatim}

Paso 2)

\begin{verbatim}

```{r}
AInv <- cbind(A,I); (AInv)
A <- AInv
```
\end{verbatim}

Paso 3)

\begin{verbatim}
```{r}
l21 <- A[2,1]/A[1,1];(l21)
l31 <- A[3,1]/A[1,1];(l31)
l41 <- A[4,1]/A[1,1];(l41)
```
\end{verbatim}

Paso 4)

\begin{verbatim}
```{r}
A[2,] <- A[2,]-l21*A[1,]; 
A[3,] <- A[3,]-l31*A[1,]; 
A[4,] <- A[4,]-l41*A[1,]; 
(A)
```
\end{verbatim}

Paso 5)
\begin{verbatim}
```{r}
Atmp <- A[2,]
A[2,] <- A[3,]
A[3,] <- Atmp
(A)
```
\end{verbatim}

Paso 6)

\begin{verbatim}
```{r}
l32 <- A[3,2]/A[2,2];(l32)
l42 <- A[4,2]/A[2,2];(l42)
```

\end{verbatim}

Paso 7)

\begin{verbatim}
```{r}
A[3,] <- A[3,]-l32*A[2,]; 
A[4,] <- A[4,]-l42*A[2,]; 
(A)
```

\end{verbatim}

Paso 8)

\begin{verbatim}
```{r}
esc <- 1/A[3,3]
A[3,] <- esc*A[3,]
(A)
```

\end{verbatim}

Paso 9)

\begin{verbatim}
```{r}
l43 <- A[4,3]/A[3,3];(l43)
```

\end{verbatim}

Paso 10)

\begin{verbatim}
```{r}
A[4,] <- A[4,]-l43*A[3,];
(A)
```

\end{verbatim}

Paso 11)

\begin{verbatim}
```{r}
esc <- 1/A[4,4]
A[4,] <- esc*A[4,]
(A)
```
\end{verbatim}

Paso 12)

\begin{verbatim}
```{r}
l34 <- A[3,4]/A[4,4];(l34); A[3,] <- A[3,]-l34*A[4,]
l24 <- A[2,4]/A[4,4];(l24); A[2,] <- A[2,]-l24*A[4,]
l14 <- A[1,4]/A[4,4];(l14); A[1,] <- A[1,]-l14*A[4,]
(A)
```

\end{verbatim}

Paso 13)

\begin{verbatim}
```{r}
l23 <- A[2,3]/A[3,3];(l23); A[2,] <- A[2,]-l23*A[3,]
l13 <- A[1,3]/A[3,3];(l13); A[1,] <- A[1,]-l13*A[3,]
(A)

```

\end{verbatim}

Paso 14)

\begin{verbatim}
```{r}
l12 <- A[1,2]/A[2,2];(l12); A[1,] <- A[1,]-l12*A[2,]
(A)
```

\end{verbatim}

Paso 15)
\begin{verbatim}
```{r}
AInv <- A[,5:8]; (AInv)
```

\end{verbatim}

Paso 16)
\begin{verbatim}
```{r}
IdCalc <- Aorig%*%AInv; (IdCalc)
```
\end{verbatim}


\subsubsection{Factorización LU}

\begin{Ejem}

\begin{verbatim}
```{r}
A=matrix(c(1,1,1,1,2,3,1,5,-1,1,-5,3,3,1,7,-2),byrow = TRUE,nrow = 4); (A)
```

```{r}
b=matrix(c(10,31,-2,18),nrow = 4);(b)
```

```{r}
Ab <- cbind(A,b); (Ab)
```
\end{verbatim}

Los pivotes se definen como $a_{kk}^{(k)}$, luego construimos los multiplicadores: $l_{i,k}=a_{ik}^{(k)}/a_{kk}^{(k)}$, $l_{21}=a_{21}/a_{aa}$ y $l_{31}=a_{31}/a_{11}$ y $l_{41}=a_{41}/a_{11}$

\begin{verbatim}
```{r}
A <- Ab;
l21 <- A[2,1]/A[1,1];(l21)
l31 <- A[3,1]/A[1,1];(l31)
l41 <- A[4,1]/A[1,1];(l41)
```
\end{verbatim}

Construimos la matriz $U$, donde las entradas $a_{ij}^{(k+1)}=a_{ik}^{(k)}-l_{ik}\times a_{kj}^{(k)}$

\begin{verbatim}
```{r}
A[2,] <- A[2,]-l21*A[1,]; (A[2,])
A[3,] <- A[3,]-l31*A[1,]; (A[3,])
A[4,] <- A[4,]-l41*A[1,]; (A[4,])
```
\end{verbatim}

Es decir la matriz resultante es:

\begin{verbatim}
```{r}
(A)
```
\end{verbatim}

entonces el pivote es $A_{22} = 1$, calculemos los $l_{32}$ y $l_{42}$

\begin{verbatim}
```{r}
l32 <- A[3,2]/A[2,2];(l32)
l42 <- A[4,2]/A[2,2];(l42)
```
\end{verbatim}

Haciendo cero debajo del pivote

\begin{verbatim}
```{r}
A[3,] <- A[3,]-l32*A[2,];(A[3,])
A[4,] <- A[4,]-l42*A[2,];(A[4,])
```
\end{verbatim}

La matriz $A$ queda de la forma:

\begin{verbatim}
```{r}
(A)
```
\end{verbatim}

por tanto el pivote es $A_{33}=-2$, y resta calcylar $l_{43}$

\begin{verbatim}
```{r}
l43 <- A[4,3]/A[3,3];(l43)
```
\end{verbatim}

haciendo cero debajo del pivote

\begin{verbatim}
```{r}
A[4,] <- A[4,]-l43*A[3,];(A[4,])
```
\end{verbatim}

Por lo tanto la matriz $A$ resultante es

\begin{verbatim}
```{r}
(A)
```
\end{verbatim}

entonces podemos construir la matriz $L$ con los valores $l_{ij}$ calculados en los pasos anteriores

\begin{verbatim}
```{r}
L=diag(4);(L)
```

```{r}
L[2,1] <- l21
L[3,1] <- l31
L[4,1] <- l41
L[3,2] <- l32
L[4,2] <- l42
L[4,3] <- l43
(L)
```

```{r}
U <- A[,1:4];(U)
```
\end{verbatim}

Verifiquemos que efectivamente $A=LU$
\begin{verbatim}
```{r}
Acalculada <- L%*%U; (Acalculada)
```
\end{verbatim}
\end{Ejem}


\begin{Ejem}

Apliquemos lo desarrollado para la siguiente matriz $A$
\begin{verbatim}
```{r}
A=matrix(c(4,0,1,1,3,1,3,1,0,1,2,0,3,2,4,1),nrow = 4,byrow = TRUE); (A)
```

```{r}
b=matrix(c(5,6,13,1),nrow = 4);(b)
```
\end{verbatim}

Paso 1)

\begin{verbatim}
```{r}
Ab <- cbind(A,b)
A <- Ab;
```

\end{verbatim}

Paso 2) 

\begin{verbatim}
```{r}
l21 <- A[2,1]/A[1,1];(l21)
l31 <- A[3,1]/A[1,1];(l31)
l41 <- A[4,1]/A[1,1];(l41)
```

\end{verbatim}

Paso 3)
\begin{verbatim}
```{r}
A[2,] <- A[2,]-l21*A[1,]; (A[2,])
A[3,] <- A[3,]-l31*A[1,]; (A[3,])
A[4,] <- A[4,]-l41*A[1,]; (A[4,])
```

\end{verbatim}

Paso 4)
\begin{verbatim}
```{r}
l32 <- A[3,2]/A[2,2];(l32)
l42 <- A[4,2]/A[2,2];(l42)
```

\end{verbatim}

Paso 5)
\begin{verbatim}
```{r}
A[3,] <- A[3,]-l32*A[2,];(A[3,])
A[4,] <- A[4,]-l42*A[2,];(A[4,])
```

\end{verbatim}

Paso 6)
\begin{verbatim}
```{r}
l43 <- A[4,3]/A[3,3];(l43)
```

\end{verbatim}

Paso 7)
\begin{verbatim}
```{r}
A[4,] <- A[4,]-l43*A[3,];(A[4,])
```

\end{verbatim}

Paso 8)
\begin{verbatim}
```{r}
L=diag(4);(L)
```

\end{verbatim}

Paso 9)
\begin{verbatim}
```{r}
L[2,1] <- l21
L[3,1] <- l31
L[4,1] <- l41
L[3,2] <- l32
L[4,2] <- l42
L[4,3] <- l43; 
(L)
```

\end{verbatim}

Paso 10)
\begin{verbatim}
```{r}
U <- A[,1:4];(U)
```

\end{verbatim}

Paso 11)
\begin{verbatim}
```{r}
Acalculada <- L%*%U; (Acalculada)
```
\end{verbatim}

\end{Ejem}

\begin{Ejer}

Aplicar la factorización $LU$ a la matriz A definida por
\begin{verbatim}
```{r}
A=matrix(c(2,3,2,4,
           4,10,-4,0,
           -3,-2,-5,-2,
           -2,4,4,-7),nrow = 4,byrow = TRUE)
(A)
```
\end{verbatim}
\end{Ejer}

\subsubsection{Notas importantes}

\begin{Note}
Si se fija $l_{ii}=1$ se le denomina factorizacion \textit{Doolitle}.
\end{Note}

\begin{Note}
**Nota 2** Una matriz $A$ tiene factorización de Doolitle si y sólo sí se le puede aplicar el metodo de eliminación de Gauss sin pivoteo.
\end{Note}

\begin{Note}
Si $U=L^{T}$ entonces la factorización se le denomina \textit{factorización de Cholesky}.
\end{Note}

\begin{Note}
Si la matriz es simétrica y definida positiva, entonces $A=LL^{T}$.
\end{Note}

\subsection{M\'etodos de Eliminaci\'on directa}

\subsubsection{Gaussiana Simple}


\begin{verbatim}

```{r,echo=FALSE}
gauss_simple <- function(A, b, tolerancia, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  if (!is.numeric(b)) stop("b debe ser numérico.")
  n <- nrow(A)
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (length(b) != n) stop("Longitud de b debe ser igual al número de filas de A.")
  Ab <- cbind(A, b);  
  numcols = ncol(Ab)
  for (k in 1:(n - 1)) {
    pivote <- Ab[k, k]
    if (abs(pivote) < tolerancia) {
      stop(sprintf("Pivote casi cero en fila %d (%.3e). 
      El método sin pivoteo falla.", k, pivote))
    }
    if (k + 1 <= n) {
      for (i in (k + 1):n) {
        m <- Ab[i, k] / pivote;
        Ab[i, k:numcols] <- Ab[i, k:numcols] - m * Ab[k, k:numcols]
        if (abs(Ab[i, k]) < tolerancia) Ab[i, k] <- 0
        if (verbose) {
        cat(sprintf("k=%d, i=%d, m=%.6g\n", k, i, m));
        print(Ab)}
      }
    }
  }
  x <- numeric(n)
  for (i in n:1) {
    if (i == n) {
      suma <- 0
    } else {suma <- sum(Ab[i, (i + 1):n] * x[(i + 1):n])}
    x[i] <- (Ab[i, n + 1] - suma) / Ab[i, i]
  }
  list(x=x,U=Ab[, 1:n],Ab=Ab)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}

```{r,echo=FALSE}
A <- matrix(c(
  2,  1, -1,
 -3, -1,  2,
 -2,  1,  2
), nrow = 3, byrow = TRUE)
b <- c(8, -11, -3);
tolerancia <- 1e-12;
res <- gauss_simple(A, b,tolerancia);
res <- gauss_simple(A, b,tolerancia, verbose = TRUE)
res$x;res$U;res$Ab;A %*% res$x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Gaussiana con pivoteo parcial}

\begin{verbatim}

```{r}
gauss_piv_parcial <- function(A, b, tolerancia, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); 
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (length(b) != n) stop("Dimensiones de b incorrectas.")
  Ab <- cbind(A, b)
  for (k in 1:(n-1)) {
    # Selección del pivote (máximo en valor absoluto en la 
    #columna k desde la fila k)
    max_row <- which.max(abs(Ab[k:n, k])) + (k - 1)
    if (abs(Ab[max_row, k]) < tolerancia){
    stop(sprintf("Pivote casi nulo en columna %d", k))}
    # Intercambio de filas si es necesario
    if (max_row != k) {
      Ab[c(k, max_row), ] <- Ab[c(max_row, k), ]
      if (verbose) {
      cat(sprintf("Intercambio de fila %d con fila %d\n", k, max_row));
      print(Ab)}
    }
    for (i in (k + 1):n) {
      m <- Ab[i, k] / Ab[k, k];
      Ab[i, k:ncol(Ab)] <- Ab[i, k:ncol(Ab)] - m * Ab[k, k:ncol(Ab)]
      if (abs(Ab[i, k]) < tolerancia) Ab[i, k] <- 0
      if (verbose) {
      cat(sprintf("k=%d, i=%d, m=%.6g\n", k, i, m));print(Ab)}
    }
  }
  x <- numeric(n)
  for (i in n:1) {
    if (i == n) {
      suma <- 0
    } else {
    suma <- sum(Ab[i, (i + 1):n] * x[(i + 1):n])
    }
    x[i] <- (Ab[i, n + 1] - suma) / Ab[i, i]
  }
  list(x = x,U = Ab[, 1:n],Ab = Ab)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}

```{r,echo=FALSE}
A <- matrix(c(
  0, 2, 1,
  1, -2, -3,
  -1, 1, 2), nrow = 3, byrow = TRUE)
b <- c(3, -3, -1);
tolerancia <- 1e-12
res <- gauss_piv_parcial(A, b, tolerancia, verbose = TRUE);
res$x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Gauss Jordan}

\begin{verbatim}
```{r}
gauss_jordan <- function(A, b, tolerancia, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); 
  m <- ncol(A)
  if (!is.null(b) && length(b) != n) 
  stop("Dimensiones incompatibles entre A y b.")
  Ab <- cbind(A, as.matrix(b)); 
   ncols <- ncol(Ab); row <- 1
  for (col in 1:m) {
    if (row > n) break
    pivot_row_rel <- which.max(abs(Ab[row:n, col]))
    pivot_row <- row + pivot_row_rel - 1
    if (abs(Ab[pivot_row, col]) < tolerancia) {
      if (verbose) cat(sprintf("Sin pivote usable en col=%d (|piv <tol). Se omite columna.\n", col))
      next
    }
    if (pivot_row != row) {
      Ab[c(row, pivot_row), ] <- Ab[c(pivot_row, row), ]
      if (verbose) {
      cat(sprintf("Swap filas %d <-> %d (col=%d)\n", row,
      pivot_row, col));
      print(Ab)}
    }
    pivote <- Ab[row, col];    
    Ab[row, ] <- Ab[row, ] / pivote
    if (verbose) {
    cat(sprintf("Normaliza fila %d por pivote %.6g (col=%d)\n", 
    row, pivote, col));
    print(Ab)}
    for (r in 1:n) {
      if (r == row) next
      factor <- Ab[r, col]
      if (abs(factor) > tolerancia) {
        Ab[r, ] <- Ab[r, ] - factor * Ab[row, ]
        if (verbose) {
        cat(sprintf("R%d := R%d - (%.6g)*R%d (col=%d)\n", 
        r, r, factor, row, col));print(Ab)}
      }
    }
    row <- row + 1
  }
  Ab[abs(Ab) < tolerancia] <- 0;  
  out <- list(RREF = Ab);  
  X <- Ab[, (m + 1):ncols, drop = FALSE]
  out$x <- if (ncol(X) == 1) as.vector(X) else X
  return(out)
}
```
\end{verbatim}

\begin{Ejem} 
\begin{verbatim}
```{r,echo=FALSE}
A <- matrix(c(
  2,  1, -1,
 -3, -1,  2,
 -2,  1,  2), nrow = 3, byrow = TRUE)
b <- c(8, -11, -3);
tolerancia <- 1e-12;
res <- gauss_jordan(A, b, tolerancia, verbose = TRUE);
res$x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Cálculo de Inversa}
\begin{verbatim}
```{r}
gauss_jordan_inversa <- function(A,tolerancia, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); m <- ncol(A)
  if (n != m) stop("Para invertir, A debe ser cuadrada.")
  Ab <- cbind(A, diag(n));
  ncols <- ncol(Ab);
  row <- 1
  for (col in 1:m) {
    if (row > n) break
    pivot_row_rel <- which.max(abs(Ab[row:n, col]))
    pivot_row <- row + pivot_row_rel - 1
    if (abs(Ab[pivot_row, col]) < tolerancia) {
      stop(sprintf("No hay pivote en la columna %d (|piv|<tol).", 
      col))
    }
    if (pivot_row != row) {
      Ab[c(row, pivot_row), ] <- Ab[c(pivot_row, row), ]
      if (verbose) {
      cat(sprintf("Intercambia filas %d <-> %d (col=%d)\n", 
      row, pivot_row, col)); 
      print(Ab)
      }
    }
    pivote <- Ab[row, col];
    Ab[row, ] <- Ab[row, ] / pivote
    if (verbose) {
    cat(sprintf("Normaliza fila %d por pivote %.6g (col=%d)\n",
    row, pivote, col));
    print(Ab)}
    for (r in 1:n) {
      if (r == row) next
      factor <- Ab[r, col]
      if (abs(factor) > tolerancia) {
        Ab[r, ] <- Ab[r, ] - factor * Ab[row, ]
        if (verbose){
        cat(sprintf("R%d := R%d - (%.6g)*R%d (col=%d)\n", 
        r, r, factor, row, col)); 
        print(Ab)
        }
      }
    }
    row <- row + 1
  }
  Ab[abs(Ab) < tolerancia] <- 0;
  LHS <- Ab[, 1:m, drop = FALSE]
  if (!all(LHS == diag(n))) {
  stop("La matriz es singular o la tolerancia es muy estricta.")
  }
  Ainv <- Ab[, (m + 1):ncols, drop = FALSE]
  list(Ainv = Ainv, RREF = Ab)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
tolerancia <- 1e-12
A <- matrix(c(
  1, 2, 3,
  0, 1, 4,
  5, 6, 0), nrow = 3, byrow = TRUE)
res <- gauss_jordan_inversa(A, tolerancia,
 verbose = TRUE);
 res$Ainv;
 round(A %*% res$Ainv, 6)
```
\end{verbatim}
\end{Ejem}


\subsubsection{Factorización LU}

\subsubsection*{Sin Pivoteo}

\begin{verbatim}
```{r}
lu_simple <- function(A, tol = 1e-12, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); m <- ncol(A)
  if (n != m) stop("A debe ser cuadrada.")
  U <- A;  
  L <- diag(n)
  for (k in 1:(n - 1)) {
    piv <- U[k, k]
    if (abs(piv) < tol) {
    stop(sprintf("Pivote casi cero en k=%d. No se puede continuar.",
    k))
    }
    for (i in (k + 1):n) {
      m <- U[i, k] / piv;
      L[i, k] <- m;
      U[i, k:n] <- U[i, k:n] - m * U[k, k:n]
      if (verbose) {
      cat(sprintf("k=%d, i=%d, m=%.6g\n", k, i, m));
      print(U)
      }
    }
  }
  list(L = L, U = U)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
A <- matrix(c(
  2,  1, -1,
 -3, -1,  2,
 -2,  1,  2
), 3, 3, byrow = TRUE)
lu <- lu_simple(A);lu$L; lu$U
```
\end{verbatim}
\end{Ejem}

\subsubsection*{Con Pivoteo}
\begin{verbatim}

```{r}
lu_piv_parcial <- function(A, tol = 1e-12, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); m <- ncol(A); 
  if (n != m) stop("A debe ser cuadrada.")
  U <- A;
  L <- diag(n);
  P <- diag(n)
  for (k in 1:(n - 1)) {
    max_row <- which.max(abs(U[k:n, k])) + (k - 1)
    if (abs(U[max_row, k]) < tol) {
    stop(sprintf("Matriz singular (pivote ~ 0).", k))
    }
    if (max_row != k) {
      U[c(k, max_row), ] <- U[c(max_row, k), ];
      P[c(k, max_row), ] <- P[c(max_row, k), ]
      if (k > 1) {
      L[c(k, max_row), 1:(k - 1)] <- L[c(max_row, k), 1:(k - 1)]
      }
      if (verbose) {
      cat(sprintf("Intercambia filas %d <-> %d\n",
      k, max_row)); 
      print(U)
      }
    }
    for (i in (k + 1):n) {
      m <- U[i, k] / U[k, k];
      L[i, k] <- m;
      U[i, k:n] <- U[i, k:n] - m * U[k, k:n];
      if (verbose) {
      cat(sprintf("k=%d, i=%d, m=%.6g\n", k, i, m));
      print(U)
      }
    }
  }
  list(P = P, L = L, U = U)
}
```
\end{verbatim}


\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
A <- matrix(c(
  0,  2,  1,
  1, -2, -3,
 -1,  1,  2), 3, 3, byrow = TRUE)
lu <- lu_piv_parcial(A, verbose = FALSE);
lu$P;
lu$L; 
lu$U
```
\end{verbatim}
\end{Ejem}

\subsubsection*{Solucion vía LU}

\begin{verbatim}
```{r}
forward_sub <- function(L, b) {
  n <- nrow(L);  
  y <- numeric(n)
  for (i in 1:n) {
  y[i] <- (b[i] - sum(L[i, 1:(i - 1)] * y[1:(i - 1)]))
  }
  y
}
```


```{r}
# x en Ux = y
back_sub <- function(U, y, tol = 1e-12) {
  n <- nrow(U);  
  x <- numeric(n)
  for (i in n:1) {
    s <- if (i == n) 0 else sum(U[i, (i + 1):n] * x[(i + 1):n])
    if (abs(U[i, i]) < tol) stop(sprintf("Pivote ~0 en U[%d,%d].", 
    i, i))
    x[i] <- (y - s) / U[i, i]
  }
  x
}
```
\end{verbatim}


\subsubsection*{Resolucion sin pivoteo}

\begin{verbatim}
```{r}
solve_lu_simple <- function(A, b, tol = 1e-12) {
  lu <- lu_simple(A, tol = tol);
  y <- forward_sub(lu$L, b)
  x <- back_sub(lu$U, y, tol = tol)
  x
}
```
\end{verbatim}

\subsubsection{Resolucion con pivoteo}

\begin{verbatim}
```{r}
solve_lu_piv_parcial <- function(A, b, tol = 1e-12) {
  lu <- lu_piv_parcial(A, tol = tol);
  Pb <- lu$P %*% b
  y  <- forward_sub(lu$L, as.vector(Pb))
  x  <- back_sub(lu$U, y, tol = tol)
  x
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
A <- matrix(c(
  0,  2,  1,
  1, -2, -3,
 -1,  1,  2), 3, 3, byrow = TRUE)
b <- c(3, -3, -1); 
x <- solve_lu_piv_parcial(A, b);
x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Factorización de Cholesky}

\begin{verbatim}
```{r}
tolerancia <- 1e-12
cholesky_fact <- function(A,tolerancia) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A)
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (!all(abs(A - t(A)) < tolerancia)) stop("A debe ser simétrica.")
  L <- matrix(0, n, n)
  for (j in 1:n) {
    suma <- sum(L[j, 1:(j-1)]^2)
    val <- A[j, j] - suma
    if (val <= 0) stop("A no es definida positiva (falló Cholesky).")
    L[j, j] <- sqrt(val)
    if (j < n) {
      for (i in (j+1):n) {
        suma <- sum(L[i, 1:(j-1)] * L[j, 1:(j-1)])
        L[i, j] <- (A[i, j] - suma) / L[j, j]
      }
    }
    cat(sprintf("Paso j=%d\n", j))
    print(L)
  }
  return(L)
}
```
\end{verbatim}


\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
# Matriz simétrica definida positiva
A <- matrix(c(
  4,  12, -16,
 12,  37, -43,
-16, -43,  98), nrow = 3, byrow = TRUE)

L <- cholesky_fact(A,tolerancia)
L
```
\end{verbatim}
\end{Ejem}

\subsubsection{Solución vía Cholesky}

\begin{verbatim}
```{r}
tolerancia <- 1e-12
solve_cholesky <- function(A, b, tolerancia) {
  L <- cholesky_fact(A, tolerancia)
  y <- forward_sub(L, b)
  x <- back_sub(t(L), y, tolerancia)
  x
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}

```{r,echo=FALSE}
b <- c(1, 2, 3)
x <- solve_cholesky(A, b,tolerancia)
x
A %*% x   
```
\end{verbatim}
\end{Ejem}

\subsection{Métodos Iterativos}

\subsubsection{Gauss Jacobi}

\begin{verbatim}
```{r}
jacobi <- function(A, b, x0 = NULL, tol = 1e-8, maxiter = 1000) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A)
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (length(b) != n) stop("Dimensiones de b incompatibles.")
  if (is.null(x0)) x0 <- rep(0, n)
  x <- x0;  
  D <- diag(diag(A));  
  R <- A - D;
  for (k in 1:maxiter) {
    x_new <- (b - R %*% x) / diag(D)
    cat(sprintf("Iter %d: %s\n", k, 
    paste(round(x_new, 6), collapse = " ")))
    if (sqrt(sum((x_new - x)^2)) < tol) {
    return(list(x = as.vector(x_new), 
    iter = k, convergencia = TRUE))}
    x <- x_new
  }
  list(x = as.vector(x), 
  iter = maxiter, 
  convergencia = FALSE)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}


```{r,echo=FALSE}
A <- matrix(c(
  4, -1,  0,
 -1,  4, -1,
  0, -1,  3), 3, 3, byrow = TRUE)

b <- c(15, 10, 10)
res_jacobi <- jacobi(A, b, x0 = c(0, 0, 0), tol = 1e-8); 
res_jacobi$x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Gauss Seidel}

\begin{verbatim}
```{r}
gauss_seidel <- function(A, b, x0 = NULL, 
tol = 1e-8, maxiter = 1000, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A)
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (length(b) != n) stop("Dimensiones de b incompatibles.")
  x <- if (is.null(x0)) rep(0, n) else as.numeric(x0)
  if (length(x) != n) stop("Dimensión de x0 incompatible con A.")
  for (k in 1:maxiter) {
    x_old <- x
    for (i in 1:n) {
      aii <- A[i, i]
      if (abs(aii) < .Machine$double.eps) {
      stop(sprintf("A[%d,%d] = 0: Gauss–Seidel No se puede aplicar",
       i, i))}
      s1 <- if (i > 1) sum(A[i, 1:(i - 1)] * x[1:(i - 1)]) else 0
      s2 <- if (i < n) sum(A[i, (i + 1):n] * x_old[(i + 1):n]) else 0
      x[i] <- (b[i] - s1 - s2) / aii
    }
    dx  <- sqrt(sum((x - x_old)^2));
    res <- sqrt(sum((b - A %*% x)^2))
    cat(sprintf("Iter %4d |dx|=%.3e  |res|=%.3e   x=%s\n",
    k, dx, res, paste(round(x, 6), collapse=" ")))
    if (dx < tol && res < tol) {
      return(list(x = as.vector(x), iter = k,
      convergencia = TRUE,delta = dx, residuo = res))
    }
  }
  list(x = as.vector(x), iter = maxiter, convergencia = FALSE,
       delta = sqrt(sum((x - x)^2)), 
       residuo = sqrt(sum((b - A %*% x)^2)))
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}

```{r,echo=FALSE}
A <- matrix(c(
  4, -1,  0,
 -1,  4, -1,
  0, -1,  3
), 3, 3, byrow = TRUE)
b <- c(15, 10, 10)
res_gs <- gauss_seidel(A, b, x0 = c(0,0,0), tol = 1e-8);
res_gs$x
```
\end{verbatim}
\end{Ejem}



\subsection{Clases}

\subsubsection{M\'etodos Iterativos}

\subsubsection{Gauss-Jacobi}
\begin{equation*}
A =
\begin{pmatrix}
10 & -1 & 2 & 0\\
-1 & 11 & -1 & 3\\
2 & -1 & 10 & -1\\
0 & 3 & -1 & 8
\end{pmatrix}, \qquad
\mathbf{b} =
\begin{pmatrix}
6\\ 25\\ -11\\ 15
\end{pmatrix}, \qquad
\mathbf{x}^{\*} = (1,2,-1,1)^{T}.
\end{equation*}

Iteraci\'on 
\begin{eqnarray*}
x_1^{(k+1)} &=& \frac{6 + x_2^{(k)} - 2x_3^{(k)}}{10},\\
x_2^{(k+1)} &=& \frac{25 + x_1^{(k)} + x_3^{(k)} - 3x_4^{(k)}}{11},\\
x_3^{(k+1)} &=& \frac{-11 - 2x_1^{(k)} + x_2^{(k)} + x_4^{(k)}}{10},\\
x_4^{(k+1)} &=& \frac{15 - 3x_2^{(k)} + x_3^{(k)}}{8}.
\end{eqnarray*}


Tomamos $\mathbf{x}^{(0)}=(0,0,0,0)^\top$ y actualizamos cada $x_i^{(k+1)}$ usa sólo valores del paso $k$.

\paragraph{Iteración 1:}
\begin{eqnarray*}
x_1^{(1)}&=&\frac{6 + 0 - 20}{10}=0.6, \\
x_2^{(1)}&=&\frac{25 + 0+ 0 - 3\cdot0}{11}=\frac{25}{11}\approx 2.272727,\\
x_3^{(1)}&=&\frac{-11 - 2\cdot0 + 0 + 0}{10}=-1.1, \\
x_4^{(1)}&=&\frac{15 - 3\cdot0 + 0}{8}=\frac{15}{8}=1.875.
\end{eqnarray*}

\paragraph{Iteración 2} Con $\mathbf{x}^{(1)}=(0.6,\,2.272727,\,-1.1,\,1.875)$,
\begin{eqnarray*}
x_1^{(2)}&=&\frac{6 + 2.272727 - 2(-1.1)}{10}=\frac{10.472727}{10}=1.047273,\\
x_2^{(2)}&=&\frac{25 + 0.6 + (-1.1) - 3(1.875)}{11}=\frac{18.875}{11}\approx 1.715909,\\
x_3^{(2)}&=&\frac{-11 - 2(0.6) + 2.272727 + 1.875}{10}=\frac{-8.052273}{10}\approx -0.805227,\\
x_4^{(2)}&=&\frac{15 - 3(2.272727) + (-1.1)}{8}=\frac{7.081819}{8}\approx 0.885227.
\end{eqnarray*}

\paragraph{Iteración 3} Con $\mathbf{x}^{(2)}=(1.047273,\,1.715909,\,-0.805227,\,0.885227)$,
\begin{eqnarray*}
x_1^{(3)}&=&\frac{6 + 1.715909 - 2(-0.805227)}{10}
=\frac{9.326363}{10}=0.932636,\\
x_2^{(3)}&=&\frac{25 + 1.047273 + (-0.805227) - 3(0.885227)}{11}
=\frac{22.586365}{11}=2.053306,\\
x_3^{(3)}&=&\frac{-11 - 2(1.047273) + 1.715909 + 0.885227}{10}
=\frac{-10.493410}{10}=-1.049341,\\
x_4^{(3)}&=&\frac{15 - 3(1.715909) + (-0.805227)}{8}
=\frac{9.047046}{8}=1.130881.
\end{eqnarray*}

\paragraph{Iteración 4} Con $\mathbf{x}^{(3)}=(0.932636,\,2.053306,\,-1.049341,\,1.130881)$,
\begin{eqnarray*}
x_1^{(4)}&=&\frac{6 + 2.053306 - 2(-1.049341)}{10}
=\frac{10.151988}{10}=1.015199,\\
x_2^{(4)}&=&\frac{25 + 0.932636 + (-1.049341) - 3(1.130881)}{11}
=\frac{21.490652}{11}=1.953696,\\
x_3^{(4)}&=&\frac{-11 - 2(0.932636) + 2.053306 + 1.130881}{10}
=\frac{-9.681085}{10}=-0.968109,\\
x_4^{(4)}&=&\frac{15 - 3(2.053306) + (-1.049341)}{8}
=\frac{7.790741}{8}=0.973843.
\end{eqnarray*}

\paragraph{Iteración 5} Con $\mathbf{x}^{(4)}=(1.015199,\,1.953696,\,-0.968109,\,0.973843)$,
\begin{eqnarray*}
x_1^{(5)}&=&\frac{6 + 1.953696 - 2(-0.968109)}{10}
=\frac{9.889914}{10}=0.988991,\\
x_2^{(5)}&=&\frac{25 + 1.015199 + (-0.968109) - 3(0.973843)}{11}
=\frac{22.125561}{11}=2.011415,\\
x_3^{(5)}&=&\frac{-11 - 2(1.015199) + 1.953696 + 0.973843}{10}
=\frac{-10.102859}{10}=-1.010286,\\
x_4^{(5)}&=&\frac{15 - 3(1.953696) + (-0.968109)}{8}
=\frac{8.170803}{8}=1.021351.
\end{eqnarray*}

Tabla de resultados

\begin{center}
\begin{tabular}{c|rrrr}
%%\toprule
k & $x_1^{(k)}$ & $x_2^{(k)}$ & $x_3^{(k)}$ & $x_4^{(k)}$\\
%%\midrule
0 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\
1 & 0.6000 & 2.2727 & -1.1000 & 1.8750 \\
2 & 1.0473 & 1.7159 & -0.8052 & 0.8852 \\
3 & 0.9326 & 2.0533 & -1.0493 & 1.1309 \\
4 & 1.0152 & 1.9537 & -0.9681 & 0.9738 \\
5 & 0.9890 & 2.0114 & -1.0103 & 1.0214 \\
%%\bottomrule
\end{tabular}
\end{center}

Tabla de errores

\begin{center}
\begin{tabular}{c|cc}
%\toprule
k & $\|x^{(k)}-x^{*}|_\infty$ & $\|x^{(k)}-x^{(k-1)}\|_\infty$\\
%\midrule
0 & 2.0000 & -- \\
1 & 0.8750 & 2.2727 \\
2 & 0.2841 & 0.9898 \\
3 & 0.1309 & 0.3374 \\
4 & 0.0463 & 0.1570 \\
5 & 0.0214 & 0.0577 \\
%\bottomrule
\end{tabular}
\end{center}

Ejercicios:instrucciones: usa $x^{(0)}=\mathbf{0}$ y detén cuando $\|x^{(k+1)}-x^{(k)}\|_\infty<10^{-3}$. 
Comprueba dominancia diagonal, escribe las fórmulas de Jacobi e itera con tabla.

\begin{Ejer}
\begin{equation*}
\begin{pmatrix}
5 & 1\\
2 & 6
\end{pmatrix}
\begin{pmatrix}x_1\\x_2\end{pmatrix}
=
\begin{pmatrix}3\\-10\end{pmatrix}.
\end{equation*}
\end{Ejer}


\begin{Ejer}
\begin{equation*}
\underbrace{\begin{pmatrix}
9 & -1 & 0 & 0\\
-1 & 12 & -2 & 1\\
0 & -1 & 11 & -2\\
0 & 2 & -1 & 10
\end{pmatrix}}_{A_{4\times4}}\,
\underbrace{\begin{pmatrix}x_1\\x_2\\x_3\\x_4\end{pmatrix}}_{x}
=
\underbrace{\begin{pmatrix}7\\26\\-15\\15\end{pmatrix}}_{b},
\end{equation*}
\end{Ejer}


\begin{Ejer}

\begin{equation*}
\underbrace{\begin{pmatrix}
10 & -1 & 2 & 0 & 0\\
-1 & 12 & -2 & 3 & 0\\
2 & -1 & 11 & -2 & 1\\
0 & 3 & -1 & 10 & -2\\
0 & 0 & 2 & -1 & 9
\end{pmatrix}}_{A_{5\times5}}\,
\underbrace{\begin{pmatrix}x_1\\x_2\\x_3\\x_4\\x_5\end{pmatrix}}_{x}
=
\underbrace{\begin{pmatrix}15\\-17\\26\\-7\\13\end{pmatrix}}_{b},
\end{equation*}

\end{Ejer}

\subsubsection{Gauss-Seidel}

Considerar

\[
A =
\begin{pmatrix}
9 & -1 & 0 & 2\\
-1 & 10 & -2 & 1\\
0 & -1 & 8 & -1\\
2 & 1 & -1 & 11
\end{pmatrix},\quad
\mathbf{b} =
\begin{pmatrix}
10\\ 8\\ 6\\ 13
\end{pmatrix},\quad
\mathbf{x}^{*}=(1,1,1,1)^\top.
\]
$A$ es diagonalmente dominante por renglones, por lo que Gauss--Seidel converge.

Iteraci\'on general
Con ecuaciones por rengl\'on:
\[
\begin{aligned}
9x_1 - x_2 + 2x_4 &= 10,&
\Rightarrow\;& x_1 \;=\; \frac{10 + x_2 - 2x_4}{9},\\
- x_1 + 10x_2 - 2x_3 + x_4 &= 8,&
\Rightarrow\;& x_2 \;=\; \frac{8 + x_1 + 2x_3 - x_4}{10},\\
- x_2 + 8x_3 - x_4 &= 6,&
\Rightarrow\;& x_3 \;=\; \frac{6 + x_2 + x_4}{8},\\
2x_1 + x_2 - x_3 + 11x_4 &= 13,&
\Rightarrow\;& x_4 \;=\; \frac{13 - 2x_1 - x_2 + x_3}{11}.
\end{aligned}
\]
\textbf{Regla de GS:} cada $x_i^{(k+1)}$ usa los \emph{valores m\'as recientes disponibles} dentro del mismo paso $k+1$.


Partimos de $\mathbf{x}^{(0)}=(0,0,0,0)^\top$.

\paragraph{Iteraci\'on 1}
\[
\begin{aligned}
x_1^{(1)}&=\frac{10+0-20}{9}=1.111111,\\
x_2^{(1)}&=\frac{8+\underline{x_1^{(1)}}+20-0}{10}
=\frac{8+1.111111}{10}=0.911111,\\
x_3^{(1)}&=\frac{6+\underline{x_2^{(1)}}+0}{8}
=\frac{6+0.911111}{8}=0.863889,\\
x_4^{(1)}&=\frac{13-2\underline{x_1^{(1)}}-\underline{x_2^{(1)}}+\underline{x_3^{(1)}}}{11}
=\frac{13-2(1.111111)-0.911111+0.863889}{11}=0.975505.
\end{aligned}
\]

\paragraph{Iteraci\'on 2}
\[
\begin{aligned}
x_1^{(2)}&=\frac{10+x_2^{(1)}-2x_4^{(1)}}{9}
=\frac{10+0.911111-2(0.975505)}{9}=0.995567,\\
x_2^{(2)}&=\frac{8+\underline{x_1^{(2)}}+2x_3^{(1)}-x_4^{(1)}}{10}
=\frac{8+0.995567+2(0.863889)-0.975505}{10}=0.974784,\\
x_3^{(2)}&=\frac{6+\underline{x_2^{(2)}}+x_4^{(1)}}{8}
=\frac{6+0.974784+0.975505}{8}=0.993786,\\
x_4^{(2)}&=\frac{13-2\underline{x_1^{(2)}}-\underline{x_2^{(2)}}+\underline{x_3^{(2)}}}{11}
=\frac{13-2(0.995567)-0.974784+0.993786}{11}=1.002534.
\end{aligned}
\]

\paragraph{Iteraci\'on 3}
\[
\begin{aligned}
x_1^{(3)}&=\frac{10+0.974784-2(1.002534)}{9}=0.996635,\\
x_2^{(3)}&=\frac{8+\underline{0.996635}+2(0.993786)-1.002534}{10}=0.998167,\\
x_3^{(3)}&=\frac{6+\underline{0.998167}+1.002534}{8}=1.000088,\\
x_4^{(3)}&=\frac{13-2\underline{0.996635}-\underline{0.998167}+\underline{1.000088}}{11}=1.000786.
\end{aligned}
\]

\paragraph{Iteraci\'on 4}
\[
\begin{aligned}
x_1^{(4)}&=\frac{10+0.998167-2(1.000786)}{9}=0.999622,\\
x_2^{(4)}&=\frac{8+\underline{0.999622}+2(1.000088)-1.000786}{10}=0.999901,\\
x_3^{(4)}&=\frac{6+\underline{0.999901}+1.000786}{8}=1.000086,\\
x_4^{(4)}&=\frac{13-2\underline{0.999622}-\underline{0.999901}+\underline{1.000086}}{11}=1.000086.
\end{aligned}
\]

\paragraph{Iteraci\'on 5}
\[
\begin{aligned}
x_1^{(5)}&=\frac{10+0.999901-2(1.000086)}{9}=0.999970,\\
x_2^{(5)}&=\frac{8+\underline{0.999970}+2(1.000086)-1.000086}{10}=1.000006,\\
x_3^{(5)}&=\frac{6+\underline{1.000006}+1.000086}{8}=1.000011,\\
x_4^{(5)}&=\frac{13-2\underline{0.999970}-\underline{1.000006}+\underline{1.000011}}{11}=1.000006.
\end{aligned}
\]

Tabla de resultados

\begin{center}
\begin{tabular}{c|rrrr}
%\toprule
k & $x_1^{(k)}$ & $x_2^{(k)}$ & $x_3^{(k)}$ & $x_4^{(k)}$\\
%\midrule
0 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
1 & 1.111111 & 0.911111 & 0.863889 & 0.975505 \\
2 & 0.995567 & 0.974784 & 0.993786 & 1.002534 \\
3 & 0.996635 & 0.998167 & 1.000088 & 1.000786 \\
4 & 0.999622 & 0.999901 & 1.000086 & 1.000086 \\
5 & 0.999970 & 1.000006 & 1.000011 & 1.000006 \\
%\bottomrule
\end{tabular}
\end{center}

Tabla de errores

\begin{center}
\begin{tabular}{c|cc}
%\toprule
k & $\|x^{(k)}-x^{*}\|_\infty$ & $\|x^{(k)}-x^{(k-1)}\|_\infty$\\
%\midrule
0 & 1.000000 & -- \\
1 & 0.136111 & 1.111111 \\
2 & 0.025216 & 0.129897 \\
3 & 0.003365 & 0.023383 \\
4 & 0.000378 & 0.002986 \\
5 & 0.000030 & 0.000348 \\
%\bottomrule
\end{tabular}
\end{center}


Instrucciones: usa $x^{(0)}=\mathbf{0}$ y det\'en cuando $\|x^{(k+1)}-x^{(k)}\|_\infty<10^{-3}$. Escribe las f\'ormulas de GS e itera con tabla.


\begin{Ejer}
\[
\underbrace{\begin{pmatrix}
6 & -1\\
1 & 5
\end{pmatrix}}_{A_{2\times2}}
\begin{pmatrix}x_1\\x_2\end{pmatrix}
=
\underbrace{\begin{pmatrix}13\\-3\end{pmatrix}}_{b}.
\]

\end{Ejer}


\begin{Ejer}\[
\underbrace{\begin{pmatrix}
8 & 1 & -1 & 0\\
2 & 10 & -2 & 1\\
1 & -1 & 9 & -1\\
0 & 2 & -1 & 7
\end{pmatrix}}_{A_{4\times4}}
\begin{pmatrix}x_1\\x_2\\x_3\\x_4\end{pmatrix}
=
\underbrace{\begin{pmatrix}6\\-3\\20\\-9\end{pmatrix}}_{b}.
\]

\end{Ejer}


\begin{Ejer}
\[
\underbrace{\begin{pmatrix}
10 & -1 & 0 & 0 & 2\\
-1 & 11 & -1 & 0 & 0\\
0 & -1 & 12 & -2 & 1\\
0 & 0 & -2 & 9 & -1\\
2 & 0 & 1 & -1 & 8
\end{pmatrix}}_{A_{5\times5}}
\begin{pmatrix}x_1\\x_2\\x_3\\x_4\\x_5\end{pmatrix}
=
\underbrace{\begin{pmatrix}10\\21\\1\\-10\\11\end{pmatrix}}_{b}.
\]
\end{Ejer}





\begin{thebibliography}{9}

\bibitem{Isaacson} 
Isaacson, W. (2014). 
\textit{Los innovadores: Los genios que inventaron el futuro}. 
Debate.

\end{thebibliography}

\end{document}

