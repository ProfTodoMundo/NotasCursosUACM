%===========================================
\documentclass[12pt]{article}
%===========================================
\usepackage[utf8]{inputenc}
%\usepackage[margin=2.5in]{geometry}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{hyperref}
\usepackage[spanish]{babel}
\decimalpoint
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{graphicx,graphics}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{color}
\usepackage{float} 
\usepackage{subfig}
\usepackage[figuresright]{rotating}
\usepackage{enumerate}
\usepackage{anysize} 
\usepackage{url}
\usepackage{imakeidx}
\usepackage[left=0.5in, right=0.5in, top=1in, bottom=1in]{geometry}
% Opcional: para incluir gráficos con control de tamaño
\usepackage{float}

\hyphenation{mo-de-ra-da-men-te}
\addto\captionsspanish{\renewcommand{\figurename}{Figura}}

%===========================================
% Ajustes de Sweave
%\usepackage{Sweave}
%===========================================
\title{Notas sobre Métodos Numéricos con R}
\author{
Carlos E. Martínez-Rodríguez \\
Universidad Autónoma de la Ciudad de México \\
Academia de Matemáticas \\
\texttt{carlos.martinez@uacm.edu.mx}
}
\date{Agosto 2025}
\date{}
%===========================================
\newtheorem{Criterio}{Criterio}%[section]
\newtheorem{Sup}{Supuesto}[section]
\newtheorem{Note}{Nota}[section]
\newtheorem{Ejem}{Ejemplo}[section]
\newtheorem{Ejer}{Ejercicio}[section]
\newtheorem{Prop}{Proposici\'on}[section]
\newtheorem{Def}{Definici\'on}[section]
\newtheorem{Teo}{Teorema}[section]
\newtheorem{Algthm}{Algoritmo}[section]
\newtheorem{Sol}{Soluci\'on}[section]
\newtheorem{Ses}{Sesi\'on}[section]
%===========================================
\begin{document}
\maketitle
\tableofcontents

%===========================================
\section{Sobre el curso}
%===========================================

El curso se llevará a cabo tres veces a la semana,  con sesiones de 1.5 horas,  de las cuales una de ellas se realizará en el laboratorio de cómputo 3.

%---------------------------------------------------
\subsection{Requisitos para acreditar la materia}
%---------------------------------------------------

El curso por su naturaleza implica que la/el estudiante implemente los distintos algoritmos que se revisan en el curso,  por lo tanto es importante que demuestre que efectivamente puede implementar, revisar y mejorar los algoritmos ya existentes.\bigskip

Hay dos maneras de certificar la materia: a) Portafolio y b) Examen de certificación.  Al menos una semana antes de que termine el curso las y los estudiantes tendrán conocimiento de sus calificaciones parciales y por tanto de la calificación promedio obtenida al momento, para que sea el/la mismo(a) estudiante quién decida si certifica por la modalidad de portafolio,  o por la modalidad de examen de certificación.\bigskip


El portafolio se conforma de evaluaciones ($40\%$), programas, ($40\%$) tareas ($10\%$), tareitas ($5\%$)y trabajos adicionales ($5\%$). Mientras que la certificación es un examen elaborado por el comité de certificación y que será presentado por todas y todos los estudiantes que se inscriban en esta modalidad en las fechas establecidas por la Coordinación de Certificación y Registro.  En cualquiera de las dos modalidades es indispensable que el/la estudiante se registre a este proceso para que su calificación pueda ser asignada al final del proceso de Certificación.

%---------------------------------------------------
\subsection{De la naturaleza del curso}
%---------------------------------------------------

El curso tiene un fuerte sustento en la programación constante, sin embargo, es importante resaltar que los conceptos teóricos deben ser dominados por las y los estudiantes, por lo tanto las evaluaciones y las tareas tendrán estas dos componentes principales.  Al contrario de lo que pueda pensarse la asistencia al curso es obligatoria pero no influye directamente en la calificación obtenida.  Sin embargo,  hay que mencionar que si la asistencia se realiza de manera intermitente es probable que cueste un poco de trabajo reincorporarse a la dinámica de trabajo que se irá construyendo con el grupo con el transcurso de las clases. 

%===========================================
\section{Introducción}
%===========================================

En este curso estudiaremos los elementos básicos de los métodos numéricos, utilizando el programa de distribución libre \textit{R}. Antes de iniciar propiamente con el estudio de los métodos numéricos realizaremos un breve repaso de algunos conceptos de álgebra lineal, cálculo diferencial mismos que son fundamentales en esta materia y de la que se supone las y los estudiantes se encuentran familiarizados con ellos.\medskip

\textit{An\'alisis Num\'erico} es una rama de las matem\'aticas que, mediante el uso de algoritmos iterativos, obtiene soluciones num\'ericas a problemas en los cuales la matem\'atica simb\'olica (o anal\'itica) resulta poco eficiente o no puede ofrecer un resultado. En particular, a estos algoritmos se les denomina \textit{m\'etodos num\'ericos}.Por lo general los m\'etodos num\'ericos se componen de un n\'umero de pasos finitos que se ejecutan de manera l\'ogica, mejorando aproximaciones iniciales a cierta cantidad, tal como la ra\'iz de una ecuaci\'on, hasta que se cumple con cierta cota de error. A esta operaci\'on c\'iclica de mejora del valor se le conoce como \textit{iteraci\'on}. El an\'alisis num\'erico es una alternativa muy eficiente para la resoluci\'on de ecuaciones, tanto algebraicas (polinomios) como trascendentes teniendo una ventaja muy importante respecto a otro tipo de m\'etodos: La repetici\'on de instrucciones l\'ogicas (iteraciones), proceso que permite mejorar los valores inicialmente considerados como soluci\'on. Dado que se trata siempre de la misma operaci\'on l\'ogica, resulta muy pertinente el uso de recursos de c\'omputo para realizar esta tarea. Sin embargo, debe haber claridad en el sentido de que el an\'alisis num\'erico no es la panacea en la soluci\'on de problemas matem\'aticos; los m\'etodos num\'ericos arrojan \textit{aproximaciones}, es decir, est\'an sujetos a un error. Esto quiere decir que si se puede ser tan preciso como los recursos de c\'alculo lo permitan, siempre est\'a presente y debe considerarse su manejo en el desarrollo de las soluciones requeridas. El uso de diversos sistemas de c\'omputo determina qu\'e soluciones anal\'itico–num\'ericas son viables en la pr\'actica, lo que implica que se deben tomar en cuenta el proceso iterativo, el costo de los recursos f\'isicos que se emplean en el an\'alisis, y el tipo de pr\'actica de la Ingenier\'ia. 

%===========================================
\section{Revisión de temas importantes}
%===========================================

%-------------------------------------------
\subsection{Cálculo}
%-------------------------------------------

Comenzamos el capítulo con un repaso de algunos aspectos importantes del cálculo que son necesarios a lo largo del texto. Suponemos que los estudiantes que lean este texto conocen la terminología, la notación y los resultados que se dan en un curso típico de cálculo.

%...............................................................
\subsubsection{Límites y continuidad}
%...............................................................

El límite de una función nos dice qué tan cerca se encuentran las imágenes de una función si dos elementos de su dominio se encuentran lo suficientemente cerca, es decir, decimos que una función $f(x)$ definida en el intervalo $(a,b)$ tiene \textbf{límite} $L$ en el punto $x = x_0$, lo que denotamos por $$\lim_{x \to x_0} f(x) = L,$$ si para cualquier $\varepsilon > 0$, existe un número real $\delta > 0$ tal que $|f(x) - L| < \varepsilon$ siempre que $0 < |x - x_0| < \delta $. Es decir, que los valores de la función estarán cerca de $L$ siempre que $x$ esté suficientemente cerca de $x_0$.
\bigskip

Se dice que una función $f$ es continua en $a$ si cuando $x$ se aproxima al valor de $a$, entonces también $f(x)$ se aproxima a $f(a)$, es decir, $f(x)$ es \textbf{continua} en el punto $x = a $ si $$\lim_{x \to a} f(x) = f(a),$$
y se dice que $f$ es \textbf{continua en el conjunto} $(a,b)$ si es continua en cada uno de los puntos del intervalo. Denotaremos el conjunto de todas las funciones $f$ que son continuas en $E=(a,b)$ por $C(E)$. \bigskip

Se dice que una sucesión $\{x_n\}_{n=1}^\infty$ \textbf{converge} a un número $x$, si
$\lim_{n \to \infty} x_n = x$ o bien, $x_n \to x$ cuando $n \to \infty$, si para cualquier $\varepsilon > 0$, existe un número natural $N(\varepsilon)$ tal que $|x_n - x| < \varepsilon$ para cada $n > N(\varepsilon)$. Cuando una sucesión tiene límite, se dice que la \textbf{sucesión converge}.

%........................................................
\subsubsection{Continuidad y convergencia de sucesiones}
%........................................................

Si $f(x)$ es una función definida en un conjunto $S$ de números reales y $x_0 \in S$, entonces las siguientes afirmaciones son equivalentes:
\begin{enumerate}
\item $f(x) $ es continua en $x = x_0 $,
\item Si $\{x_n\}_{n=1}^\infty $ es cualquier sucesión en $S $ que converge a $x_0 $, entonces $\lim_{n \to \infty} f(x_n) = f(x_0).$
\end{enumerate}

\begin{Teo}[Teorema del valor intermedio o de Bolzano.]
Si $f \in C[a, b] $ y $\ell $ es un número cualquiera entre $f(a) $ y $f(b) $, entonces existe al menos un número $c \in (a, b) $ tal que $f(c) = \ell $. Véase la Figura~\ref{fig:bolzano}.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Fig1.png}
\caption{Teorema del valor intermedio o de Bolzano}
\label{fig:bolzano}
\end{figure}
\end{Teo}


\begin{Note}
Todas las funciones con las que se van a trabajar en este curso de métodos numéricos serán continuas, ya que esto es lo mínimo que debemos exigir para asegurar que la conducta de un método se puede predecir.
\end{Note}

%........................................................
\subsubsection{Derivabilidad}
%........................................................

Si $f(x) $ es una función definida en un intervalo abierto que contiene un punto $x_0 $, entonces se dice que $f(x) $ es \textbf{derivable} en $x = x_0 $ cuando existe el límite
$$f'(x_0) = \lim_{x \to x_0} \frac{f(x) - f(x_0)}{x - x_0}.$$

El número $f'(x_0)$ se llama \textbf{derivada} de $f$ en $x_0$ y coincide con la pendiente de la recta tangente a la gráfica de $f$ en el punto $(x_0, f(x_0))$, Figura~\ref{fig:derivada}.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Fig2.png}
\caption{Derivada de una función en un punto}
\label{fig:derivada}
\end{figure}

\textbf{Derivabilidad implica continuidad.} Si la función $f(x)$ es derivable en $x = x_0$, entonces $f(x)$ es continua en $x = x_0$. El conjunto de todas las funciones que admiten $n$ derivadas continuas en $S $ se denota por $C^n(S)$, mientras que el conjunto de todas las funciones indefinidamente derivables en $S$ se denota por $C^\infty(S)$. Las funciones polinómicas, racionales, trigonométricas, exponenciales y logarítmicas están en $C^\infty(S)$, siendo $S$ el conjunto de puntos en los que están definidas.

\begin{Teo}[Teorema del valor medio o de Lagrange.]
Si $f\in C[a, b]$ y es derivable en $(a,b)$, entonces existe un punto $c\in (a,b)$ tal que
$$f'(c) = \frac{f(b) - f(a)}{b - a}.$$
\end{Teo}

Geométricamente hablando, Figura~\ref{fig:lagrange}, el teorema del valor medio dice que hay al menos un número $c \in (a, b) $ tal que la pendiente de la recta tangente a la curva $y = f(x) $ en el punto $(c, f(c)) $ es igual a la pendiente de la recta secante que pasa por los puntos $(a, f(a)) $ y $(b, f(b)) $.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Fig3.png}
\caption{Teorema del valor medio o de Lagrange}
\label{fig:lagrange}
\end{figure}

\begin{Teo}[Teorema de los valores extremos.]  
Si $f \in C[a,b] $, entonces existen $c_1 $ y $c_2 $ en $(a,b) $ tales que $f(c_1) \leq f(x) \leq f(c_2) $ para todo $x \in [a,b] $. Si además, $f $ es derivable en $(a,b) $, entonces los puntos $c_1 $ y $c_2 $ están en los extremos de $[a,b] $ o bien son puntos críticos.
\end{Teo}

%........................................................
\subsubsection{Integración}
%........................................................


\begin{Teo}[Primer teorema fundamental o regla de Barrow.]
  Si $f \in C[a, b] $ y $F $ es una primitiva cualquiera de $f $ en $[a, b] $ (es decir, $F'(x) = f(x) $), entonces $$\int_a^b f(x) \, dx = F(b) - F(a). $$
\end{Teo}

\begin{Teo}[Segundo teorema fundamental.]
  Si $f \in C[a, b] $ y $x \in (a, b) $, entonces $$\frac{d}{dx} \int_a^x f(t) \, dt = f(x).$$
\end{Teo}

\begin{Teo}[Teorema del valor medio para integrales.]
  Si $f \in C[a, b] $, $g $ es integrable en $[a, b] $ y $g(x) $ no cambia de signo en $[a, b] $, entonces existe un punto $c \in (a, b) $ tal que  $$\int_a^b f(x)g(x) \, dx = f(c) \int_a^b g(x) \, dx.$$
\end{Teo}

\begin{Note}
Cuando $g(x) = 1 $, véase la Figura~\ref{fig:valormedio-integral}, este resultado es el habitual teorema del valor medio para integrales y proporciona el valor medio de la función $f $ en el intervalo $[a, b] $, que está dado por $$f(c) = \frac{1}{b-a} \int_a^b f(x) \, dx.$$

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{Fig4.png}
\caption{Teorema del valor medio para integrales}
\label{fig:valormedio-integral}
\end{figure}
\end{Note}

%...............................................................
\subsubsection{Polinomios de Taylor}
%...............................................................

\begin{Teo}[Teorema de Taylor.]
Supongamos que $f \in C^{(n)}[a,b] $ y que $f^{(n+1)} $ existe en $[a,b] $. Sea $x_0 $ un punto en $[a,b] $. Entonces, para cada $x $ en $[a,b] $, existe un punto $\xi(x) $ entre $x_0 $ y $x $ tal que
$$f(x) = P_n(x) + R_n(x),$$
donde
\begin{eqnarray}
P_n(x) &=& f(x_0) + f'(x_0)h + \frac{f''(x_0)}{2!}h^2 + \cdots + \frac{f^{(n)}(x_0)}{n!}h^n = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!} h^k,\\
R_n(x)&=& \frac{f^{(n+1)}(\xi(x))}{(n+1)!} h^{n+1}, \quad \text{y} \quad h = x - x_0.
\end{eqnarray}
\end{Teo}


El polinomio $P_n(x) $ se llama \textbf{n-ésimo polinomio de Taylor} de $f $ alrededor de $x_0 $ (véase la Figura~\ref{fig:taylor}).   $R_n(x) $ se llama \textbf{error de truncamiento} (o \textit{resto de Taylor}) asociado a $P_n(x) $.   Como el punto $\xi(x) $ en el error de truncamiento $R_n(x) $ depende del punto $x $ en el que se evalúa el polinomio $P_n(x) $, podemos verlo como una función de la variable $x $.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{Fig5.png}
\caption{Gráficas de $y = f(x) $ y de su polinomio de Taylor $y = P_2(x) $ alrededor de $x_0 $.}
\label{fig:taylor}
\end{figure}

\begin{Note}
La serie infinita que resulta al tomar límite en la expresión de $P_n(x) $ cuando $n \to \infty $ se llama \textbf{serie de Taylor} de $f $ alrededor de $x_0 $. Cuando $x_0 = 0 $, el polinomio de Taylor se suele denominar \textbf{polinomio de Maclaurin}, y la serie de Taylor se llama \textbf{serie de Maclaurin}.\bigskip

La denominación \textit{error de truncamiento} en el teorema de Taylor se refiere al error que se comete al usar una suma truncada al aproximar la suma de una serie infinita.
\end{Note}

%...............................................................
\subsubsection{Teoremas adicionales}
%...............................................................

A continuación enunciaremos algunas de las definiciones y teoremas básicos que utilizaremos a lo largo de estas notas.

\begin{Def}$f$ es de clase $C^1$ en el intervalo $[a;b]$ si $f'$ es continua en $[a;b]$.
\end{Def}

\begin{Def}$f$ es de clase $C^n$ en el intervalo $[a;b]$ si $f^{(n)}$ es continua en $[a;b]$.
\end{Def}

\begin{Def}$f$ es de clase $C^\infty$ en el intervalo $I$ si $f$ es infinitas veces derivable y continua en $I$.
\end{Def}

\begin{Teo} (Teorema de los valores intermedios). Sea $f$ continua en el intervalo $[a;b]$. Si $k \in \mathbb{R}$ es un número comprendido entre $f(a)$ y $f(b)$, entonces existe al menos un punto $\xi$ perteneciente al intervalo $(a;b)$ tal que $f(\xi) = k$.
\end{Teo}

\begin{Teo}[Bolzano]Si $f$ es continua en el intervalo $[a;b]$ y $f(a) \cdot f(b) < 0$, entonces existe un $\xi$ perteneciente al $(a;b)$ tal que $f(\xi) = 0$.
\end{Teo}

\begin{Teo}[Teorema de acotabilidad] Si $f : [a;b] \mapsto \mathbb{R}$ es continua en $[a;b]$, entonces $f$ está acotada en $[a;b]$.
\end{Teo}

\begin{Teo}[Teorema de Weierstrass] Si $f : [a;b] \mapsto \mathbb{R}$ es continua en $[a;b]$, entonces $f$ tiene un máximo global y un mínimo global en $[a;b]$.
\end{Teo}

\begin{Teo}[Teorema Generalizado de Rolle] Si $f$ continua en $[a;b]$, y existen las derivadas $f'(x), f''(x), \ldots, f^{(n)}(x)$ en $(a;b)$ y $f(x_0) = f(x_1) = \cdots = f(x_n) = 0$ (con $x_0, x_1, \ldots, x_n \in [a;b]$) entonces existe $\xi$ perteneciente al $(a;b)$ tal que $f^{(n)}(\xi) = 0$.
\end{Teo}

\begin{Teo}[Teorema de Lagrange] Si $f$ continua en $[a;b]$ y derivable en $(a;b)$ entonces existe $\xi$ perteneciente al $(a;b)$ tal que $f(b) - f(a) = f'(\xi)(b - a)$.
\end{Teo}

\begin{Teo}[Teorema del valor medio ponderado] Sea $f$ continua en $[a,b]$ y $g$ una función integrable Riemann en $[a,b]$. Si $g$ no cambia de signo en $[a,b]$, entonces existe un número $c \in (a,b)$ tal que:
$$\int_a^b f(x)g(x)dx = f(c) \int_a^b g(x)dx$$
\end{Teo}

%-------------------------------------------
\subsection{Álgebra Lineal}
%-------------------------------------------

%...............................................................
\subsubsection{Matrices}
%...............................................................

Una \textbf{matriz} es un arreglo multidimensional de escalares, llamados \textit{elementos}, ordenados en filas y columnas.  Una matriz de $ m $ filas y $ n $ columnas, o \textit{matriz (de orden) $ m \times n $}, es un conjunto de $ m \cdot n $ elementos $ a_{ij} $, con $ i = 1, 2, \ldots, m $ y $ j = 1, 2, \ldots, n $, que se representa de la siguiente forma:
$$A = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}$$

Se puede abreviar la representación de la matriz anterior de la forma $ A = (a_{ij}) $ con $ i = 1, 2, \ldots, m $; $ j = 1, 2, \ldots, n $.

Hay una relación directa entre matrices y vectores puesto que podemos pensar una matriz como una composición de vectores fila o de vectores columna. Además, un vector es un caso especial de matriz: un \textit{vector fila} es una matriz con una sola fila y varias columnas, y un \textit{vector columna} es una matriz con varias filas y una sola columna. 

%...............................................................
\subsubsection{Operaciones con matrices}
%...............................................................

\begin{itemize}
\item Si $ A = (a_{ij}) $ y $ B = (b_{ij}) $ son dos matrices que tienen el mismo orden, $ m \times n $, decimos que $ A $ y $ B $ son \textbf{iguales} si $ a_{ij} = b_{ij} $ para todo $ i = 1, \ldots, m $ y $ j = 1, \ldots, n $.

\item Si $ A = (a_{ij}) $ y $ B = (b_{ij}) $ son dos matrices que tienen el mismo orden $ m \times n $, la \textbf{suma} de $ A $ y $ B $ es una matriz $ C = (c_{ij}) $ del mismo orden con $ c_{ij} = a_{ij} + b_{ij} $ para todo $ i = 1, \ldots, m $ y $ j = 1, \ldots, n $.

\item Si $ A = (a_{ij}) $ es una matriz de orden $ m \times n $, la \textbf{multiplicación de $ A $ por un escalar} $ \lambda $ es una matriz $ C = (c_{ij}) $ del mismo orden $ m \times n $ con $ c_{ij} = \lambda a_{ij} $ para todo $ i = 1, \ldots, m $ y $ j = 1, \ldots, n $.

\item Si $ A = (a_{ij}) $ es una matriz de orden $ m \times n $, la \textbf{matriz traspuesta} de $ A $ es la matriz que resulta de intercambiar sus filas por sus columnas, se denota por $ A^T $ y es de orden $ n \times m $.

\item Si $ A = (a_{ij}) $ es una matriz de orden $ m \times p $ y $ B = (b_{ij}) $ es una matriz de orden $ p \times n $, el \textbf{producto de $ A $ por $ B $} es una matriz $ C = (c_{ij}) $ de orden $ m \times n $ con $$c_{ij} = \sum_{k=1}^p a_{ik}b_{kj}, \quad \text{para todo } i = 1, \ldots, m \text{ y } j = 1, \ldots, n.$$ Obsérvese que el producto de dos matrices solo está definido si el número de columnas de la primera matriz coincide con el número de filas de la segunda.
\end{itemize}

%...............................................................
\subsubsection{Matrices especiales}
%...............................................................
\begin{itemize}

\item Una matriz $ A = (a_{ij}) $ es \textbf{cuadrada} si tiene el mismo número de filas que de columnas, y de orden $ n $ si tiene $ n $ filas y $ n $ columnas. Se llama \textbf{diagonal principal} al conjunto de elementos $ a_{11}, a_{22}, \ldots, a_{nn} $.

\item Una \textbf{matriz diagonal} es una matriz cuadrada que tiene algún elemento distinto de cero en la diagonal principal y ceros en el resto de elementos.

\item Una matriz cuadrada con ceros en todos los elementos por encima (debajo) de la diagonal principal se llama \textbf{matriz triangular inferior (superior)}.

\item Una matriz diagonal con unos en la diagonal principal se denomina \textbf{matriz identidad} y se denota por $I$. Es la única matriz cuadrada tal que $ AI = IA = A $ para cualquier matriz cuadrada $A$.

\item Una \textbf{matriz simétrica} es una matriz cuadrada $A$ tal que $A=A^T$.

\item La \textbf{matriz cero} es una matriz con todos sus elementos iguales a cero.

\item Decimos que una matriz cuadrada $A$ es \textbf{invertible} (o \textit{regular} o \textit{no singular}) si existe una matriz cuadrada $ B $ tal que $AB = BA = I$. Se dice entonces que $ B $ es la \textbf{matriz inversa} de $A$ y se denota por $A^{-1}$. (Una matriz que no es invertible se dice \textit{singular}.)

\item Si una matriz $ A $ es invertible, su inversa también lo es y $A^{-1})^{-1} = A$.

\item Si $A$ y $B$ son dos matrices invertibles, su producto también lo es y $(AB)^{-1} = B^{-1}A^{-1}$.
\end{itemize}


%...............................................................
\subsubsection{Determinante de una matriz}
%...............................................................

El \textbf{determinante} de una matriz solo está definido para matrices cuadradas y su valor es un escalar.  El determinante de una matriz $ A $ cuadrada de orden $ n $ se denota por $ |A| $ o $ \det(A) $, y se define como
$$\det(A) = \sum_{j} (-1)^{i+j} a_{ij} \cdot \det(A_{ij}),$$
donde la suma se toma para todas las $ n! $ permutaciones de grado $n$ y $s$ es el número de intercambios necesarios para poner el segundo subíndice en el orden $1, 2, \ldots, n$.

\textbf{Algunas propiedades de los determinantes son:}
\begin{itemize}
\item $\det(A^T) = \det(A)$
\item $\det(AB) = \det(A)\det(B)$
\item $\det(A^{-1}) = \frac{1}{\det(A)}$
\item Si dos filas o dos columnas de una matriz coinciden, el determinante de esta matriz es cero.
\item Cuando se intercambian dos filas o dos columnas de una matriz, su determinante cambia de signo.
\item El determinante de una matriz diagonal es el producto de los elementos de la diagonal.
\item Si denotamos por $A_{ij}$ la matriz de orden $ (n-1) $ que se obtiene de eliminar la fila $i$ y la columna $j$ de la matriz $A$, llamamos \textbf{menor complementario} asociado al elemento $a_{ij}$ de la matriz $A$ al $\det(A_{ij})$.

\item Se llama \textbf{$k$-ésimo menor principal} de la matriz $A$ al determinante de la submatriz principal de orden $k$.

\item Definimos el \textbf{cofactor} del elemento $a_{ij}$ de la matriz $A$ por $\Delta_{ij} = (-1)^{i+j} \det(A_{ij})$.

\item Si $A$ es una matriz invertible de orden $ n $, entonces $$A^{-1} = \frac{1}{\det(A)} C,$$ donde $ C $ es la matriz de elementos $\Delta_{ij}$ para todo $i,j = 1,2,\ldots,n$. Obsérvese entonces que una matriz cuadrada es invertible si y s\'olo si su determinante es distinto de cero.
\end{itemize}

%...............................................................
\subsubsection{Valores propios y vectores propios}
%...............................................................

\begin{itemize}
\item Si $A$ es una matriz cuadrada de orden $n$, un número $\lambda$ es un \textbf{valor propio} de $A$ si existe un vector no nulo $v$ tal que $Av = \lambda v$. Al vector $v$ se le llama \textbf{vector propio} asociado al valor propio $\lambda$.

\item El valor propio $\lambda$ es solución de la \textbf{ecuación característica} $$\det(A - \lambda I) = 0,$$ donde $\det(A - \lambda I)$ se llama \textbf{polinomio característico}.  Este polinomio es de grado $n$ en $\lambda$  y tiene $n$ valores propios (no necesariamente distintos).
\end{itemize}

%...............................................................
\subsubsection{Normas vectoriales y normas matriciales}
%...............................................................
Para medir la \textbf{longitud} de los vectores y el \textbf{tamaño} de las matrices se suele utilizar el concepto de \textbf{norma}, que es una función que toma valores reales. Un ejemplo simple en el espacio euclidiano tridimensional es un vector $v = (v_1, v_2, v_3)$, donde $v_1, v_2$ y $v_3$ son las distancias a lo largo de los ejes $x, y, z$ respectivamente.  \bigskip

La \textbf{longitud del vector} $v$ (es decir, la distancia del punto $(0,0,0)$ al punto $(v_1, v_2, v_3)$) se calcula como $$\|v\| = \sqrt{v_1^2 + v_2^2 + v_3^2},$$
donde la notación $\|v\|$ indica que esta longitud se refiere a la \textit{norma euclidiana} del vector $v$.   De forma similar, para un vector $v$ de dimensión $n$, $v = (v_1, v_2, \ldots, v_n)$, la norma euclidiana se calcula como $$\|v\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.$$ Este concepto puede extenderse a una matriz $ m \times n $, $ A = (a_{ij}) $, de la siguiente manera: $$\|A\|_F = \sqrt{ \sum_{i=1}^m \sum_{j=1}^n a_{ij}^2 },$$
que recibe el nombre de \textbf{norma de Frobenius}.

Hay otras alternativas a las normas euclidiana y de Frobenius. Dos normas usuales son la \textbf{norma 1} y la \textbf{norma infinito}:

\begin{itemize}
\item La \textbf{norma 1} de un vector $ v = (v_1, v_2, \ldots, v_n) $ se define como $ \|v\|_1 = \sum_{i=1}^n |v_i| $.  De forma similar, la norma 1 de una matriz $ m \times n $, $ A = (a_{ij}) $, se define como $$\|A\|_1 = \max_{1 \leq j \leq n} \sum_{i=1}^m |a_{ij}|.$$

\item La \textbf{norma infinito} de un vector $ v = (v_1, v_2, \ldots, v_n) $ se define como $ \|v\|_\infty = \max_i |v_i| $.   La norma infinito de una matriz $ m \times n $, $ A = (a_{ij}) $, se define como  $$\|A\|_\infty = \max_{1 \leq i \leq m} \sum_{j=1}^n |a_{ij}|.$$

\item Todas las normas son equivalentes en un espacio vectorial de dimensión finita. 

\end{itemize}

%===========================================
\section{Operaciones de punto flotante}
%===========================================

%-------------------------------------------
\subsection{Sistemas decimal y binario}
%-------------------------------------------

El sistema numérico que se utiliza frecuentemente es el sistema decimal, en la que la base de expresión es el $10$. Sin embargo las computadoras utilizan el sistema binario, sistema de base 2, es decir solamente $\left\{0,1\right\}$.

\begin{Prop}
Para cualquier número natural $N$, existen $a_0,a_1,a_2,\ldots,a_K$, con $a_i\in \mathbb{R}$ tales que
\begin{equation}\label{Ec.Expansion.Binaria}
N=a_{K}\times2^{K}+a_{K-1}\times2^{K-1}+a_{K-2}\times2^{K-2}+\cdots+a_{1}\times2+a_{0}\times2^0
\end{equation}

Para ver lo anterior lo que tenemos que hacer es calcula $\frac{N}{2}$, es decir, $\frac{N}{2}=P_{0}+\frac{a_{0}}{2}$, donde $P_{0}=a_{K}\times2^{K-1}+a_{K-1}\times2^{K-2}+a_{K-2}\times2^{K-3}+\cdots+a_{1}\times2^{0}$, es decir $a_{0}$ es el resto de dividir $N$ entre $2$. Ahora hagamos lo mismo para $P_{0}$: $$\frac{P_{0}}{2}=a_{K}\times2^{K-2}+a_{K-1}\times2^{K-3}+a_{K-2}\times2^{K-4}+\cdots+a_{2}\times2^{0}+\frac{a_{1}}{2},$$ por lo tanto $$\frac{P_{0}}{2}=P_{1}+\frac{a_{1}}{2},$$ donde $$P_{1}=a_{K}\times2^{K-2}+a_{K-1}\times2^{K-3}+a_{K-2}\times2^{K-4}+\cdots+a_{2}\times2^{0},$$ es decir $P_1$ es el resto de dividir $P_0$ entre $2$. Siguiendo este procedimiento de manera análoga hasta que encontremos un valor $K$ tal que $P_K=0$. Por lo tanto tenemos el siguiente algoritmo:
\begin{Algthm}
Para un valor $N$ natural, los términos $a_{k}$ en la ecuación \ref{Ec.Expansion.Binaria} se encuentran
\begin{eqnarray}
\begin{array}{l}
N=2P_{0}+a_{0},\\
P_{0}=2P_{1}+a_{1},\\
\vdots\\
P_{K-2}=2P_{K-1}+a_{K-1},\\
P_{K-1}=2P_{K}+a_{K},\\
P_{K}=0.
\end{array}
\end{eqnarray}
\end{Algthm}
\end{Prop}

\begin{Ejem}
Convertir $24563$ 
\begin{eqnarray*}
24563 &=& 12281 \times 2 + 1, \quad a_{0} = 1\\
12281 &=& 6140 \times 2 + 1, \quad a_{1} = 1\\
6140 &=& 3070 \times 2 + 0, \quad a_{2} = 0\\
3070 &=& 1535 \times 2 + 0, \quad a_{3} = 0\\
1535 &=& 767 \times 2 + 1, \quad a_{4} = 1\\
767 &=& 383 \times 2 + 1, \quad a_{5} = 1\\
383 &=& 191 \times 2 + 1, \quad a_{6} = 1\\
191 &=& 95 \times 2 + 1, \quad a_{7} = 1\\
95 &=& 47 \times 2 + 1, \quad a_{8} = 1\\
47 &=& 23 \times 2 + 1, \quad a_{9} = 1\\
23 &=& 11 \times 2 + 1, \quad a_{10} = 1\\
11 &=& 5 \times 2 + 1, \quad a_{11} = 1\\
5 &=& 2 \times 2 + 1, \quad a_{12} = 1\\
2 &=& 1 \times 2 + 0, \quad a_{13} = 0\\
1 &=& 0 \times 2 + 1, \quad a_{14} = 1
\end{eqnarray*}
Por lo tanto el número binario es: $(24563)_{10}=(101111111110011)_{2}$.

\end{Ejem}


\begin{Prop}
Sea $Q\in\mathbb{R}$, tal que $0<Q<1$, entonces existen términos $b_{1},b_{2},\ldots,b_{k}$ tales que $Q=0.b_{1}b_{2}b_{3}\cdots b_{k}$, y por tanto

\begin{eqnarray}
Q=b_{1}\times2^{-1}+b_{2}\times2^{-2}+b_{3}\times2^{-3}+\cdots+b_{k}\times2^{-k}+\cdots
\end{eqnarray}

Si multiplicamos $Q$ por $2$, se tiene que
$$2Q=b_{1}+b_{2}\times2^{-1}+b_{3}\times2^{-2}+b_{4}\times2^{-3}+\cdots+b_{k}\times2^{-k+1}+\cdots$$

Si $F_{1}=frac(2Q)$, con $frac(x)$ la parte fraccionaria de $x$, y $b_{1}=[[2Q]]$, donde $[[x]]$ es la parte entera de $x$, entonces $$F_1=b_{2}\times2^{-1}+b_{3}\times2^{-2}+b_{4}\times2^{-3}+\cdots+b_{k}\times2^{-k+1}+\cdots,$$ de donde $$2F_1=b_{2}\times2^{0}+b_{3}\times2^{-1}+b_{4}\times2^{-2}+\cdots+b_{k}\times2^{-k+2}+\cdots=b_{2}+F_{2},$$ donde $F_{2}=frac(2F_{1})$, y $b_{2}=[[2F_1]]$. Procediendo de manera análoga para el resto de los términos se tienen las suceciones $\left\{b_{k}\right\}$ y $\left\{F_{k}\right\}$, dadas por $b_{k}=[[2F_{k-1}]]$ y $F_{k}=frac(2F_{k-1})$, con $b_{1}=[[2Q]]$ y $F_{1}=frac(2Q)$. Por lo tanto se tiene la representación binaria de Q dada por

\begin{equation}
Q=\sum_{i=1}^{\infty}b_{i}2^{-i}
\end{equation}
\end{Prop}


\begin{Ejem}
Convertir el número $3.5786$. Sea $Q = 0.5786$, entonces
\begin{eqnarray*}
2Q &=& 1.1572, b_{1} = [[1.1572]]= 1, F_{1} = frac(1.1572) = 0.1572\\
2F_{1}& =& 0.3144, b_{2} = [[0.3144 ]]= 0,F_{2} = frac(0.3144) = 0.3144\\
2F_{2} &=& 0.6288, b_{3} = [[0.6288 ]]= 0, F_{3} = frac(0.6288) = 0.6288\\
2F_{3} &=& 1.2576, b_{4} = [[1.2576 ]]= 1 F_{4} = frac(1.2576) = 0.2576\\
2F_{4} &=& 0.5152, b_{5} = [[0.5152 ]]= 0, F_{5} = frac(0.5152) = 0.5152\\
2F_{5} &=& 1.0304, b_{6} = [[1.0304 ]] = 1 F_{6} = frac(1.0304) = 0.0304\\
2F_{6} &=& 0.0608, b_{7} = [[0.0608]]= 0, F_{7} = frac(0.0608) = 0.0608\\
2F_{7} &=& 0.1216, b_{8} = [[0.1216]] = 0, F_{8} = frac(0.1216) = 0.1216
\end{eqnarray*}

De lo anterior se tiene que:
$$0.5786 = (0.10010100\ldots)_2$$
Por lo tanto:$$3.5786 = (11.10010100\ldots)_2.$$


\end{Ejem}

\begin{Ejer}
Convertir los siguientes números de base 10 a base 2.
\begin{enumerate}
\item $324$
\item $27$
\item $1423$
\item $235.25$
\item $41.596$
\end{enumerate}
\end{Ejer}

%-------------------------------------------
\subsection{Números en punto flotante}
%-------------------------------------------

\begin{Def}
Los n\'umeros en punto flotante son n\'umero reales de la forma 
\begin{equation}
\pm\alpha \times \beta^{e},
\end{equation}

donde $\alpha$ tiene un n\'umero de d\'igitos limitados, $\beta$ es la base y $e$ es el exponente que modifica la posici\'on del punto decimal.
\end{Def}

\begin{Def}
Un n\'umero real $x$ tiene una representaci\'on punto flotante normalizada si 
\begin{equation}
\pm\alpha \times \beta^{e},
\end{equation}
con $\frac{1}{\beta}<|\alpha|<1$
\end{Def}

\begin{Note}
En este caso $x$ se puede escribir en la forma 
\begin{equation}
x=\pm0,d_{1}d_{2}\cdots d_{k}\times\beta^{e},
\end{equation} 

donde si $x\neq0$,$d_{1}\neq0$, y adem\'as $0\leq d_{i}<\beta$, para $i=1,2,3\dots,k$ y $L\leq e\leq U$.
\end{Note}

\begin{Def}
El conjunto de los n\'umeros en punto flotante se le llama \textbf{conjunto de n\'umeros m\'aquina}.
\end{Def}

\begin{Note}
El conjunto de n\'umeros m\'aquina es finito. Para ver esto consideremos que si $x$ es de la forma 
\begin{equation}\label{Eq.Punto.Flotate}
\pm0,d_{1}d_{2}\cdots d_{t}\times\beta^{e},
\end{equation}
dado que $d_{1}\neq0$ y $0\leq d_{i}<\beta$ entonces $d_1$ puede tomar $\beta-1$ valores distintos, mientras que para $d_{i}$ hay $\beta$ posibilidades. Por lo tanto se tienen $\left(\beta-1\right)\beta\beta\cdots\beta=\left(\beta-1\right)\beta^t$. El n\'umero de exponentes posibles son $U-L+1$, en total hay $\left(\beta-1\right)\beta^t\left(U-L+1\right)$ n\'umeros m\'aquina positivos, por lo tanto, considerando positivos y negativos hay $2\left(\beta-1\right)\beta^t\left(U-L+1\right)$  n\'umeros m\'aquina, considerando que el creo tambi\'es es un n\'umero de m\'aquina hay en realidad $2\left(\beta-1\right)\beta^t\left(U-L+1\right)+1$. Es decir, cualquier n\'umero real puede ser representado por uno de los $2\left(\beta-1\right)\beta^t\left(U-L+1\right)+1$ n\'umeros de m\'aquina.
\end{Note}


\begin{Ejem}
Recordemos la expresi\'on (\ref{Eq.Punto.Flotate}), consideremos $\beta=2$, $t=3$, $L=-2$ y $U=2$. 
Entonces $x=\pm d_{1}d_{2}d_{3}\times\left(2\right)^{e}$, con $-2\leq e\leq 2$ y $0\leq d_{1},d_{2}<2$. Por lo tanto se tiene que $d_{1},d_{2},d_{3}=1$; $e=\left\{-2,-1,0,1,2\right\}$.  Entonces $d_{1}d_{2}d_{3}=\left\{100,101,110,111\right\}=\left\{\frac{1}{2},\frac{5}{8},\frac{3}{4},\frac{7}{8}\right\}$
\begin{eqnarray}
\begin{array}{|c|c|c|c|c|}\hline
-2&-1&0&1&2\\\hline
0.111\times 2^{-2}&0.111\times 2^{-1}&0.111\times 2^{0}&0.111\times 2^{1}&0.111\times 2^{2}\\\hline
0.110\times 2^{-2}&0.110\times 2^{-1}&0.110\times 2^{0}&0.110\times 2^{1}&0.110\times 2^{2}\\\hline
0.101\times 2^{-2}&0.101\times 2^{-1}&0.101\times 2^{0}&0.101\times 2^{1}&0.101\times 2^{2}\\\hline
0.100\times 2^{-2}&0.100\times 2^{-1}&0.100\times 2^{0}&0.100\times 2^{1}&0.100\times 2^{2}\\\hline
\end{array}
\end{eqnarray}
sustituyendo y resolviendo las operaciones

\begin{eqnarray}
\begin{array}{|c|c|c|c|c|}\hline
-2&-1&0&1&2\\\hline
\frac{7}{8}\times 2^{-2}=\frac{7}{32}&\frac{7}{8}\times 2^{-1}=\frac{7}{16}&\frac{7}{8}\times 2^{0}=\frac{7}{8}&\frac{7}{8}\times 2^{1}=\frac{7}{4}&\frac{7}{8}\times 2^{2}=\frac{7}{2}\\\hline\hline
\frac{3}{4}\times 2^{-2}=\frac{3}{16}&\frac{3}{4}\times 2^{-1}=\frac{3}{8}&\frac{3}{4}\times 2^{0}=\frac{3}{4}&\frac{3}{4}\times 2^{1}=\frac{3}{2}&\frac{3}{4}\times 2^{2}=3\\\hline\hline
\frac{5}{8}\times 2^{-2}=\frac{5}{32}&\frac{5}{8}\times 2^{-1}=\frac{5}{16}&\frac{5}{8}\times 2^{0}=\frac{5}{8}&\frac{5}{8}\times 2^{1}=\frac{5}{4}&\frac{5}{8}\times 2^{2}=\frac{5}{2}\\\hline\hline
\frac{1}{2}\times 2^{-2}=\frac{1}{8}&\frac{1}{2}\times 2^{-1}=\frac{1}{4}&\frac{1}{2}\times 2^{0}=\frac{1}{2}&\frac{1}{2}\times 2^{1}=1&\frac{1}{2}\times 2^{2}=2\\\hline\hline
\end{array}
\end{eqnarray}
Por tanto el n\'umero total de n\'umeros m\'aquina son 41.
\end{Ejem}

\begin{Ejer}
\begin{enumerate}
\item Describir todos los n\'umeros m\'aquina para $\beta=2$, $t=4$, $L=-3$ y $U=3$. 
\item Escribir el n\'umero $732.5051$ en notaci\'on de punto flotante. Respuesta $0.7325051\times10^{-3}$
\item Escribir el n\'umero $0.006521$ en notaci\'on de punto flotante.  Respuesta $0.06521\times10^{-2}$
\item $\left(101.01\right)_2$. Respuesta $0.10101\times2^{3}$
\item $\left(0.00101111\right)_2$. Respuesta $0.101111\times2^{-2}$
\end{enumerate}
\end{Ejer}

\begin{Note}
$\left(0.1\right)_2=1\times2^{-1}=0.5$
\end{Note}


\subsection{Representaci\'on}

En una computadora los n\'umeros se expresan como se ha descrito, pero con restricciones sobre $q$ y $m$ dadas por la longitud de la palabra. Supongamos que la longitud de la palabra es de $32$ bits, los cuales se distribuyen de la siguiente manera: los dos primeros se reservan para los signos: es $0$ si es positivo y $1$ si es negativo; los siguientes $7$ espacios para el exponente, y los restantes para la mantisa. Considerando que cualquier n\'umero puede normalizarse, recordar $x=\pm q\times 2^{m}$, con $\frac{1}{2}\leq q<1$, se puede asumir que el primer bit en $q$ es $1$ y por tanto no se requeire almacenar,.

\begin{Ejem}
Representar y almacenar en punto flotante normalizado  el n\'umero $-0.125$.  A saber $(-0.125)=(-0.001)_{2}=(-0.1)_2\times2^{-2}$,  adem\'as $2=(10)_{2}$; por lo tanto su representaci\'on es: $1,1|,$ $0,0,0,0,0,1,0|$ $0,0,0,0,0,0,0,\dots0$.
\end{Ejem}

\begin{Ejem}
Represente y almacene el n\'umero $117.125$. Respuesta $117=(1110101)_2$ y $0.125=(0.001)_2$, por tanto $117.125=(1110101.001)_2=(0.1110101001)_2\times2^{7}$ y $y=(111)_{2}$, por tanto se almacena: $0,0||0000111||110101\dots0$
\end{Ejem}


\begin{Note}
$|m|$ no requiere m\'as de $7$ bits, es decir $|m|\leq(1111111)_{2}=2^{7}-1=127$, por tanto el exponente de $7$ d\'igitos binarios proporciona un intervalo de $0$ a $127$, para n\'umeros peque\~ nos se toma el exponente en el intervalo $[-63,64]$. Adem\'as $q$ requiere de a lo m\'as 24 bits, por tanto la m\'aquina de $32$ bits tienen una precisi\'on limitada entre $7$ y $8$ d\'igitos decimales, ya que el bit menos significativo en la mantisa representa unidades del orden $2^{-24}\approx10^{-7}$. Esto quiere decir que n\'umeros expresados por m\'as de siete d\'igitos decimales ser\'an aproximados cuando se dan como datos de entrada o como resultados de operaciones.
\end{Note}

\subsection{Errores}

A la hora de realizar un c\'alculo es importante asegurarse de que los n\'umeros que intervienen en el c\'alculo se pueden utilizar con confianza.   Para ello, se introduce el concepto de \textbf{cifras significativas}, que designa formalmente la notaci\'on de un valor num\'erico, y se usa en aquellos que gu\'ian visualmente la precisi\'on.\medskip

Los \textbf{errores de truncamiento} se producen cuando utilizamos una aproximaci\'on en lugar de un procedimiento matem\'atico exacto. Para conocer las caracter\'isticas de estos errores se suelen considerar los polinomios de Taylor.   Cuando se aproxima un proceso continuo por uno discreto, para errores provocados por un tama\~no de paso finito $h$, resulta a menudo \'util describir la dependencia del error $e$ con $ h $ cuando $h$ tiende a cero.\medskip

Decimos que una funci\'on $ f(h) $ es una $ \mathcal{O} $ grande de $ h^n $ si $ |f(h)| \leq c h^n $ para alguna constante $ c $, cuando $ h $ es cercano a cero; se escribe \begin{equation} f(h) = \mathcal{O}(h^n)\end{equation}.

Si un m\'etodo tiene un t\'ermino de error que es $\mathcal{O}(h^n)$, se suele decir que es un \textbf{m\'etodo de orden $n$}.  Por ejemplo, si utilizamos un polinomio de Taylor para aproximar la funci\'on $ g $ en $ x = a + h $, tenemos:v$$g(x) = g(a + h) = g(a) + h g'(a) + \frac{h^2}{2!} g''(a) + \frac{h^3}{3!} g^{(3)}(\xi), \quad \text{para alg\'un } \xi \in [a, a+h].$$

Suponiendo que $ g $ es suficientemente derivable, la aproximaci\'on anterior es $ \mathcal{O}(h^3) $, puesto que el error $$\frac{h^3}{3!} g^{(3)}(\xi), \quad \text{satisface} \quad \left| \frac{h^3}{3!} g^{(3)}(\xi) \right| \leq \frac{M}{3!} |h^3|,$$
donde $ M $ es el m\'aximo de $ g^{(3)}(x) $ para $ x \in [a, a+h] $.

Las diferencias (errores) son m\'ultiples y de diversa naturaleza, aunque pueden separarse en dos grupos gen\'ericos:

\begin{itemize}
    \item \textbf{Los errores que provienen del modelado te\'orico} (o abstracci\'on matem\'atica) del fen\'omeno real; estos errores se denominan \textit{Errores del modelo o inherentes}. Los errores inherentes son producto de factores intr\'insecos a la naturaleza, al ambiente y las personas mismas. Los errores inherentes son imposibles de remediar aunque pueden minimizarse; en consecuencia, no pueden cuantificarse.

    \begin{quote}
    Se distinguen dos tipos de errores inherentes: \textbf{Las incertidumbres} hacen referencia a las dimensiones f\'isicas que nunca podr\'an ser medidas en forma exacta debido a la naturaleza de la materia y a las imperfecciones de los instrumentos de medici\'on. \textbf{Las verdaderas equivocaciones} son las situaciones que se producen en la lectura de instrumentos de medici\'on o en el traslado de informaci\'on y que son inadvertidas a las personas; un claro ejemplo de estas situaciones es la denominada \textit{ceguera de taller}.
    \end{quote}

    \item \textbf{Los errores del m\'etodo} son producto de la limitante en la representaci\'on y manipulaci\'on de cantidades num\'ericas utilizadas en los c\'alculos necesarios en el desarrollo del modelo matem\'atico. Es de destacar que los dispositivos de c\'alculo (tales como calculadoras y computadoras) utilizan y manipulan cantidades en forma imprecisa.
\end{itemize}
    Existen dos grandes tipos de errores del m\'etodo: \textit{El truncamiento} se provoca ante la imposibilidad de manipular, por parte de un instrumento de c\'omputo, una cantidad infinita de t\'erminos o cifras. Los t\'erminos o cifras omitidas (que son infinitas en n\'umero) introducen un error en los resultados calculados. \textit{El redondeo} se produce por el mismo motivo que el truncamiento pero, a diferencia de \'este, las cifras omitidas s\'i son consideradas en la cifra resultante

  
En general, si se incrementa el n\'umero de cifras significativas en el ordenador, se minimizan los errores de redondeo, y los errores de truncamiento disminuyen a medida que los errores de redondeo se incrementan.  Por lo tanto, para disminuir uno de los dos sumandos del error total debemos incrementar el otro.  Como el error total no se puede calcular en la mayor\'ia de los casos, se suelen utilizar otras medidas para estimar la exactitud de un m\'etodo num\'erico, que suelen depender del m\'etodo espec\'ifico.  En algunos m\'etodos el error num\'erico se puede acotar, mientras que en otros se determina una estimaci\'on del \textit{orden de magnitud del error}. Cuando se buscanlas soluciones num\'ericas de un problema real los resultados que se obtienen por lo general no son exactos.  

\subsection{Cuantificaci\'on de errores}

Los errores se cuantifican de dos formas diferentes:

\begin{enumerate}
  \item \textbf{Error absoluto}. El error absoluto es la diferencia absoluta entre un valor real y un aproximado. Est\'a dado por la siguiente f\'ormula:

  $$E = \left| V_{real} - V_{aprox} \right| $$

  El error absoluto recibe este nombre ya que posee las mismas dimensiones que la variable bajo estudio.

  \item \textbf{Error relativo}. Corresponde a la expresi\'on en porcentaje de un error absoluto; en consecuencia, este error es adimensional.

  $$  e = \left| \frac{V_{real} - V_{aprox}}{V_{real}} \right| \cdot 100\%$$
\end{enumerate}

La diferencia entre la preferencia en el uso de los dos tipos de error consiste precisamente en la presencia de las dimensiones f\'isicas. Debido a las unidades de medici\'on utilizadas, el manejo y la percepci\'on del error absoluto suele ser enga\~noso o dif\'icil de comprender r\'apidamente. Sin embargo, el manejo de porcentajes (o valores relativos) resulta m\'as natural y sencillo de comprender. Sin embargo, el uso de estos dos tipos de errores est\'a sujeto siempre al objetivo de las actividades desarrolladas.

Las expresiones que definen a los errores absoluto y relativo requieren del conocimiento de la variable $V_{real}$ que representa un valor ideal que no posee error alguno. Como podr\'a suponerse, en la realidad resulta imposible determinar este valor. Una pr\'actica com\'un en los an\'alisis elementales sobre errores es considerar como un valor real a los resultados arrojados por la medici\'on experimental de los fen\'omenos y a los valores aproximados como los proporcionados por los modelos matem\'aticos (o viceversa).  En realidad, ambos valores son valores aproximados.  Para lograr un resultado coherente, en la pr\'actica debe sustituirse al valor real por un valor que se considere posee un error menor. 

En el caso del an\'alisis num\'erico, dado que los resultados se obtienen a partir de procesos iterativos que se mejoran los inicialmente obtenidos, debe partirse del supuesto que el \'ultimo valor obtenido posee un nivel menor de error que el valor previo. Dado lo anterior, los errores absoluto y relativo se calcular\'an de la siguiente forma:

\textbf{Error absoluto:}
$$E = \left| V_i - V_{i-1} \right|$$

\textbf{Error relativo:}
$$e = \left| \frac{V_i - V_{i-1}}{V_i} \right| \cdot 100\%$$

En ambas ecuaciones, $V_i$ es el valor de la \'ultima iteraci\'on y $V_{i-1}$ es el valor de la iteraci\'on anterior $i – 1$.

\begin{itemize}
    \item \textbf{Error absoluto:} Se define como la diferencia entre el valor real (experimental o exacto) y el valor aproximado (te\'orico o calculado):

    $$
    E_a = |x_{\text{real}} - x_{\text{aproximado}}|
    $$

    \item \textbf{Error relativo:} Es la relaci\'on entre el error absoluto y el valor real:

    $$
    E_r = \frac{E_a}{|x_{\text{real}}|}
    $$

    \item \textbf{Error porcentual:} Es el error relativo expresado en porcentaje:

    $$
    E_p = E_r \cdot 100 = \frac{E_a}{|x_{\text{real}}|} \cdot 100
    $$
\end{itemize}

\subsection{Errores en punto flotante}

Un n\'umero real $ x \in \mathbb{R} $, cuando no pertenece a $ F $, se representa mediante una aproximaci\'on flotante $ fl(x) \in F $, de modo que:

$$x = fl(x)(1 + \varepsilon), \quad |\varepsilon| \leq \epsilon_{\text{mach}}.$$

Este $ \varepsilon $ se llama \textbf{error relativo} y $ \epsilon_{\text{mach}} $ es la \textbf{precisi\'on de la m\'aquina}.  En general se tiene que:

$$\epsilon_{\text{mach}} = \frac{1}{2} \beta^{1 - t}.$$

\begin{Note}
Los ordenadores almacenan los n\'umeros reales en forma binaria (base 2).   Cada d\'igito binario (0 \'o 1) se llama \textbf{bit} (por digital binary).   La memoria de los ordenadores est\'a organizada en \textbf{bytes}, siendo un byte = 8 bits.   En el est\'andar IEEE, los ordenadores representan n\'umeros reales en precisi\'on simple (32 bits) y en precisi\'on doble (64 bits).  Este est\'andar tambi\'en define los c\'odigos para representar valores especiales como NaN (Not a Number), infinitos, y ceros con signo. Para representar un n\'umero real, se utiliza la forma normalizada:
$$x = (-1)^s \cdot (1 + f) \cdot 2^e,$$
donde: $ s $: es el bit de signo (0 para positivo, 1 para negativo),  $ f $: es la fracci\'on,  $ e $: es el exponente con sesgo.\medskip

Por ejemplo, el n\'umero 0.15625 en binario es 0.00101, que se escribe como $ 1.01 \cdot 2^{-3} $ y se almacena como: signo = 0, exponente = $124$ (con sesgo de $127$), y mantisa = $010000...$ En este formato, la precisi\'on depende de la cantidad de bits reservados a la fracci\'on $ f $.   En doble precisi\'on (64 bits), se reservan 52 bits para la fracci\'on, 11 para el exponente y 1 para el signo.
\end{Note}


\subsection{Aproximaci\'on num\'erica y errores}

Una \textit{aproximaci\'on} es un valor cercano a uno considerado como real o verdadero. Esta cercan\'ia, o diferencia, se conoce como \textit{error}. Normalmente, la consideraci\'on de la validez de una aproximaci\'on depende de la cota de error que el experimentador considere pertinente en funci\'on del contexto del fen\'omeno bajo estudio. Esto implica que tambi\'en debe considerarse que una magnitud debe ser un valor real, que en el \'ambito de la Ingenier\'ia pocas veces se conoce, lo que obliga a adoptar convenciones. En Ingenier\'ia, se denomina \textit{exactitud} a la capacidad de un instrumento de medir un valor cercano al de la magnitud real. Exactitud implica precisi\'on, pero no al contrario. Exactitud y precisi\'on no son equivalentes. Exactitud es capacidad para acercarse a la magnitud real, y precisi\'on es la capacidad de generar resultados similares. La precisi\'on se logra cuando un instrumento para repetir mediciones exactas cuando \'estas se realizan consecutivamente. De acuerdo con la definici\'on de aproximaci\'on num\'erica, la exactitud se aplica en los m\'etodos num\'ericos en cuanto a la capacidad del m\'etodo de generar un resultado muy cercano al valor real; se percibe la cercan\'ia entre la exactitud y el concepto de error. Por otra parte, los m\'etodos num\'ericos a trav\'es de iteraciones generan valores aproximados cada vez m\'as exactos, es decir, estas iteraciones deber\'an ser precisas. Dado lo anterior, los m\'etodos num\'ericos deber\'an tener como cualidades la exactitud y la precisi\'on. Matem\'aticamente, la \textbf{convergencia} es la propiedad de algunas sucesiones y series de tender progresivamente a un l\'imite, de tal forma, si este l\'imite existe, se dice que la sucesi\'on o la serie \textit{converge}.  En forma an\'aloga, si un m\'etodo num\'erico en su funcionamiento iterativo nos proporciona aproximaciones cada vez m\'as cercanas al valor buscado, se dice que el m\'etodo converge. La convergencia se mide a trav\'es de los errores; si el error entre dos aproximaciones sucesivas se reduce, el m\'etodo converge; se debe cumplir que:

$$|x_n - x_{n-1}| \leq |x_{n-1} - x_{n-2}|$$

Es decir, la diferencia en\'esima $ (x_n - x_{n-1}) $ debe ser menor que la diferencia $ (n-1)$\'esima $ x_{n-1} - x_{n-2} $. Se dice que un sistema (o un proceso) es \textbf{estable} si a peque\~nas variaciones en la entrada o en la excitaci\'on corresponden peque\~nas variaciones en la salida o en la respuesta. La estabilidad de un m\'etodo num\'erico tiene que ver con la manera en que los errores num\'ericos se propagan a lo largo del algoritmo.  Cuando un m\'etodo converge, lo m\'as deseable es que en los resultados que se obtengan, los niveles de error se disminuyan en la forma m\'as r\'apida posible. Sin embargo, ocurre que durante la operaci\'on del algoritmo, ya sea por el manejo de los datos num\'ericos o bien por la naturaleza propia del modelo matem\'atico con el que se est\'e trabajando, los errores entre aproximaciones no disminuyan en forma progresiva, sino que incluso aumenten en alguna etapa del proceso para despu\'es reducirse mostrando un comportamiento aleatorio.

La \textbf{robustez} de un m\'etodo num\'erico radica en su convergencia y su estabilidad. Pueden utilizarse m\'etodos cuya prueba de convergencia indique la pertinencia de su uso, pero que durante su aplicaci\'on se obtengan resultados inestables que repercutan en el n\'umero de iteraciones y en consecuencia en el tiempo invertido en la soluci\'on. El ideal lo constituyen m\'etodos que a la vez de ser convergentes resulten estables.

\begin{Note}
\textbf{Convergencia} de un m\'etodo se refiere a que sea posible la obtenci\'on del valor buscado cuando el n\'umero de pasos tiende a infinito.  T\'ipicamente, la convergencia se analiza en m\'etodos iterativos, es decir, aquellos en los que el resultado final se obtiene tras una repetici\'on de c\'alculos.  Cuando repetimos estas iteraciones, los datos iniciales producen valoraciones progresivas del resultado.
\end{Note}




%-------------------------------------------
\subsection{Ejercicios}
%-------------------------------------------
\begin{enumerate}
\item Realizar una revisión de la historia de los m\'etodos num\'ericos, elaborar un documento de hasta dos cuartillas.
\item Realiza las siguientes conversiones de base $10$ a base $2$:
\begin{enumerate}
\item 246
\item 345.68
\item 4586632.2846
\item 984365.27463
\item 79905523
\end{enumerate}
\item Elabora el c\'odigo en R para realizar la conversi\'on de base $10$ a base $2$.
\end{enumerate}


%===========================================
\section{Solución de Sistemas de Ecuaciones Lineales}
%===========================================

\subsubsection{Definiciones sobre matrices}

\begin{Def}[Matriz transpuesta]
Sea $A=(a_{ij})\in \mathbb{R}^{m\times n}$.  
La \textbf{matriz transpuesta} de $A$, denotada $A^\top$, es la matriz $n\times m$ definida por
\begin{eqnarray*}
(A^\top)_{ij}=a_{ji}.
\end{eqnarray*}
\end{Def}

\begin{Def}[Matriz simétrica]
Una matriz $A\in \mathbb{R}^{n\times n}$ es \textbf{simétrica} si
\begin{eqnarray*}
A^\top=A.
\end{eqnarray*}
\end{Def}

\begin{Def}[Matriz definida positiva]
Una matriz simétrica $A\in \mathbb{R}^{n\times n}$ es \textbf{definida positiva} si
\begin{eqnarray*}
x^\top A x > 0 \quad \text{para todo } x\in \mathbb{R}^n, \; x\neq 0.
\end{eqnarray*}
\end{Def}

\begin{Def}[Matriz semidefinida positiva]
Una matriz simétrica $A\in \mathbb{R}^{n\times n}$ es \textbf{semidefinida positiva} si
\begin{eqnarray*}
x^\top A x \geq 0 \quad \text{para todo } x\in \mathbb{R}^n.
\end{eqnarray*}
\end{Def}


Hasta ahora hemos visto métodos \textbf{directos} (eliminación Gaussiana, factorizaciones $LU$, Doolittle, Cholesky).  
En problemas de gran tamaño, estos métodos pueden ser muy costosos en tiempo y memoria.  
Por ello se usan \textbf{métodos iterativos}, que generan una sucesión de aproximaciones $\{x^{(k)}\}$ que converge a la solución exacta $x$ de
\begin{eqnarray*}
Ax=b.
\end{eqnarray*}


Un sistema de ecuaciones lineales puede escribirse como:
\begin{eqnarray*}
Ax=b, \qquad A=(a_{ij})\in\mathbb{R}^{n\times n},\quad x,b\in\mathbb{R}^n.
\end{eqnarray*}

Nuestro objetivo es encontrar $x$. La eliminación gaussiana consiste en aplicar operaciones de filas equivalentes que transforman $A$ en una matriz triangular superior $U$, de modo que el sistema resultante pueda resolverse por sustitución regresiva.

\subsection{Eliminación gaussiana simple}

\paragraph{Planteamiento.}
Sea $A=(a_{ij})\in\mathbb{R}^{n\times n}$ y $b=(b_i)\in\mathbb{R}^n$.  
El sistema $Ax=b$ se expresa como:
\begin{eqnarray*}
\sum_{j=1}^n a_{ij}x_j=b_i,\qquad i=1,\dots,n.
\end{eqnarray*}

\paragraph{Etapas de eliminación.}
En el paso $k$ se anulan las entradas de la columna $k$ por debajo del pivote $a_{kk}^{(k)}$.  
Para cada $i=k+1,\dots,n$:
\begin{eqnarray*}
m_{ik}=\frac{a_{ik}^{(k)}}{a_{kk}^{(k)}},\qquad
a_{ij}^{(k+1)}=a_{ij}^{(k)}-m_{ik}\,a_{kj}^{(k)},\quad
b_i^{(k+1)}=b_i^{(k)}-m_{ik}\,b_k^{(k)}.
\end{eqnarray*}

\paragraph{Ejemplo simbólico $3\times 3$.}
\begin{eqnarray*}
A=\begin{bmatrix}
a_{11} & a_{12} & a_{13}\\
a_{21} & a_{22} & a_{23}\\
a_{31} & a_{32} & a_{33}
\end{bmatrix},\quad
b=\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 1 ($k=1$):}  
\begin{eqnarray*}
m_{21}=\tfrac{a_{21}}{a_{11}},\qquad m_{31}=\tfrac{a_{31}}{a_{11}}.
\end{eqnarray*}
\begin{eqnarray*}
\begin{aligned}
a_{22}^{(2)}&=a_{22}-m_{21}a_{12}, & a_{23}^{(2)}&=a_{23}-m_{21}a_{13}, & b_2^{(2)}&=b_2-m_{21}b_1,\\
a_{32}^{(2)}&=a_{32}-m_{31}a_{12}, & a_{33}^{(2)}&=a_{33}-m_{31}a_{13}, & b_3^{(2)}&=b_3-m_{31}b_1.
\end{aligned}
\end{eqnarray*}

\emph{Paso 2 ($k=2$):}  
\begin{eqnarray*}
m_{32}=\frac{a_{32}^{(2)}}{a_{22}^{(2)}},\quad
a_{33}^{(3)}=a_{33}^{(2)}-m_{32}a_{23}^{(2)},\quad
b_3^{(3)}=b_3^{(2)}-m_{32}b_2^{(2)}.
\end{eqnarray*}

Resultado: sistema triangular superior $Ux=c$.  

En el paso $k$ de la eliminación:
\begin{eqnarray*}
a_{ij}^{(k+1)}=a_{ij}^{(k)}-m_{ik}a_{kj}^{(k)},\qquad
b_i^{(k+1)}=b_i^{(k)}-m_{ik}b_k^{(k)},
\end{eqnarray*}
para $i=k+1,\dots,n$ y $j=k,\dots,n$, donde
\begin{eqnarray*}
m_{ik}=\frac{a_{ik}^{(k)}}{a_{kk}^{(k)}}.
\end{eqnarray*}

Es decir:
\begin{itemize}
  \item $m_{ik}$ mide cuántas veces la fila $k$ debe restarse a la fila $i$ para anular $a_{ik}$.
  \item Cada $m_{ik}$ se almacena en la matriz $L$.
\end{itemize}

\paragraph{Idea general.}
Transformar el sistema $Ax=b$ en uno triangular superior $Ux=c$ mediante operaciones elementales de filas, y resolver luego por sustitución regresiva.

\paragraph{Ejemplo.}
Resolver el sistema
\begin{eqnarray*}
\begin{cases}
2x+y-z=8,\\
-3x-y+2z=-11,\\
-2x+y+2z=-3.
\end{cases}
\end{eqnarray*}
Forma matricial:
\begin{eqnarray*}
\begin{bmatrix}
2&1&-1\\
-3&-1&2\\
-2&1&2
\end{bmatrix}
\begin{bmatrix}x\\y\\z\end{bmatrix}
=
\begin{bmatrix}8\\-11\\-3\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 1.} Pivote en $a_{11}=2$.  
Eliminamos en la primera columna:
\begin{eqnarray*}
m_{21}=\tfrac{-3}{2},\quad m_{31}=\tfrac{-2}{2}=-1.
\end{eqnarray*}
Se obtiene
\begin{eqnarray*}
\begin{bmatrix}
2 & 1 & -1 & | & 8\\
0 & 0.5 & 0.5 & | & 1\\
0 & 2 & 1 & | & 5
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 2.} Pivote en $a_{22}=0.5$.  
Eliminamos debajo:
\begin{eqnarray*}
m_{32}=\tfrac{2}{0.5}=4.
\end{eqnarray*}
\begin{eqnarray*}
\begin{bmatrix}
2 & 1 & -1 & | & 8\\
0 & 0.5 & 0.5 & | & 1\\
0 & 0 & -1 & | & 1
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 3.} Sustitución regresiva:
\begin{eqnarray*}
z=-1,\quad 0.5y+0.5z=1 \Rightarrow y=3,\quad 2x+y-z=8 \Rightarrow x=2.
\end{eqnarray*}
\begin{eqnarray*}
\boxed{x=2,\;y=3,\;z=-1}.
\end{eqnarray*}


\subsection{Eliminación con pivoteo y escalamiento}

Mismo proceso, pero en cada paso $k$ se intercambia la fila $k$ con la fila $p$ tal que
\begin{eqnarray*}
|a_{pk}^{(k)}|=\max_{i\geq k}|a_{ik}^{(k)}|.
\end{eqnarray*}
Esto asegura que $a_{kk}^{(k)}$ sea lo suficientemente grande.  


Antes de seleccionar pivote, cada fila $i$ se escala por
\begin{eqnarray*}
s_i=\max_j |a_{ij}|,
\end{eqnarray*}
y se elige como pivote aquel que maximiza
\begin{eqnarray*}
\frac{|a_{ik}^{(k)}|}{s_i}.
\end{eqnarray*}

\paragraph{Idea.}
Si en algún paso el pivote es $0$ o muy pequeño, se intercambia la fila pivote con otra fila inferior que tenga el mayor valor absoluto en la misma columna. Esto aumenta la estabilidad.

\paragraph{Ejemplo.}
\begin{eqnarray*}
\begin{bmatrix}
0 & 2 & 1 & | & 4\\
1 & -1 & 2 & | & 6\\
2 & 1 & -1 & | & 1
\end{bmatrix}.
\end{eqnarray*}
\emph{Problema:} $a_{11}=0$.  
\emph{Solución:} intercambiamos fila 1 con fila 2.  

Nuevo sistema:
\begin{eqnarray*}
\begin{bmatrix}
1 & -1 & 2 & | & 6\\
0 & 2 & 1 & | & 4\\
2 & 1 & -1 & | & 1
\end{bmatrix}.
\end{eqnarray*}

Ahora se procede con eliminación gaussiana simple. El pivoteo evita la división por cero.

\subsubsection{Eliminación con Pivoteo y Escalamiento}
\paragraph{Idea.}
Si los coeficientes varían mucho en magnitud, usar sólo pivoteo puede ser insuficiente.  
\emph{Escalamiento}: dividir cada fila por su elemento de mayor valor absoluto antes de seleccionar pivote.  
Así, se compara \(|a_{ik}|/s_i\), donde $s_i=\max_j |a_{ij}|$ es el factor de escala de la fila $i$.

\paragraph{Comentario.}
Esto evita que números muy grandes o muy pequeños enmascaren el pivote real, aumentando la precisión numérica en cómputo.

\subsubsection{Pivoteo y escalamiento}
\begin{itemize}
  \item \textbf{Pivoteo parcial:} intercambiar la fila $k$ con la fila $p$ tal que 
  \(|a_{pk}^{(k)}|=\max_{i\geq k}|a_{ik}^{(k)}|\).
  \item \textbf{Escalamiento:} definir factores $s_i=\max_j |a_{ij}|$ y escoger como pivote el mayor cociente
  \begin{eqnarray*}
  \frac{|a_{ik}^{(k)}|}{s_i}.
  \end{eqnarray*}
  Esto controla la propagación de errores de redondeo.
\end{itemize}


\subsection{Gauss--Jordan y cálculo de inversas}
\paragraph{Idea.}
Extender la eliminación hasta reducir $A$ a la matriz identidad. Si se parte de la matriz aumentada $(A|I)$, el resultado final es $(I|A^{-1})$.

Si extendemos la eliminación a toda la matriz (no solo a triangular superior), obtenemos:
\begin{eqnarray*}
A \;\longrightarrow\; I, \quad (A|I)\;\longrightarrow\;(I|A^{-1}).
\end{eqnarray*}

Algebraicamente:
\begin{eqnarray*}
A^{-1}=M^{(n-1)^{-1}}\cdots M^{(2)^{-1}}M^{(1)^{-1}},
\end{eqnarray*}
donde cada $M^{(k)}$ es la matriz elemental usada en la etapa $k$ de la eliminación.

\subsubsection{Gauss--Jordan e inversas}
En vez de detener la eliminación en forma triangular superior, se continúa hasta obtener la matriz identidad:
\begin{eqnarray*}
(A|I)\;\longrightarrow\;(I|A^{-1}).
\end{eqnarray*}
Así, cada paso se formaliza como multiplicación por matrices elementales:
\begin{eqnarray*}
A^{-1}=M_1^{-1}M_2^{-1}\cdots M_r^{-1}.
\end{eqnarray*}
\paragraph{Ejemplo.}
Calcular $A^{-1}$ con
\begin{eqnarray*}
A=\begin{bmatrix}
2 & 1\\
5 & 3
\end{bmatrix}.
\end{eqnarray*}
Matriz aumentada:
\begin{eqnarray*}
\begin{bmatrix}
2 & 1 & | & 1 & 0\\
5 & 3 & | & 0 & 1
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 1.} Pivote $2$. Normalizar fila 1:
\begin{eqnarray*}
\begin{bmatrix}
1 & 0.5 & | & 0.5 & 0\\
5 & 3 & | & 0 & 1
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 2.} Eliminar columna 1 de fila 2:
\begin{eqnarray*}
R_2 \leftarrow R_2-5R_1.
\end{eqnarray*}
\begin{eqnarray*}
\begin{bmatrix}
1 & 0.5 & | & 0.5 & 0\\
0 & 0.5 & | & -2.5 & 1
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 3.} Normalizar fila 2:
\begin{eqnarray*}
\begin{bmatrix}
1 & 0.5 & | & 0.5 & 0\\
0 & 1 & | & -5 & 2
\end{bmatrix}.
\end{eqnarray*}

\emph{Paso 4.} Eliminar columna 2 de fila 1:
\begin{eqnarray*}
R_1 \leftarrow R_1-0.5R_2.
\end{eqnarray*}
\begin{eqnarray*}
\begin{bmatrix}
1 & 0 & | & 3 & -1\\
0 & 1 & | & -5 & 2
\end{bmatrix}.
\end{eqnarray*}

\paragraph{Resultado.}
\begin{eqnarray*}
A^{-1}=
\begin{bmatrix}
3 & -1\\
-5 & 2
\end{bmatrix}.
\end{eqnarray*}

\subsection{Factorizaci\'on  $LU$}

La \textbf{factorización $LU$} consiste en descomponer una matriz cuadrada 
\begin{eqnarray*}A \in \mathbb{R}^{n \times n}\end{eqnarray*}
en el producto de dos matrices:
\begin{eqnarray}A = LU,\end{eqnarray}
donde:
\begin{itemize}
    \item $L$ es una matriz \textbf{triangular inferior} con unos en la diagonal.
    \item $U$ es una matriz \textbf{triangular superior}.
\end{itemize}

\begin{Note}
En algunos casos es necesario introducir una matriz de permutación $P$ para realizar el proceso de eliminación gaussiana de manera estable:
\begin{eqnarray}
PA = LU.
\end{eqnarray}
\end{Note}

\begin{eqnarray}
L=\begin{bmatrix}
1 & 0 & \cdots & 0\\
m_{21} & 1 & \cdots & 0\\
\vdots & \ddots & \ddots & \vdots\\
m_{n1} & \cdots & m_{n,n-1} & 1
\end{bmatrix},\end{eqnarray}

\begin{eqnarray*}
U=\text{matriz triangular superior obtenida tras la eliminación}.
\end{eqnarray*}

Por tanto:
\begin{eqnarray*}A=LU.\end{eqnarray*}

El procedimiento se basa en la eliminación gaussiana. Dada
\begin{eqnarray}
A = (a_{ij}) \in \mathbb{R}^{n \times n},
\end{eqnarray}
se definen los \textbf{multiplicadores}:
\begin{eqnarray}
m_{ij} = \frac{a_{ij}^{(k)}}{a_{kk}^{(k)}} \quad \text{para } i > k,
\end{eqnarray}
donde $a_{ij}^{(k)}$ denota la entrada de la matriz en la etapa $k$ de la eliminación.

Los pasos son:
\begin{enumerate}
    \item Inicialmente $L = I_n$ y $U = A$.
    \item Para cada paso $k = 1,2,\dots,n-1$:
    \begin{enumerate}
        \item Calcular los multiplicadores
        \begin{eqnarray}l_{ik} = \frac{u_{ik}}{u_{kk}}, \quad i = k+1, \dots, n.\end{eqnarray}
        \item Restar $l_{ik}$ veces la fila $k$ a la fila $i$ de $U$.
        \item Almacenar cada $l_{ik}$ en la posición correspondiente de $L$.
    \end{enumerate}
\end{enumerate}

De este modo se obtiene:
\begin{eqnarray}
A = LU, \qquad
L = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
l_{21} & 1 & 0 & \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
l_{n1} & \cdots & l_{n,n-1} & 1
\end{bmatrix}, \qquad
U = \begin{bmatrix}
u_{11} & u_{12} & \cdots & u_{1n} \\
0 & u_{22} & \cdots & u_{2n} \\
\vdots & \ddots & \ddots & \vdots \\
0 & \cdots & 0 & u_{nn}
\end{bmatrix}.
\end{eqnarray}

\begin{Note}
\begin{itemize}
    \item La factorización $LU$ es el corazón de la eliminación gaussiana.
    \item Permite resolver sistemas lineales $Ax = b$ mediante:
    \begin{eqnarray*}Ax = b \quad \Rightarrow \quad LUx = b.\end{eqnarray*}
    \item Se resuelve en dos etapas:
    \begin{eqnarray*}Ly = b, \qquad Ux = y,\end{eqnarray*}
    usando sustitución progresiva y regresiva.
\end{itemize}
\end{Note}

\begin{Note}
En los métodos directos como $LU$ o Cholesky, al factorizar la matriz $A$ en dos bloques (p.ej.\ $A=LU$ o $A=LL^\top$), la resolución de $Ax=b$ se divide en dos etapas fundamentales:

\begin{Note}[Sustitución hacia adelante (forward substitution).]
Se aplica cuando se resuelve un sistema triangular inferior:
\begin{eqnarray}
Ly=b,\qquad L=(\ell_{ij})\ \text{triangular inferior con $\ell_{ii}\neq 0$}.
\end{eqnarray}
El proceso es:
\begin{eqnarray}
y_1=\frac{b_1}{\ell_{11}},\qquad
y_i=\frac{1}{\ell_{ii}}\left(b_i-\sum_{j=1}^{i-1}\ell_{ij}y_j\right),\quad i=2,\dots,n.
\end{eqnarray}
\end{Note}

\begin{Note}[Sustitución hacia atrás (backward substitution).]
Se aplica cuando se resuelve un sistema triangular superior:
\begin{eqnarray}
Ux=y,\qquad U=(u_{ij})\ \text{triangular superior con $u_{ii}\neq 0$}.
\end{eqnarray}
El proceso es:
\begin{eqnarray}
x_n=\frac{y_n}{u_{nn}},\qquad
x_i=\frac{1}{u_{ii}}\left(y_i-\sum_{j=i+1}^n u_{ij}x_j\right),\quad i=n-1,\dots,1.
\end{eqnarray}
\end{Note}

\begin{Ejem}
Sea
\begin{eqnarray}
U=\begin{bmatrix}
u_{11} & u_{12} & u_{13}\\
0 & u_{22} & u_{23}\\
0 & 0 & u_{33}
\end{bmatrix},\qquad
y=\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}.
\end{eqnarray}
La sustitución hacia atrás da:
\begin{eqnarray}
x_3=\tfrac{y_3}{u_{33}},\quad
x_2=\tfrac{y_2-u_{23}x_3}{u_{22}},\quad
x_1=\tfrac{y_1-u_{12}x_2-u_{13}x_3}{u_{11}}.
\end{eqnarray}
\end{Ejem}
\end{Note}

\subsubsection{Método con permutaciones}

Si en algún paso se cumple $u_{kk} = 0$, el algoritmo falla. Para evitarlo se aplica \textbf{pivoteo parcial}: se intercambia la fila $k$ con otra fila $p > k$ tal que
\begin{eqnarray}
|u_{pk}| = \max_{i \geq k} |u_{ik}|.
\end{eqnarray}
Esto se representa mediante una matriz de permutación $P$, resultando:
\begin{eqnarray*}
PA = LU.
\end{eqnarray*}



\begin{Ejem} $A=LU$]
Sea
\begin{eqnarray*}
A=\begin{bmatrix}
2 & 3 & 1\\
4 & 7 & 7\\
-2 & 4 & 5
\end{bmatrix}.
\end{eqnarray*}
Aplicamos eliminación gaussiana guardando los multiplicadores en $L$ (con $1$'s en la diagonal).
\begin{itemize}
\item Paso $k=1$.
Pivote $u_{11}=2$. Multiplicadores:
\begin{eqnarray*}
\ell_{21}=\frac{4}{2}=2,\qquad \ell_{31}=\frac{-2}{2}=-1.
\end{eqnarray*}
Actualizamos filas de $U$:
\begin{eqnarray*}
R_2 \leftarrow R_2-2R_1,\quad R_3 \leftarrow R_3-(-1)R_1=R_3+R_1.
\end{eqnarray*}
Esto produce
\begin{eqnarray*}
U^{(1)}=\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 7 & 6
\end{bmatrix},\qquad
L^{(1)}=\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 0 & 1
\end{bmatrix}.
\end{eqnarray*}

\item Paso $k=2$.
Pivote $u_{22}=1$. Multiplicador:
\begin{eqnarray*}
\ell_{32}=\frac{7}{1}=7.
\end{eqnarray*}
Actualizamos fila 3 de $U$:
\begin{eqnarray*}
R_3 \leftarrow R_3-7R_2 \;\;\Longrightarrow\;\;
U=\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 0 & -29
\end{bmatrix}.
\end{eqnarray*}
Insertamos $\ell_{32}$ en $L$:
\begin{eqnarray*}
L=\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 7 & 1
\end{bmatrix}.
\end{eqnarray*}

\item Resultado. Se tiene
\begin{eqnarray*}
\boxed{
A=LU,\quad
L=\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 7 & 1
\end{bmatrix},\quad
U=\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 0 & -29
\end{bmatrix}.
}
\end{eqnarray*}
(Verificación rápida: $LU=A$ por multiplicación directa).
\end{itemize}


\end{Ejem}
\bigskip

\begin{Ejem} [$PA=LU$]
Sea
\begin{eqnarray*}
A=\begin{bmatrix}
0 & 2 & 1\\
1 & -2 & -3\\
-1 & 1 & 2
\end{bmatrix}.
\end{eqnarray*}
El pivote inicial $a_{11}=0$ es inválido; aplicamos \textbf{pivoteo parcial} intercambiando $R_1 \leftrightarrow R_2$.
La matriz de permutación que permuta las primeras dos filas es
\begin{eqnarray*}
P=\begin{bmatrix}
0 & 1 & 0\\
1 & 0 & 0\\
0 & 0 & 1
\end{bmatrix},\qquad
PA=\begin{bmatrix}
1 & -2 & -3\\
0 &  2 &  1\\
-1&  1 &  2
\end{bmatrix}.
\end{eqnarray*}
\begin{itemize}
\item Paso $k=1$ sobre $PA$.
Pivote $u_{11}=1$. Multiplicador:
\begin{eqnarray*}
\ell_{31}=\frac{-1}{1}=-1.
\end{eqnarray*}
Actualizamos fila 3:
\begin{eqnarray*}
R_3 \leftarrow R_3-(-1)R_1=R_3+R_1 \;\;\Longrightarrow\;\;
U^{(1)}=\begin{bmatrix}
1 & -2 & -3\\
0 &  2 &  1\\
0 & -1 & -1
\end{bmatrix},\quad
L^{(1)}=\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-1& 0 & 1
\end{bmatrix}.
\end{eqnarray*}

\item Paso $k=2$. Pivote $u_{22}=2$. Multiplicador:
\begin{eqnarray*}
\ell_{32}=\frac{-1}{2}=-\tfrac{1}{2}.
\end{eqnarray*}
Actualizamos fila 3:
\begin{eqnarray*}
R_3 \leftarrow R_3-(-\tfrac{1}{2})R_2=R_3+\tfrac{1}{2}R_2 \;\;\Longrightarrow\;\;
U=\begin{bmatrix}
1 & -2 & -3\\
0 &  2 &  1\\
0 &  0 & -\tfrac{1}{2}
\end{bmatrix}.
\end{eqnarray*}
Insertamos $\ell_{32}$ en $L$:
\begin{eqnarray*}
L=\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-1& -\tfrac{1}{2} & 1
\end{bmatrix}.
\end{eqnarray*}

\item Resultado. Se obtiene
\begin{eqnarray*}
\boxed{
PA=LU,\quad
P=\begin{bmatrix}
0 & 1 & 0\\
1 & 0 & 0\\
0 & 0 & 1
\end{bmatrix},\quad
L=\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
-1& -\tfrac{1}{2} & 1
\end{bmatrix},\quad
U=\begin{bmatrix}
1 & -2 & -3\\
0 &  2 &  1\\
0 &  0 & -\tfrac{1}{2}
\end{bmatrix}.
}
\end{eqnarray*}
(Comprobación: $LU=PA$ y, por tanto, $A=P^{\!\top}LU$ ya que $P^{-1}=P^{\!\top}$.)
\end{itemize}


\end{Ejem}

\begin{Ejem}Resolver $Ax=b$ vía $LU$
Consideremos
\begin{eqnarray*}
A=\begin{bmatrix}
2 & 3 & 1\\
4 & 7 & 7\\
-2 & 4 & 5
\end{bmatrix}, 
\qquad 
b=\begin{bmatrix}
1\\
2\\
3
\end{bmatrix}.
\end{eqnarray*}

Ya hemos obtenido la factorización
\begin{eqnarray*}
A=LU,\quad
L=\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 7 & 1
\end{bmatrix},\quad
U=\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 0 & -29
\end{bmatrix}.
\end{eqnarray*}

El sistema $Ax=b$ se resuelve en dos etapas:

\paragraph{1. Resolver $Ly=b$.}
\begin{eqnarray*}
\begin{bmatrix}
1 & 0 & 0\\
2 & 1 & 0\\
-1 & 7 & 1
\end{bmatrix}
\begin{bmatrix}
y_1\\y_2\\y_3
\end{bmatrix}
=
\begin{bmatrix}
1\\2\\3
\end{bmatrix}.
\end{eqnarray*}
De aquí:
\begin{eqnarray*}
y_1=1, \quad
2y_1+y_2=2 \;\Rightarrow\; y_2=0, \quad
-1\cdot y_1+7y_2+y_3=3 \;\Rightarrow\; y_3=4.
\end{eqnarray*}
Por lo tanto,
\begin{eqnarray*}
y=\begin{bmatrix}1\\0\\4\end{bmatrix}.
\end{eqnarray*}

\paragraph{2. Resolver $Ux=y$.}
\begin{eqnarray*}
\begin{bmatrix}
2 & 3 & 1\\
0 & 1 & 5\\
0 & 0 & -29
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2\\x_3
\end{bmatrix}
=
\begin{bmatrix}
1\\0\\4
\end{bmatrix}.
\end{eqnarray*}
De abajo hacia arriba:
\begin{eqnarray*}
-29x_3=4 \;\Rightarrow\; x_3=-\tfrac{4}{29},
\end{eqnarray*}
\begin{eqnarray*}
x_2+5x_3=0 \;\Rightarrow\; x_2=-5x_3=\tfrac{20}{29},
\end{eqnarray*}
\begin{eqnarray*}
2x_1+3x_2+x_3=1 \;\Rightarrow\; 2x_1+3\cdot\tfrac{20}{29}-\tfrac{4}{29}=1.
\end{eqnarray*}
\begin{eqnarray*}
2x_1+\tfrac{60-4}{29}=1 \;\Rightarrow\; 2x_1+\tfrac{56}{29}=1.
\end{eqnarray*}
\begin{eqnarray*}
2x_1=\tfrac{29}{29}-\tfrac{56}{29}=-\tfrac{27}{29} \;\Rightarrow\;
x_1=-\tfrac{27}{58}.
\end{eqnarray*}

\paragraph{Solución final:}
\begin{eqnarray*}
\boxed{
x=\begin{bmatrix}
-\tfrac{27}{58}\\begin{eqnarray*}6pt]
\tfrac{20}{29}\\begin{eqnarray*}6pt]
-\tfrac{4}{29}
\end{bmatrix}.
}
\end{eqnarray*}
\end{Ejem}


\subsubsection{Factorización de Cholesky}

Si $A$ es simétrica y definida positiva, entonces admite una factorización especial:
\begin{eqnarray*}
A = LL^\top,
\end{eqnarray*}
donde $L$ es triangular inferior con entradas reales y diagonales positivas.  
Esta factorización es más eficiente y estable que la $LU$ en estos casos.

Sea $A\in\mathbb{R}^{n\times n}$ simétrica definida positiva. La factorización de Cholesky busca
\begin{eqnarray*}
A = LL^\top,
\end{eqnarray*}
con $L$ triangular inferior y diagonal positiva. Las fórmulas recursivas son, para $i=1,\dots,n$:
\begin{eqnarray*}
\ell_{ii}=\sqrt{\,a_{ii}-\sum_{k=1}^{i-1}\ell_{ik}^2\,},\qquad
\ell_{ji}=\frac{1}{\ell_{ii}}\Bigl(a_{ji}-\sum_{k=1}^{i-1}\ell_{jk}\,\ell_{ik}\Bigr)\quad (j=i+1,\dots,n).
\end{eqnarray*}

\begin{Ejem}
Considérese
\begin{eqnarray*}
A=\begin{bmatrix}
4 & 2 & 2\\
2 & 2 & 1\\
2 & 1 & 2
\end{bmatrix}.
\end{eqnarray*}
Es simétrica y definida positiva (sus menores principales son positivos). Apliquemos las fórmulas:
\begin{itemize}
\item Columna $i=1$.
\begin{eqnarray*}
\ell_{11}=\sqrt{4}=2,\qquad
\ell_{21}=\frac{2}{2}=1,\qquad
\ell_{31}=\frac{2}{2}=1.
\end{eqnarray*}

\item Columna $i=2$.
\begin{eqnarray*}
\ell_{22}=\sqrt{\,2-\ell_{21}^2\,}=\sqrt{2-1}=1,\qquad
\ell_{32}=\frac{1-\ell_{31}\ell_{21}}{\ell_{22}}=\frac{1-1\cdot1}{1}=0.
\end{eqnarray*}

\item Columna $i=3$.
\begin{eqnarray*}
\ell_{33}=\sqrt{\,2-\ell_{31}^2-\ell_{32}^2\,}=\sqrt{2-1-0}=1.
\end{eqnarray*}

\item Resultado.
\begin{eqnarray*}
L=\begin{bmatrix}
2&0&0\\
1&1&0\\
1&0&1
\end{bmatrix},\qquad
\boxed{A=LL^\top}.
\end{eqnarray*}
(Verificación rápida: $LL^\top=\begin{bmatrix}4&2&2\\2&2&1\\2&1&2\end{bmatrix}$.)

\end{itemize}
\end{Ejem}
\bigskip

\begin{Ejem}
Considérese
\begin{eqnarray*}
A=\begin{bmatrix}
9 & 3 & 6\\
3 & 5 & 3\\
6 & 3 & 14
\end{bmatrix}.
\end{eqnarray*}
También es simétrica definida positiva. Procedamos:
\begin{itemize}
\item Columna $i=1$.
\begin{eqnarray*}
\ell_{11}=\sqrt{9}=3,\qquad
\ell_{21}=\frac{3}{3}=1,\qquad
\ell_{31}=\frac{6}{3}=2.
\end{eqnarray*}

\item Columna $i=2$.
\begin{eqnarray*}
\ell_{22}=\sqrt{\,5-\ell_{21}^2\,}=\sqrt{5-1}=2,\qquad
\ell_{32}=\frac{3-\ell_{31}\ell_{21}}{\ell_{22}}=\frac{3-2\cdot1}{2}=\tfrac{1}{2}.
\end{eqnarray*}

\item Columna $i=3$.
\begin{eqnarray*}
\ell_{33}=\sqrt{\,14-\ell_{31}^2-\ell_{32}^2\,}
=\sqrt{\,14-4-\tfrac{1}{4}\,}
=\sqrt{\tfrac{39}{4}}=\frac{\sqrt{39}}{2}.
\end{eqnarray*}

\item Resultado.
\begin{eqnarray*}
L=\begin{bmatrix}
3&0&0\\
1&2&0\\
2&\tfrac{1}{2}&\tfrac{\sqrt{39}}{2}
\end{bmatrix},\qquad
\boxed{A=LL^\top}.
\end{eqnarray*}
(Verificación: $LL^\top$ reproduce $A$; la última diagonal verifica $2^2+(\tfrac{1}{2})^2+(\tfrac{\sqrt{39}}{2})^2=14$.)
\end{itemize}
\end{Ejem}

\begin{itemize}
  \item \textbf{Forma:} $A=LL^\top$, con $L$ triangular inferior y diagonal positiva.
  \item \textbf{Requisitos:} $A$ debe ser \textbf{simétrica definida positiva (SDP)}.
  \item \textbf{Pivoteo:} No requiere pivoteo si $A$ es SDP (bien condicionada).
  \item \textbf{Ventajas:} Más rápida y estable (sin pivoteo) para matrices SDP; mejor aprovechamiento de memoria (simetría).
  \item \textbf{Cuándo usarla:} Sistemas simétricos definidos positivos (p.ej.\ matrices de Gram, problemas de mínimos cuadrados regulares, discretizaciones elípticas).
\end{itemize}

\begin{Note}Si $A$ es SDP, use Cholesky. En caso general, use $LU$ con pivoteo.
\end{Note}


\subsubsection{Factorización de Doolittle}

Es una variante de la factorización $LU$ en la cual la matriz $L$ tiene $1$'s en la diagonal principal:
\begin{eqnarray*}
A = LU, \quad L \text{ con unos en la diagonal}, \; U \text{ triangular superior}.
\end{eqnarray*}

\begin{itemize}
  \item \textbf{Forma:} $A=LU$, con $L$ triangular inferior con $1$ en la diagonal y $U$ triangular superior.
  \item \textbf{Requisitos:} Válida para matrices cuadradas no singulares (en la práctica, puede requerir permutaciones: $PA=LU$).
  \item \textbf{Pivoteo:} Suele emplear \emph{pivoteo parcial} para estabilidad numérica.
  \item \textbf{Cuándo usarla:} Sistemas generales (no necesariamente simétricos ni definidos positivos), múltiples RHS $b$ con la misma $A$.
\end{itemize}


\begin{Note}[Esquema general]
Si $A=LL^\top$ con $L$ triangular inferior y diagonal positiva, resolver $Ax=b$ equivale a:
\begin{eqnarray*}
Ly=b \quad\text{(sustitución progresiva)}, \qquad
L^\top x=y \quad\text{(sustitución regresiva)}.
\end{eqnarray*}
\end{Note}

\begin{Ejem}
Sea
\begin{eqnarray*}
A=\begin{bmatrix}
4 & 2 & 2\\
2 & 2 & 1\\
2 & 1 & 2
\end{bmatrix}, 
\qquad
b=\begin{bmatrix}1\\2\\3\end{bmatrix}.
\end{eqnarray*}
Su factorización de Cholesky es
\begin{eqnarray*}
L=\begin{bmatrix}
2&0&0\\
1&1&0\\
1&0&1
\end{bmatrix}
\quad\Longrightarrow\quad
A=LL^\top.
\end{eqnarray*}
\begin{itemize}

\item Paso 1: $Ly=b$.

\begin{eqnarray*}
\begin{bmatrix}
2&0&0\\
1&1&0\\
1&0&1
\end{bmatrix}
\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}
=
\begin{bmatrix}1\\2\\3\end{bmatrix}
\end{eqnarray*}
\begin{eqnarray*}
2y_1=1 \ \Rightarrow\ y_1=\tfrac{1}{2},\\
y_1+y_2=2 \ \Rightarrow\ y_2=\tfrac{3}{2},\\
y_1+y_3=3 \ \Rightarrow\ y_3=\tfrac{5}{2}.
\end{eqnarray*}
Por tanto, $y=\left[\tfrac{1}{2},\ \tfrac{3}{2},\ \tfrac{5}{2}\right]^\top$.

\item Paso 2: $L^\top x=y$.
\begin{eqnarray*}
L^\top=\begin{bmatrix}
2&1&1\\
0&1&0\\
0&0&1
\end{bmatrix},\quad
\end{eqnarray*}

\begin{eqnarray*}
\begin{bmatrix}
2&1&1\\
0&1&0\\
0&0&1
\end{bmatrix}
\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}
=
\begin{bmatrix}\tfrac{1}{2}\\
\tfrac{3}{2}\\
\tfrac{5}{2}
\end{bmatrix}
\Rightarrow
\begin{cases}
x_3=\tfrac{5}{2},\\
x_2=\tfrac{3}{2},\\
2x_1+x_2+x_3=\tfrac{1}{2}\ \Rightarrow\ 2x_1=\tfrac{1}{2}-\tfrac{3}{2}-\tfrac{5}{2}=-\tfrac{7}{2}\ \Rightarrow\ x_1=-\tfrac{7}{4}.
\end{cases}
\end{eqnarray*}

\begin{eqnarray*}
\boxed{
x=\begin{bmatrix}
-\tfrac{7}{4}\\
\tfrac{3}{2}\\
\tfrac{5}{2}
\end{bmatrix}}
\end{eqnarray*}
\end{itemize}
\end{Ejem}


\begin{Ejem}
Sea
\begin{eqnarray*}
A=\begin{bmatrix}
9 & 3 & 6\\
3 & 5 & 3\\
6 & 3 & 14
\end{bmatrix}, 
\qquad
b=\begin{bmatrix}1\\2\\3\end{bmatrix}.
\end{eqnarray*}
Una factorización de Cholesky es
\begin{eqnarray*}
L=\begin{bmatrix}
3&0&0\\
1&2&0\\
2&\tfrac{1}{2}&\tfrac{\sqrt{39}}{2}
\end{bmatrix}
\quad\Longrightarrow\quad
A=LL^\top.
\end{eqnarray*}

\begin{itemize}
\item Paso 1: $Ly=b$.

\begin{eqnarray*}
\begin{bmatrix}
3&0&0\\
1&2&0\\
2&\tfrac{1}{2}&\tfrac{\sqrt{39}}{2}
\end{bmatrix}
\begin{bmatrix}y_1\\y_2\\y_3\end{bmatrix}
=
\begin{bmatrix}1\\2\\3\end{bmatrix}
\end{eqnarray*}

\begin{eqnarray*}
3y_1&=&1 \ \Rightarrow\ y_1=\tfrac{1}{3},\\
y_1+2y_2&=&2 \ \Rightarrow\ y_2=\tfrac{2-\tfrac{1}{3}}{2}=\tfrac{5}{6},\\
2y_1+\tfrac{1}{2}y_2+\tfrac{\sqrt{39}}{2}y_3&=&3 
\ \Rightarrow\ 
\tfrac{\sqrt{39}}{2}y_3=3-\tfrac{2}{3}-\tfrac{5}{12}=\tfrac{23}{12}\\
y_3&=&\frac{23}{12}\cdot\frac{2}{\sqrt{39}}=\frac{23}{6\sqrt{39}}.
\end{eqnarray*}

Por tanto, $y=\bigl[\tfrac{1}{3},\ \tfrac{5}{6},\ \tfrac{23}{6\sqrt{39}}\bigr]^\top$.

\item Paso 2: $L^\top x=y$.

\begin{eqnarray*}
L^\top=\begin{bmatrix}
3&1&2\\
0&2&\tfrac{1}{2}\\
0&0&\tfrac{\sqrt{39}}{2}
\end{bmatrix},
\end{eqnarray*}


\begin{eqnarray*}
\begin{bmatrix}
3&1&2\\
0&2&\tfrac{1}{2}\\
0&0&\tfrac{\sqrt{39}}{2}
\end{bmatrix}
\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}
=
\begin{bmatrix}\tfrac{1}{3}\\
\tfrac{5}{6}\\
\tfrac{23}{6\sqrt{39}}\end{bmatrix}
\Rightarrow
\begin{cases}
\tfrac{\sqrt{39}}{2}\,x_3=\tfrac{23}{6\sqrt{39}} \ \Rightarrow\ x_3=\tfrac{23}{117},\\
2x_2+\tfrac{1}{2}x_3=\tfrac{5}{6} \ \Rightarrow\ 2x_2=\tfrac{5}{6}-\tfrac{1}{2}\cdot\tfrac{23}{117}=\tfrac{86}{117} \ \Rightarrow\ x_2=\tfrac{43}{117},\\
3x_1+x_2+2x_3=\tfrac{1}{3}=\tfrac{39}{117} \ \Rightarrow\ 3x_1=\tfrac{39-43-46}{117}=-\tfrac{50}{117} \ \Rightarrow\\\
x_1=-\tfrac{50}{351}.
\end{cases}
\end{eqnarray*}

\begin{eqnarray*}
\boxed{
x=\begin{bmatrix}
-\tfrac{50}{351}\\
\tfrac{43}{117}\\
\tfrac{23}{117}
\end{bmatrix}}
\end{eqnarray*}
\end{itemize}
\end{Ejem}


\begin{Ejem}
\begin{eqnarray*}
A=\begin{pmatrix}
2 & -1 & 3\\
0 & 4 & 5\\
0 & 0 & -2
\end{pmatrix},\qquad
L=\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}=L^{T},\qquad
U=\begin{pmatrix}
2 & -1 & 3\\
0 & 4 & 5\\
0 & 0 & -2
\end{pmatrix}.
\end{eqnarray*}

\begin{eqnarray*}
A = LU = I \cdot U = U, \quad \text{y } L=L^{T}.
\end{eqnarray*}
\end{Ejem}


\begin{Ejem}
\begin{eqnarray*}
\text{Tomemos } 
L=\begin{pmatrix}
2 & 0 & 0\\
0 & 3 & 0\\
0 & 0 & 4
\end{pmatrix}
\quad\text{(simétrica y triangular inferior, }L=L^{T}\text{),}
\end{eqnarray*}

\begin{eqnarray*}
U=\begin{pmatrix}
1 & -1 & 2\\
0 & 2  & 5\\
0 & 0  & -3
\end{pmatrix}\ \text{(triangular superior).}
\end{eqnarray*}

\begin{eqnarray*}
A=LU=
\begin{pmatrix}
2 & 0 & 0\\
0 & 3 & 0\\
0 & 0 & 4
\end{pmatrix}
\begin{pmatrix}
1 & -1 & 2\\
0 & 2  & 5\\
0 & 0  & -3
\end{pmatrix}
=
\begin{pmatrix}
2 & -2 & 4\\
0 & 6  & 15\\
0 & 0  & -12
\end{pmatrix}.
\end{eqnarray*}
\end{Ejem}



\begin{Ejem}
\begin{eqnarray*}
A=\begin{pmatrix}
2 & -1 & 1 & 3\\
4 &  1 & 0 & 2\\
-2 & 2 & 5 & 1\\
6 & 0 & 2 & 4
\end{pmatrix}
\end{eqnarray*}

\begin{eqnarray*}
L=\begin{pmatrix}
1 & 0 & 0 & 0\\
2 & 1 & 0 & 0\\
-1 & 1 & 1 & 0\\
3 & -1 & 0 & 1
\end{pmatrix}, \qquad
U=\begin{pmatrix}
2 & -1 & 1 & 3\\
0 & 3 & -2 & -4\\
0 & 0 & 6 & 3\\
0 & 0 & 0 & -2
\end{pmatrix}.
\end{eqnarray*}

\end{Ejem}

\subsection{Descomposición de $A$}

Sea $A=D+L+U$, con:
\begin{eqnarray*}
D&=&\text{diag}(a_{11},\dots,a_{nn}),\\
L&=&\text{parte estrictamente triangular inferior},\\
U&=&\text{parte estrictamente triangular superior}.
\end{eqnarray*}

De $Ax=b$ obtenemos:
\begin{eqnarray}
x = -D^{-1}(L+U)x + D^{-1}b,
\end{eqnarray}
lo que sugiere la iteración:
\begin{eqnarray}
x^{(k+1)}&=&Tx^{(k)}+c,\\
T&=&-D^{-1}(L+U),\\
c&=&D^{-1}b.
\end{eqnarray}

\subsubsection{Método de Jacobi}

\begin{eqnarray}
x_i^{(k+1)}=\frac{1}{a_{ii}}\Biggl(b_i-\sum_{\substack{j=1\\j\neq i}}^n a_{ij}x_j^{(k)}\Biggr).
\end{eqnarray}
Se calcula cada $x_i^{(k+1)}$ sólo con valores de la iteración anterior.

En forma matricial:
\begin{eqnarray}
T_J=-D^{-1}(L+U),\quad c_J=D^{-1}b.
\end{eqnarray}



\begin{Ejem}
\begin{eqnarray*}
\begin{cases}
a_{11}x_1+a_{12}x_2+a_{13}x_3=b_1,\\
a_{21}x_1+a_{22}x_2+a_{23}x_3=b_2,\\
a_{31}x_1+a_{32}x_2+a_{33}x_3=b_3.
\end{cases}
\end{eqnarray*}
Iteración:
\begin{eqnarray*}
x_1^{(k+1)}&=&\tfrac{1}{a_{11}}(b_1-a_{12}x_2^{(k)}-a_{13}x_3^{(k)}),\\
x_2^{(k+1)}&=&\tfrac{1}{a_{22}}(b_2-a_{21}x_1^{(k)}-a_{23}x_3^{(k)}),\\
&\dots&
\end{eqnarray*}
\end{Ejem}


\subsubsection{Método de Gauss--Seidel}

Aprovecha los valores nuevos tan pronto como se calculan:

\begin{eqnarray}
x^{(k+1)}=(D+L)^{-1}\bigl(-Ux^{(k)}+b\bigr).
\end{eqnarray}

Por componentes:
\begin{eqnarray}
x_i^{(k+1)}=\frac{1}{a_{ii}}\left(b_i-\sum_{j=1}^{i-1} a_{ij}x_j^{(k+1)}-\sum_{j=i+1}^n a_{ij}x_j^{(k)}\right).
\end{eqnarray}


\begin{Note}[Condiciones de convergencia]
Ambos métodos convergen si el radio espectral de la matriz de iteración es menor que uno:
\begin{eqnarray*}
\rho(T)<1.
\end{eqnarray*}
Casos suficientes:
\begin{itemize}
  \item $A$ diagonalmente dominante $\;\Rightarrow\;$ Jacobi y Gauss--Seidel convergen.
  \item $A$ simétrica definida positiva $\;\Rightarrow\;$ Gauss--Seidel converge.
\end{itemize}

\begin{itemize}
  \item Convergencia $\iff \rho(T)<1$, donde $\rho$ es el radio espectral.
  \item Suficiente: $A$ diagonalmente dominante estricta $\Rightarrow$ Jacobi y GS convergen.
  \item Suficiente: $A$ simétrica definida positiva $\Rightarrow$ GS siempre converge.
\end{itemize}
\end{Note}


\subsubsection{Método SOR (Successive Over-Relaxation)}

Generaliza Gauss--Seidel:
\begin{eqnarray*}
x_i^{(k+1)}=(1-\omega)x_i^{(k)}+
\frac{\omega}{a_{ii}}\left(b_i-\sum_{j<i}a_{ij}x_j^{(k+1)}-\sum_{j>i}a_{ij}x_j^{(k)}\right).
\end{eqnarray*}

donde
\begin{itemize}
  \item $\omega=1$ reproduce Gauss--Seidel.
  \item $1<\omega<2$: \emph{sobrerrelajación}, posible aceleración.
  \item $0<\omega<1$: \emph{sub-relajación}, usada en algunos problemas mal condicionados.
\end{itemize}

Reescribiendo $Ax=b$:
\begin{eqnarray}
x&=&Tx+c,\\
T&=&-D^{-1}(L+U),\\
c&=&D^{-1}b.
\end{eqnarray}
De aquí nace el esquema iterativo:
\begin{eqnarray}
x^{(k+1)}=Tx^{(k)}+c.
\end{eqnarray}



\begin{Note}
\begin{eqnarray*}
T_J &=& -D^{-1}(L+U),  c_J=D^{-1}b,\\
T_{GS}&=&-(D+L)^{-1}U,  c_{GS}=(D+L)^{-1}b,\\
T_{SOR}&=&(D+\omega L)^{-1}\big((1-\omega)D-\omega U\big), c_{SOR}=\omega(D+\omega L)^{-1}b.
\end{eqnarray*}
\end{Note}





\subsubsection{Descomposición de la matriz}
Sea $A\in\mathbb{R}^{n\times n}$. Se descompone como
\begin{eqnarray*}
A = D - L - U,
\end{eqnarray*}
donde:
\begin{itemize}
  \item $D$: matriz diagonal de $A$,
  \item $-L$: parte estrictamente triangular inferior,
  \item $-U$: parte estrictamente triangular superior.
\end{itemize}
Así,
\begin{eqnarray*}
A = D + L + U, \quad
Ax=b \;\;\Longleftrightarrow\;\; (D+L+U)x=b.
\end{eqnarray*}

\subsubsection{Método de Jacobi}
A partir de la descomposición, se obtiene el esquema iterativo de Jacobi:
\begin{eqnarray*}
x^{(k+1)} = D^{-1}\bigl(b - (L+U)x^{(k)}\bigr).
\end{eqnarray*}
De manera componente a componente:
\begin{eqnarray*}
x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{\substack{j=1 \\ j\neq i}}^n a_{ij} x_j^{(k)}\right), 
\quad i=1,\dots,n.
\end{eqnarray*}
\begin{itemize}
  \item Cada componente $x_i^{(k+1)}$ se calcula usando exclusivamente valores de la iteración previa $x^{(k)}$.
  \item Es fácil de implementar en paralelo.
\end{itemize}

\subsubsection{Método de Gauss--Seidel}
En Gauss--Seidel se aprovecha que, en cada paso, los nuevos valores calculados pueden usarse de inmediato:
\begin{eqnarray*}
x^{(k+1)} = (D+L)^{-1}\bigl(b - Ux^{(k)}\bigr).
\end{eqnarray*}
En forma componente:
\begin{eqnarray*}
x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)}\right).
\end{eqnarray*}
\begin{itemize}
  \item La diferencia clave con Jacobi: $x_j^{(k+1)}$ ya calculados se usan de inmediato.
  \item Suele converger más rápido que Jacobi.
\end{itemize}

\subsubsection{Condiciones de convergencia}
Un resultado clásico:
\begin{itemize}
  \item Si $A$ es \textbf{diagonal dominante estricta} (es decir, $|a_{ii}| > \sum_{j\neq i}|a_{ij}|$ para todo $i$), entonces Jacobi y Gauss--Seidel convergen.
  \item Si $A$ es \textbf{simétrica definida positiva}, Gauss--Seidel siempre converge.
  \item En general, la convergencia depende de que el radio espectral de la matriz de iteración sea menor que $1$:
  \begin{eqnarray*}
  \rho(T)<1.
  \end{eqnarray*}
\end{itemize}

\begin{Note}[Esquema general] Se procede de la siguiente manera:

\begin{enumerate}
  \item Elegir un vector inicial $x^{(0)}$ (p.ej.\ el vector nulo).
  \item Repetir hasta convergencia:
  \begin{enumerate}
    \item Calcular $x^{(k+1)}$ según el método elegido (Jacobi o Gauss--Seidel).
    \item Medir el error, p.ej.\ $\|x^{(k+1)}-x^{(k)}\|$.
    \item Detenerse cuando el error sea menor que una tolerancia $\varepsilon$.
  \end{enumerate}
\end{enumerate}
\end{Note}

\begin{Ejem} Sistema de prueba (diagonalmente dominante)
Consideremos
\begin{eqnarray*}
A=
\begin{bmatrix}
10 & -1 & 2 & 0\\
-1 & 11 & -1 & 3\\
2 & -1 & 10 & -1\\
0 & 3 & -1 & 8
\end{bmatrix},\qquad
b=\begin{bmatrix}6\\25\\-11\\15\end{bmatrix},
\end{eqnarray*}
con vector inicial \(x^{(0)}=\mathbf{0}\).
Este sistema es \emph{diagonalmente dominante}, por lo que Jacobi y Gauss--Seidel convergen.

La solución exacta es
\begin{eqnarray*}
x^\ast=\begin{bmatrix}1\\
2\\
-1\\
1\end{bmatrix}.
\end{eqnarray*}
\end{Ejem}


\begin{Ejem} [Método de Jacobi]
Recordemos que
\begin{eqnarray*}
x^{(k+1)}=D^{-1}\bigl(b-(L+U)\,x^{(k)}\bigr),\qquad
x_i^{(k+1)}=\frac{1}{a_{ii}}\!\left(b_i-\sum_{j\neq i}a_{ij}\,x_j^{(k)}\right).
\end{eqnarray*}


\begin{eqnarray*}
x^{(0)}=\begin{bmatrix}0\\0\\0\\0\end{bmatrix},\quad
x^{(1)}=\begin{bmatrix}
0.6\\ 2.272727\\ -1.100000\\ 1.875000
\end{bmatrix},\quad
x^{(2)}=\begin{bmatrix}
1.047273\\ 1.715909\\ -0.805227\\ 0.885227
\end{bmatrix},\quad
x^{(3)}=\begin{bmatrix}
0.932636\\ 2.053306\\ -1.049341\\ 1.130881
\end{bmatrix}.
\end{eqnarray*}

Errores (norma infinito)
\begin{eqnarray*}
\|x^{(0)}-x^\ast\|_\infty=2.000000,\\
\|x^{(1)}-x^\ast\|_\infty=0.875000,\\
\|x^{(2)}-x^\ast\|_\infty=0.284091,\\
\|x^{(3)}-x^\ast\|_\infty=0.130881.
\end{eqnarray*}
\end{Ejem}

\begin{Ejem}[Método de Gauss--Seidel]
\begin{eqnarray*}
x^{(k+1)}&=&(D+L)^{-1}\bigl(b-U\,x^{(k)}\bigr),\\
x_i^{(k+1)}&=&\frac{1}{a_{ii}}\!\left(b_i-\sum_{j<i}a_{ij}\,x_j^{(k+1)}-\sum_{j>i}a_{ij}\,x_j^{(k)}\right).
\end{eqnarray*}

Iteraciones (redondeadas a 6 decimales).
\begin{eqnarray*}
x^{(0)}=\begin{bmatrix}0\\0\\0\\0\end{bmatrix},\quad
x^{(1)}=\begin{bmatrix}
0.6\\ 2.327273\\ -0.987273\\ 0.878864
\end{bmatrix},\quad
x^{(2)}=\begin{bmatrix}
1.030182\\ 2.036938\\ -1.014456\\ 0.984341
\end{bmatrix},\quad
x^{(3)}=\begin{bmatrix}
1.006585\\ 2.003555\\ -1.002527\\ 0.998351
\end{bmatrix}.
\end{eqnarray*}

Errores (norma infinito) frente a \(x^\ast\).
\begin{eqnarray*}
\|x^{(0)}-x^\ast\|_\infty=2.000000,\\
\|x^{(1)}-x^\ast\|_\infty=0.400000,\\
\|x^{(2)}-x^\ast\|_\infty=0.036938,\\
\|x^{(3)}-x^\ast\|_\infty=0.006585.
\end{eqnarray*}
\end{Ejem}
\begin{Note}
\begin{itemize}
  \item Ambos métodos convergen (la matriz es diagonalmente dominante), pero \textbf{Gauss--Seidel} reduce el error notablemente más rápido al reutilizar los valores nuevos dentro de la misma iteración.
  \item Criterio de paro típico: detener cuando \(\|x^{(k+1)}-x^{(k)}\| \le \varepsilon\) o \(\|Ax^{(k)}-b\| \le \varepsilon\).
  \item Para paralelización masiva, \textbf{Jacobi} es más simple; para rapidez en CPU única, \textbf{Gauss--Seidel} suele ser preferible.
\end{itemize}
\end{Note}

\begin{Ejem}[SOR en una sola iteración (comparación con Gauss--Seidel)]\medskip

Consideremos el mismo sistema diagonalmente dominante:
\begin{eqnarray*}
A=
\begin{bmatrix}
10 & -1 & 2 & 0\\
-1 & 11 & -1 & 3\\
2 & -1 & 10 & -1\\
0 & 3 & -1 & 8
\end{bmatrix},
\qquad
b=\begin{bmatrix}6\\25\\-11\\15\end{bmatrix},
\qquad
x^{(0)}=\mathbf{0}.
\end{eqnarray*}

con $\omega=1.2$
\begin{eqnarray*}
x_i^{(k+1)}=(1-\omega)\,x_i^{(k)}
+\frac{\omega}{a_{ii}}\!\left(b_i-\sum_{j<i}a_{ij}\,x_j^{(k+1)}-\sum_{j>i}a_{ij}\,x_j^{(k)}\right).
\end{eqnarray*}

Una iteración desde $x^{(0)}=\mathbf{0}$ (redondeado a $6$ decimales).
\begin{eqnarray*}
x_{\text{SOR}}^{(1)}=
\begin{bmatrix}
0.720000\\
2.805818\\
-1.156102\\
0.813967
\end{bmatrix}.
\end{eqnarray*}

Gauss--Seidel, una iteración
\begin{eqnarray*}
x_{\text{GS}}^{(1)}=
\begin{bmatrix}
0.600000\\
2.327273\\
-0.987273\\
0.878864
\end{bmatrix}.
\end{eqnarray*}


\begin{itemize}
  \item Con $\omega>1$ (sobrerrelajación), SOR puede \emph{acelerar} la convergencia, pero la \textbf{primera} iteración puede mostrar un “sobre-disparo” respecto a la solución exacta.
  \item La elección de $\omega$ es clave: en la práctica se prueba $\omega\in[1.1,1.5]$ y se observa el comportamiento; si $\omega$ es muy grande, puede empeorar o incluso hacer divergir el método.
  \item Si $A$ es SPD, métodos como \textbf{Cholesky} (directo) o iterativos tipo \textbf{Gradiente Conjugado} suelen ser preferibles por estabilidad y rapidez.
\end{itemize}
\end{Ejem}

\begin{Note} [Esquema general]
Dado $Ax=b$ y un vector inicial $x^{(0)}$, el método SOR genera la sucesión:
\begin{eqnarray*}
x_i^{(k+1)} = (1-\omega)\,x_i^{(k)} + 
\frac{\omega}{a_{ii}}
\left(b_i - \sum_{j<i} a_{ij} x_j^{(k+1)} - \sum_{j>i} a_{ij} x_j^{(k)}\right),
\quad i=1,\dots,n.
\end{eqnarray*}

\begin{itemize}
  \item Si $\omega=1$, se recupera exactamente el método de Gauss--Seidel.
  \item Si $0<\omega<1$, se llama \emph{relajación sub-sucessiva}, útil para estabilizar ciertos casos.
  \item Si $1<\omega<2$, se llama \emph{sobrerrelajación}, y puede acelerar notablemente la convergencia.
  \item La elección óptima de $\omega$ depende de la matriz $A$; en la práctica, se prueba con valores como $\omega\approx 1.1$ a $1.5$.
  \item El método es útil en problemas grandes, como los provenientes de discretización de ecuaciones diferenciales.
\end{itemize}

\begin{itemize}
  \item Jacobi: usa valores viejos.  
  \item Gauss--Seidel: usa valores nuevos en cuanto están disponibles.  
  \item SOR: Gauss--Seidel + parámetro $\omega$ que acelera la convergencia si se elige bien.
\end{itemize}
\end{Note}

%===========================================
\subsection{Ejercicios SEL}
%===========================================



\begin{Ejer}
Resolver por eliminación Gaussiana Simple los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
x_1-2x_2+0.5x_3&=&-5\\
-2x_1+5x_{2}-1.5x_3&=&0\\
-0.2x_1+1.75x_2-x_3&=&10
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-x_2+6x_4&=&2.3\\
4x_1+2x_2-x_3-5x_4&=&6.9\\
-5x_1+x_2-3x_3&=&-36\\
10x_2-4x_3+7x_4&=&-36
\end{eqnarray*}

\item \begin{eqnarray*}
0.003000x_1+59.14x_2&=&59.17\\
5.291x_1-6.130x_2&=&46.78
\end{eqnarray*}
utilizar redondeo a 4 cifras significativas.

\item \begin{eqnarray*}
4x_1+2x_2&=&2\\
2x_1+3x_2+x_3&=&-1\\
x_2+\frac{5}{2}x_3&=&3
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.1x_2-0.2x_3&=&7.85\\
0.1x_{1}+7x_2-0.3x_3=-19.3\\
0.3x_1-0.2x_2+10x_3&=&71.4
\end{eqnarray*}

\item \begin{eqnarray*}
8x_1+2x_2-2x_3&=&-2\\
10x_1+2x_2+4x_3&=&4\\
12x_1+2x_2+2x_3&=&6
\end{eqnarray*}

\item \begin{eqnarray*}
5x_1 + 2x_2 + x_3 - x_4 = 1\\
2x_1 - x_2 + 3x_3 + 2x_4 = 12\\
4x_1 + x_2 - 2x_3 + 3x_4 = 5\\
-2x_1 + 2x_2 + x_3 + x_4 = 2
\end{eqnarray*}

\item \begin{eqnarray*}
2x_1+3x_2+2x_3+4x_4&=&4\\
4x_1+10x_2-4x_3&=&-8\\
-3x_1-2x_2-5x_3-2x_4&=&-4\\
-2x_1+4x_2+4x_3-7x_4&=&-1
\end{eqnarray*}

\item \begin{eqnarray*}
1.133x_1+5.281x_2-2.454x_3&=&6.414\\
24.14x_1-1.21x_2+5.281x_3&=&113.8\\
-10.123x_1+6.387x_2-x_3&=&1
\end{eqnarray*}

\item $A=\left(\begin{array}{ccccc}
2 & 3 & 2 & 4\\
4 & 10 &-4 & 0\\
-3 & -2 & -5 &-2\\
-2 & 4 & 4 &-7\\
\end{array}\right)$ y $b=\left(\begin{array}{c}\\
9 \\
-15\\
6 \\
2\\
\end{array}\right)$

\end{enumerate}
\end{Ejer}

\begin{Ejer}
Resolver por eliminación gaussiana con pivoteo parcial los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
0.4x_1-1.5x_2+0.75x_3&=&-20\\
-0.5x_1-15x_2+10x_3&=&-10\\
-10x_1-9x_2+2.5x_3&=&30
\end{eqnarray*}

\item \begin{eqnarray*}
5x_1-8x_2+x_3&=&-71\\
-2x_1+6x_2-9x_3&=&134\\
3x_1-5x_2+2x_3&=&-58
\end{eqnarray*}

\item \begin{eqnarray*}
0.003000x_1+59.14x_2&=&59.17\\
5.291x_1-6.130x_2&=&46.78
\end{eqnarray*}
utilizar redondeo a 4 cifras significativas.


\item \begin{eqnarray*}
-x_2+4x_3-x_4&=&-1\\
-x_1+4x_2-x_3&=&2\\
-x_1-x_3+4x_4&=&4\\
4x_1-x_2&=&10
\end{eqnarray*}

\item \begin{eqnarray*}
0.00031000x_1+1.000000x_2&=&3.000000\\
1.00045534x_1+1.00034333x_2&=&7.000
\end{eqnarray*}


\item Resolver para $A=\left(\begin{array}{ccccc}\\
14 & 14 & -9 & 3 & -5\\
14 & 52 & -15 & 2 & -32\\
-9 & -15 & 36 &-5 & 16\\
3 &2 &-5&47 & 49\\
-5 & 32 & 16 &49 & 79\end{array}\right)$,  $b=\left(\begin{array}{c}\\
-15\\
-100\\
106\\
329\\
463\end{array}\right)$ y  $X=\left(\begin{array}{c}\\
x_1\\
x_2\\
x_3\\
x_4\\
x_5\end{array}\right)$


\item Resolver para $A=\left(\begin{array}{ccccc}
\frac{1}{4} &\frac{1}{5} &\frac{1}{6}\\
 \frac{1}{3} & \frac{1}{4} & \frac{1}{5} \\  
  \frac{1}{2} &  1& 2
\end{array}\right)$,  $b=\left(\begin{array}{c}
9\\
8\\
8\\\end{array}\right)$ y  $X=\left(\begin{array}{c}
x_1\\
x_2\\
x_3\\
\end{array}\right)$

\item  Resolver para $A=\left(\begin{array}{ccccc}
1 & \frac{1}{2} & \frac{1}{3} & \frac{1}{4}  \\
 \frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \frac{1}{5}\\
 \frac{1}{3} & \frac{1}{4} & \frac{1}{5} & \frac{1}{6}\\
 \frac{1}{4} & \frac{1}{5} & \frac{1}{6} & \frac{1}{7}
 \end{array}\right)$,  $b=\left(\begin{array}{c}
 \frac{1}{6} \\
 \frac{1}{7} \\
  \frac{1}{8}\\ 
 \frac{1}{9} \\\end{array}\right)$ y  $X=\left(\begin{array}{c}
x_1\\
x_2\\
x_3\\
x_4
\end{array}\right)$

\item Resolver el sistema
\begin{eqnarray*}
2x_1+x_2-x_3+x_4-3x_5&=&7\\
x_1+2x_3-x_4+x_5&=&2\\
-2x_2-x_3+x_4-x_5&=&-5\\
3x_1+x_2-4x_3+5x_5&=&6\\
x_1-x_2-x_3-x_4+x_5&=&3
\end{eqnarray*}



\item Resolver el sistema
\begin{eqnarray*}
3.333x_1+15920x_2-10.333x_3&=&15913\\
2.222x_1+16.71x_29.612x_2&=&28.544\\
1.5611x_1+5.1791x_2+1.6852x_3&=&8.4254
\end{eqnarray*}

\end{enumerate}
\end{Ejer}


\begin{Ejer}
Resolver por el método de Gauss-Jordan los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
x_1+2x_2+3x_3&=&1\\
-0.4x_1+2x_2-x_3&=&10\\
0.5x_1-3x_2+x_3&=&15
\end{eqnarray*}

\item \begin{eqnarray*}
2x_1-0.9x_2+3x_3&=&-3.61\\
-0.5x_1+0.1x_2-x_3&=&2.035\\
x_1-6.35x_2-0.45x_3&=&15.401
\end{eqnarray*}

\item \begin{eqnarray*}
0.7x_1+2.7x_2-6x_3+0.7x_4&=&1.6487\\
2x_1-0.8x_2+3x_3-x_4&=&-2.342\\
-x_1-1.5x_2+1.4x_3+3x_4&=&-4.189\\
7x_2-1.56x_3+x_4=15.792
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.1x_2-0.2x_3&=&7.85\\
0.1x_1+7x_2-0.3x_3&=&-19.3\\
0.3x_1-0.2x_2+10x_3&=&71.4
\end{eqnarray*}

\item \begin{eqnarray*}
10x_1+2x_2-x_3&=&27\\
-3x_1-6x_2+2x3&=&-61.5\\
x_1+x_2+5x_3&=&-21.5
\end{eqnarray*}

\item $A=\left(\begin{array}{ccccc}\\
1 &3 & -2 & 1\\
1  &3 & -1 & 2\\
0  &1 & -1 & 4\\
2  &6 & 1 & 2\\
\end{array}\right)$ y $b=\left(\begin{array}{c}
4 \\
1 \\
5\\ 
2\\\end{array}\right)$

\item \begin{eqnarray*}
6x_1-x_2-x_3+4x_4&=&17\\
x_1-10x_2+2x_3-x_4&=&-17\\
3x_1-2x_2+8x_3-x_4&=&19\\
x_1+x_2+x_3-5x_4&=&-14
\end{eqnarray*}

\item \begin{eqnarray*}
x+2y+3z+4w&=&1\\
x-4y+z+11w&=&2\\
-x+8y+7z+6w&=&-2\\
16x+8y-5z+6w&=&11
\end{eqnarray*}


\item \begin{eqnarray*}
x_1+x_2&=&3\\
x_1+2x_2+x_3&=&-1\\
x_2+3x_3+x_4&=&2\\
x_3+4x_4+x_5&=&1\\
x_4+5x_5&=&3
\end{eqnarray*}

\item \begin{eqnarray*}
15x_1-18x_2+15x_3-3x_4&=&11\\
-18x_1+24_2-18x_3+4x_4&=&10\\
15x_1-18x_2+18x_3-3x_4&=&11\\
-3x_1+4x_2-3x_3+x_4&=&13
\end{eqnarray*}

\end{enumerate}
\end{Ejer}

\begin{Ejer}
Resolver por el método de Gauss-Seidel los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
3x_1-0.2x_2-0.5x_3&=&8\\
0.1x_1+7x_2+0.4x_3&=&-19.5\\
0.4x_1-0.1x_2+10x_3&=&72.4
\end{eqnarray*}

\item \begin{eqnarray*}
-5x_1+1.4x_2-2.7x_3&=&94.2\\
0.7x_1-2.5x_2+15x_3&=&-6\\
3.3x_1-11x_2+4.4x_3&=&-27.5
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.5x_2+0.6x_3&=&5.24\\
0.3x_1-4x_2-x_3&=&-0.387\\
-0.7x_1+2x_2+7x_3&=&14.803
\end{eqnarray*}

\item \begin{eqnarray*}
5x_1-0.2x_2+x_3&=&1.5\\
0.1x_1+3x_2-0.5x_3&=&-2.7\\
-0.3x_1+x_2-7x_3&=&9.5
\end{eqnarray*}

\item \begin{eqnarray*}
-3x_2+7x_3&=&2\\
x_1+2x_2-x_3&=&3\\
12x_1+2x_2+2x_3&=&6
\end{eqnarray*}

\item \begin{eqnarray*}
0.15_1+2.11x_2+30.75x_3&=&-26.38\\
0.64x_1+1.21x_2+2.05x_3&=&1.01\\
3.21x_1+1.53x_2+1.04x_3&=&5.23
\end{eqnarray*}


\item \begin{eqnarray*}
x_1+x_2-x_3&=&-3\\
6x_1+2x_2+2x_3&=&2\\
-3x_1+4x_2+x_3&=&1
\end{eqnarray*}

\item \begin{eqnarray*}
2x_1+x_2-x3&=&1\\
5x_1+2x_2+2x_3&=&-4\\
3x_1+x_2+x_3&=&5
\end{eqnarray*}

\item \begin{eqnarray*}
3x-0.1y-0.2z&=&7.85\\
0.1x+7y-0.3z&=&-19.3\\
0.3x_1-0.2x_2+10x_3=71.4
\end{eqnarray*}


\item \begin{eqnarray*}
17x_1-2x_2-3x_3=500\\
-5x_1+21x_2-2x_3&=&200\\
-5x_1-5x_2+22x_3&=&30
\end{eqnarray*}

\end{enumerate}
\end{Ejer}



\begin{Ejer}
Aplicar el método de Jacobi para resolver los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item $A=\left(\begin{array}{cccc|c}\\
10 & 2 &  -1 &  0 &  26\\
1 & 20 & -2 & 3 & -15\\
-2 & 1 & 30 & 0 & 53\\
1 & 2 & 3 & 20 & 47
\end{array}\right)$


\item $A=\left(\begin{array}{ccc|c}\\
-1 & 2 & 10 & 11\\ 
11 & -1 & 2 & 12\\
1 & 5 & 2 & 8
\end{array}\right)$

\item $A=\left(\begin{array}{ccc|c}\\
8 & 2 & 3 & 51\\
2 & 5 & 1 & 23\\
-3 & 1 & 6 & 20
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
2 & -1 & 1 & 3 & 10\\
2 & 2 & 2 & 2 & 1\\
-1 & -1 & 2 & 2 & -5\\
3 & 1 & -1 & 4 & 6
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
3 & 1 & 1 & -1 & 5\\
0 & 2 & 1 & 4 & 0\\
1 & 1 & -1 & 9 & 1\\
2 & 4 & 6 & 3 & 0
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
10 & -1 & 2 & 0 & 6\\
-1 & 11 & -1 & 3 & 25\\
2 & -1 & 10 & -1 & -11\\
0 & 2 & -1 & 8 & 15
\end{array}\right)$

\item \begin{eqnarray*}
x_1+2x_2-2x_3&=&7\\
x_1+x_2+x_3&=&2\\
2x_1+2x_2+x_3&=&5
\end{eqnarray*}

\item \begin{eqnarray*}
-4x_1+14x_2=10\\
-5x_1+13x_2&=8\\
-x_1+2x_3&=&1
\end{eqnarray*}

\item \begin{eqnarray*}
x+y+2z&=&1\\
x+2y+z&=&1\\
2x+y+z&=&1
\end{eqnarray*}

\item \begin{eqnarray*}
6x_1-2x_2+2x_3+4x_4&=&10\\
12x_1-8x_2+6x_3+10x_4&=&20\\
3x_1-13x_2+9x_3+3x_4&=&2\\
-6x_1+4x_2+x_3-18x_4&=&-19
\end{eqnarray*}


\end{enumerate}
\end{Ejer}

\begin{Ejer}

\begin{enumerate}
  \item Resuelva el sistema
  \begin{eqnarray*}
  \begin{cases}
  2x+y-z=1,\\
  -x+3y+2z=12,\\
  x+2y+3z=7
  \end{cases}
  \end{eqnarray*}
  mediante eliminación gaussiana simple (sin pivoteo).

  \item Resuelva el mismo sistema anterior pero ahora aplicando eliminación gaussiana con pivoteo parcial. Compare los pasos con el ejercicio anterior.

  \item Aplique eliminación gaussiana con pivoteo y escalamiento al sistema
  \begin{eqnarray*}
  \begin{cases}
  10x+2y+z=7,\\
  2x+20y+2z=9,\\
  x+2y+30z=12
  \end{cases}
  \end{eqnarray*}
  y analice la importancia del escalamiento.

  \item Utilice el método de Gauss--Jordan para calcular la inversa de
  \begin{eqnarray*}
  A=\begin{bmatrix}
  1 & 2 & 1\\
  0 & 1 & -1\\
  2 & 3 & 4
  \end{bmatrix}.
  \end{eqnarray*}

  \item Resuelva $Ax=b$ con $A$ y $b$ dados por
  \begin{eqnarray*}
  A=\begin{bmatrix}
  4 & -2 & 1\\
  -2 & 4 & -2\\
  1 & -2 & 3
  \end{bmatrix},\qquad
  b=\begin{bmatrix}1\\4\\2\end{bmatrix},
  \end{eqnarray*}
  utilizando factorización $LU$ y sustitución hacia adelante y hacia atrás.

  \item Calcule la factorización de Cholesky de
  \begin{eqnarray*}
  A=\begin{bmatrix}
  25 & 15 & -5\\
  15 & 18 & 0\\
  -5 & 0 & 11
  \end{bmatrix}
  \end{eqnarray*}
  y resuelva $Ax=b$ con $b=(35,33,6)^\top$.
\end{enumerate}


\begin{enumerate}\setcounter{enumi}{6}
  \item Resuelva mediante sustitución hacia atrás el sistema triangular superior:
  \begin{eqnarray*}
  \begin{cases}
  2x+3y-z=5,\\
  -y+2z=4,\\
  3z=6.
  \end{cases}
  \end{eqnarray*}

  \item Resuelva mediante sustitución hacia adelante el sistema triangular inferior:
  \begin{eqnarray*}
  \begin{cases}
  x=3,\\
  2y+x=5,\\
  z-y+2x=10.
  \end{cases}
  \end{eqnarray*}

\end{enumerate}


\begin{enumerate}\setcounter{enumi}{9}
  \item Aplique el método de Jacobi para resolver
  \begin{eqnarray*}
  \begin{cases}
  10x-y+2z=6,\\
  -x+11y-z+3w=25,\\
  2x-y+10z-w=-11,\\
  3y-z+8w=15,
  \end{cases}
  \end{eqnarray*}
  realizando 3 iteraciones con $x^{(0)}=\mathbf{0}$.

  \item Repita el ejercicio anterior con el método de Gauss--Seidel. Compare la velocidad de convergencia con Jacobi.

  \item Aplique el método SOR con $\omega=1.25$ al mismo sistema y compare las tres trayectorias de convergencia.

  \item Escriba la matriz de iteración $T_J$ y el vector $c_J$ del método de Jacobi para el sistema
  \begin{eqnarray*}
  \begin{cases}
  4x+y=9,\\
  x+3y=7.
  \end{cases}
  \end{eqnarray*}
  Verifique si $\rho(T_J)<1$.

  \item Investigue experimentalmente en R cuál es el valor óptimo aproximado de $\omega$ para el método SOR en el sistema $4\times 4$ del ejercicio 10.
\end{enumerate}


\begin{enumerate}\setcounter{enumi}{14}
  \item Genere una matriz aleatoria simétrica definida positiva $5\times 5$ en R y resuelva $Ax=b$:
 \begin{itemize}
    \item[(a)] Usando factorización $LU$.  
    \item[(b)] Usando factorización de Cholesky.  
    \item[(c)] Usando Gauss--Seidel con 20 iteraciones.  
  \end{itemize}
  Compare el tiempo de cómputo y la precisión de cada método.
\end{enumerate}
\end{Ejer}


\newpage


%===========================================
\section{Introducción al uso de R}
%===========================================


%-------------------------------------------
\subsection{Sesiones en RStudio}
%-------------------------------------------


Al utilizar R, existen varios entornos que facilitan la gestión y ejecución de rutinas,  \textit{archivos con extensión .R}, el más popular es \textit{RStudio} o bien directamente desde la terminal o ejecutando simplemente \textit{R}, la ventaja de \textit{RStudio} es que permite que en una pantalla podamos visualizar: Consola (lugar donde se ejecutan los comandos directamente), History (el histórico de las variables y funciones definidas mismo que puede guardarse para ser invocado posteriormente), Plots (ventana en la que se muestran los gráficos generados), Help (la ayuda sobre comandos, funciones, sintaxis en R), Files (lugar donde se manejan los archivos, es decir leer, guardar, mover o renombrar archivos), y Packages (espacio para instalar o cargar paquetes de manera gráfica), todo esto para facilitar el manejo y ejecución de rutinas compatibles con R. Por otra parte \textbf{Workspace} es un entorno en el que se incluyen todos los objetos definidos,  al final de una sesión de R,  este entorno puede guardarse una imagen del mismo para ser cargada posteriormente. \bigskip


Durante el uso de R en ocasiones se requiere limpiar la consola, para esto al presionar \textbf{Ctrl+L}.  


%-------------------------------------------
\subsection{Uso de R}
%-------------------------------------------

\begin{itemize}
\item \textbf{Constantes}: $\pi$, $exp(1)$

\item Las constantes pueden ser de tipo \textit{integer},  \textit{double} o \textit{complex}, el tipo de constante se puede consultar con la función \textbf{typeof()}

\begin{verbatim}
> typeof('mi constante')
[1] "character"
\end{verbatim}

\item Operadores: $<,>,>=,>=,!=$ ,$\!$ (Not),  $\|$ (OR), $\&$ (And),  $==$ (comparar)

\item Operadores aritméticos: $+$, $-$, $*$, \verb|^| potencia,  \verb|%%| resto de la división entera,  \verb|%/%| división entera.

\item Logaritmos y exponenciales: \verb|log| logaritmo natural,  \verb|log(x,b)| ($log_{b}x$) y \verb|exp(x)| ($e^x$).

\item Funciones trigonométricas\verb|cos(x)|,\verb|sin(x)|, \verb|tan(x)|, \verb|acos(x)|, \verb|asin(x)|, \verb|atan(x)|.

\item Funciones misceláneas \verb|abs(x)|,  \verb|sqrt(x)|,  \verb|floor(x)|,  \verb|ceiling(x)|, \verb|max(x)|,  \verb|sign(x)|.

\item Comando \verb|options(digits=k)|:

\begin{verbatim}
> 1/3.0
[1] 0.3333333
> options(digits=3)
> 1/3
[1] 0.333
> 1/17.0
[1] 0.0588
> options(digits=3)
> 1/17.0
[1] 0.0588
> options(digits=5)
> 1/17.0
[1] 0.058824
> options(digits=9)
> 1/17.0
[1] 0.0588235294
\end{verbatim}

\item Comando \verb|round(x,n)| redondea $x$ a $n$ decimales, el valor por defecto es $n=6$.

\item Comando \verb|cat('caracter1','caracter2')| concatena dos cadenas o valores y el resultado lo convierte a un objeto tipo \textit{string}.

\end{itemize}

%-------------------------------------------
\subsection{Funciones}
%-------------------------------------------

\begin{verbatim}
nombrefun = function(a1,a2,...,an) {
# código ...
instruccion-1
instruccion-2
# ...
return( ... ) #valor que retorna (o también la última instrucción, si ésta retorna algo)
}
\end{verbatim}

\subsection{Clase en Laboratorio de Cómputo}

\begin{Ejer}
\begin{enumerate}
\item Generar un archivo tipo Rmd, personalizar de manera básica
\item Realizar las siguientes operaciones

\begin{verbatim}
x = c(1.1, 1.2, 1.3, 1.4, 1.5)
x = 1:5 
x = seq(1,3, by =0.5) 
x = seq(5,1, by =-1) 
x = rep(1, times = 5) 
length(x) 
rep(x, 2) 
set.seed(123) 
x = sample(1:10, 5) 
\end{verbatim}

\item 
\begin{verbatim}
x = 1:5 
y = rep(0.5, times = length(x)) 
x+y
x*y
x^y
1/(1:5)
\end{verbatim}

\item 
\begin{verbatim}
x = 1:5
2*x
1/x^2
x+3

\end{verbatim}

\item 
\begin{verbatim}
A = matrix(rep(0,9), nrow = 3, ncol= 3); 
B = matrix(c(1,2,3,5,6,7), nrow = 2, byrow=T); 
x = 1:3; y = seq(1,2, by = 0.5); z = rep(8, 3) ; x; y; z
C = matrix(c(x,y,z), nrow = length(x)); C # ncol no es necesario declararlo
xi = seq(1,2, by 0.1); yi = seq(5,10, by = 0.5)
rbind(xi,yi)
cbind(xi,yi)
\end{verbatim}

\item 
\begin{verbatim}

A = diag(c(3,1,3)); 
diag(A)
n = 3
I = diag(1, n);
D = diag(diag(A))
J = diag(1, 3, 4);
\end{verbatim}

\item 
\begin{verbatim}

B = matrix(c( 1, 1 ,8,
              2, 0, 8,
              3, 2, 8), nrow = 3, byrow=TRUE); B

B[2, 3]
B[3,]
B[,2]
B[1:2,c(2,3)]

\end{verbatim}

\item 
\begin{verbatim}
A = matrix(c( 1, 1 ,8,
              2, 0, 8,
              3, 2, 8), nrow = 3, byrow=TRUE); A
A[c(1,3), ] = A[c(3,1), ] 
A[2, ] = A[2, ] - A[2,1]/A[1,1]*A[1, ]

\end{verbatim}

\item 
\begin{verbatim}
x = c(2, -6, 7, 8, 0.1,-8.5, 3, -7, 3)
which.max(x)
which.max(abs(x)) 

\end{verbatim}

\item 
\begin{verbatim}

A = matrix(1:9, nrow=3); A # Por columnas
B = matrix(rep(1,9), nrow=3); B

A+B
A*B
A%*%B
A^2
A-2
3*A
t(A)
det(A)
C <- A-diag(1,3); det(C)
\end{verbatim}

\item 
\begin{verbatim}


notas = matrix(c(80, 40, 70, 30, 90, 67, 90,
                 40, 40, 30, 90, 100, 67, 90,
                 100,100,100, 100, 70, 76, 95), nrow=3, byrow=TRUE); notas
# crear columa con la suma de los renglones
#agregar la columna al final de la matriz
\end{verbatim}

\item 
\begin{verbatim}

u=c(1,2)
v=c(-2,3)
w=c(3,-5)
norma=function(u){sqrt(sum(u^2))}
\end{verbatim}

\item 
\begin{verbatim}

t(u-2*v)%*%w
norma(u+v+w)
norma(u)+norma(v)+norma(w)
t(u-v)%*%(v-w)

\end{verbatim}

\item 
\begin{verbatim}

u=c(8,3);a=c(4,-5)
ProyOrto=function(u,a){(t(u)%*%a)*a/norma(a)}
ProyOrto(u,a)
ProyOrto(c(2,1,-4),c(-5,3,11))

\end{verbatim}

\item 
\begin{verbatim}

A=matrix(c(1,0,0,1/3,4,0,1/2,3,2),ncol=3,byrow=TRUE)
B=matrix(c(9,0,0,0,1,8,0,0,0,-2,7,0,0,0,-3,6),ncol=4)
solve(A)  # Inversa de A
det(A)
solve(B);det(B)

\end{verbatim}

\item 
\begin{verbatim}

A=matrix(c(4,4.001,4.001,4.002),ncol=2)
B=A;B[2,2]=4.002001
solve(A)
solve(B)

\end{verbatim}

\item 
\begin{verbatim}

X=matrix(runif(12),ncol=3)
u=runif(4)
A=t(X)%*%X
B=u%*%t(u)
DA=eigen(A)$values
TA=eigen(A)$vectors
t(TA)%*%TA
prod(DA);det(A)
qr(A)$rank  # Rango de una matriz
sum(diag(DA)!=0)
\end{verbatim}

\item 
\begin{verbatim}

DB=eigen(B)$values
TB=eigen(B)$vectors
t(TB)%*%TB
prod(DB);det(B)
qr(B)$rank
sum(diag(DB)!=0)

\end{verbatim}

\item 
\begin{verbatim}

A=matrix(c(3,2,0,2,3,0,0,0,3),ncol=3)
eig_A=eigen(A)
eig_A$values
eigen(A%*%A)$values
eigen(solve(A))$values
eig_A$vectors%*%diag(eigen(A%*%A)$values)%*%t(eig_A$vectors);A%*%A

\end{verbatim}

\item 
\begin{verbatim}

A=matrix(c(6,10,1,10,6,5),ncol=2)
ginvMP=function(A){
        res=svd(A)
        res$v%*%diag(1/res$d)%*%t(res$u)}
B=ginvMP(A)
A%*%B%*%A

\end{verbatim}

\item 
\begin{verbatim}

es.positiva=function(A){
 if (ncol(A)!=nrow(A)) stop("Esto se hace para matrices cuadradas")
 v=eigen(A)$values
 tol=ncol(A)*max(abs(v))* .Machine$double.eps
 if (sum(v>tol)==length(v)) return(TRUE) else return(FALSE)}
 
A=matrix(c(2,-3/2,-3/2,3),ncol=2);es.positiva(A)
A=matrix(c(1,0.8,0.5,0.8,0.6,0.4,0.5,0.4,0.25),ncol=3);es.positiva(A)

\end{verbatim}

\item 
\begin{verbatim}

matrixA=function(m){
A=matrix(c(1,-2,-2,m),ncol=2)
return(A)}

dmatrixA=function(m){det(matrixA(m))}

m=seq(-4,10,len=101)
plot(m,mapply(dmatrixA,m=m),type="l") # Dibuja el determinante en funci?n de m
abline(h=0)
A=matrixA(-2)
print(z<-eigen(A))


\end{verbatim}

\item 
\begin{verbatim}

A=cbind(c(3,1,0),c(1,3,0),c(0,0,3))
B=matrix(0,ncol=3,nrow=3);B[3,3]=2
eigen(A)
eigen(B)

eigen(A+B) # Trampilla

\end{verbatim}

\end{enumerate}



\end{Ejer}


\section{Tareas del curso}

\begin{Ejer}
Convertir los siguientes números de base 10 a base 2.
\begin{enumerate}
\item $324$
\item $27$
\item $1423$
\item $235.25$
\item $41.596$
\end{enumerate}
\end{Ejer}


\begin{Ejer}
\begin{enumerate}
\item Realizar una revisión de la historia de los m\'etodos num\'ericos, elaborar un documento de hasta dos cuartillas.
\item Realiza las siguientes conversiones de base $10$ a base $2$:
\begin{enumerate}
\item 246
\item 345.68
\item 4586632.2846
\item 984365.27463
\item 79905523
\end{enumerate}
\item Elabora el c\'odigo en R para realizar la conversi\'on de base $10$ a base $2$.

\item Describe exhaustivamente los tipos de errores que existen

\end{enumerate}


\end{Ejer}


\begin{Note}
En los siguientes ejercicios se indicar\'a cuales ejercicios pueden realizarse sin el apoyo de R, se deben de resolver al menos dos ejercicios de cada serie sin el apoyo de R, es decir, se tienen que resolver manualmente.
\end{Note}

\begin{Ejer}
Resolver por eliminación Gaussiana Simple los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
x_1-2x_2+0.5x_3&=&-5\\
-2x_1+5x_{2}-1.5x_3&=&0\\
-0.2x_1+1.75x_2-x_3&=&10
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-x_2+6x_4&=&2.3\\
4x_1+2x_2-x_3-5x_4&=&6.9\\
-5x_1+x_2-3x_3&=&-36\\
10x_2-4x_3+7x_4&=&-36
\end{eqnarray*}

\item \begin{eqnarray*}
0.003000x_1+59.14x_2&=&59.17\\
5.291x_1-6.130x_2&=&46.78
\end{eqnarray*}
utilizar redondeo a 4 cifras significativas.

\item \begin{eqnarray*}
4x_1+2x_2&=&2\\
2x_1+3x_2+x_3&=&-1\\
x_2+\frac{5}{2}x_3&=&3
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.1x_2-0.2x_3&=&7.85\\
0.1x_{1}+7x_2-0.3x_3=-19.3\\
0.3x_1-0.2x_2+10x_3&=&71.4
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
8x_1+2x_2-2x_3&=&-2\\
10x_1+2x_2+4x_3&=&4\\
12x_1+2x_2+2x_3&=&6
\end{eqnarray*}

\item\textbf{*} \begin{eqnarray*}
5x_1 + 2x_2 + x_3 - x_4 = 1\\
2x_1 - x_2 + 3x_3 + 2x_4 = 12\\
4x_1 + x_2 - 2x_3 + 3x_4 = 5\\
-2x_1 + 2x_2 + x_3 + x_4 = 2
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
2x_1+3x_2+2x_3+4x_4&=&4\\
4x_1+10x_2-4x_3&=&-8\\
-3x_1-2x_2-5x_3-2x_4&=&-4\\
-2x_1+4x_2+4x_3-7x_4&=&-1
\end{eqnarray*}

\item \begin{eqnarray*}
1.133x_1+5.281x_2-2.454x_3&=&6.414\\
24.14x_1-1.21x_2+5.281x_3&=&113.8\\
-10.123x_1+6.387x_2-x_3&=&1
\end{eqnarray*}

\item \begin{eqnarray*}A=\left(\begin{array}{ccccc}
2 & 3 & 2 & 4\\
4 & 10 &-4 & 0\\
-3 & -2 & -5 &-2\\
-2 & 4 & 4 &-7\\
\end{array}\right)\end{eqnarray*} y \begin{eqnarray*}b=\left(\begin{array}{c}\\
9 \\
-15\\
6 \\
2\\
\end{array}\right)\end{eqnarray*}

\end{enumerate}
\end{Ejer}

\begin{Ejer}
Resolver por eliminación gaussiana con pivoteo parcial los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
0.4x_1-1.5x_2+0.75x_3&=&-20\\
-0.5x_1-15x_2+10x_3&=&-10\\
-10x_1-9x_2+2.5x_3&=&30
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
5x_1-8x_2+x_3&=&-71\\
-2x_1+6x_2-9x_3&=&134\\
3x_1-5x_2+2x_3&=&-58
\end{eqnarray*}

\item \begin{eqnarray*}
0.003000x_1+59.14x_2&=&59.17\\
5.291x_1-6.130x_2&=&46.78
\end{eqnarray*}
utilizar redondeo a 4 cifras significativas.


\item \textbf{*}\begin{eqnarray*}
-x_2+4x_3-x_4&=&-1\\
-x_1+4x_2-x_3&=&2\\
-x_1-x_3+4x_4&=&4\\
4x_1-x_2&=&10
\end{eqnarray*}

\item \begin{eqnarray*}
0.00031000x_1+1.000000x_2&=&3.000000\\
1.00045534x_1+1.00034333x_2&=&7.000
\end{eqnarray*}


\item \textbf{*} Resolver para $A=\left(\begin{array}{ccccc}\\
14 & 14 & -9 & 3 & -5\\
14 & 52 & -15 & 2 & -32\\
-9 & -15 & 36 &-5 & 16\\
3 &2 &-5&47 & 49\\
-5 & 32 & 16 &49 & 79\end{array}\right)$,  $b=\left(\begin{array}{c}\\
-15\\
-100\\
106\\
329\\
463\end{array}\right)$ y  $X=\left(\begin{array}{c}\\
x_1\\
x_2\\
x_3\\
x_4\\
x_5\end{array}\right)$


\item Resolver para $A=\left(\begin{array}{ccccc}
\frac{1}{4} &\frac{1}{5} &\frac{1}{6}\\
 \frac{1}{3} & \frac{1}{4} & \frac{1}{5} \\  
  \frac{1}{2} &  1& 2
\end{array}\right)$,  $b=\left(\begin{array}{c}
9\\
8\\
8\\\end{array}\right)$ y  $X=\left(\begin{array}{c}
x_1\\
x_2\\
x_3\\
\end{array}\right)$

\item  Resolver para $A=\left(\begin{array}{ccccc}
1 & \frac{1}{2} & \frac{1}{3} & \frac{1}{4}  \\
 \frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \frac{1}{5}\\
 \frac{1}{3} & \frac{1}{4} & \frac{1}{5} & \frac{1}{6}\\
 \frac{1}{4} & \frac{1}{5} & \frac{1}{6} & \frac{1}{7}
 \end{array}\right)$,  $b=\left(\begin{array}{c}
 \frac{1}{6} \\
 \frac{1}{7} \\
  \frac{1}{8}\\ 
 \frac{1}{9} \\\end{array}\right)$ y  $X=\left(\begin{array}{c}
x_1\\
x_2\\
x_3\\
x_4
\end{array}\right)$

\item Resolver el sistema
\begin{eqnarray*}
2x_1+x_2-x_3+x_4-3x_5&=&7\\
x_1+2x_3-x_4+x_5&=&2\\
-2x_2-x_3+x_4-x_5&=&-5\\
3x_1+x_2-4x_3+5x_5&=&6\\
x_1-x_2-x_3-x_4+x_5&=&3
\end{eqnarray*}



\item Resolver el sistema
\begin{eqnarray*}
3.333x_1+15920x_2-10.333x_3&=&15913\\
2.222x_1+16.71x_29.612x_2&=&28.544\\
1.5611x_1+5.1791x_2+1.6852x_3&=&8.4254
\end{eqnarray*}

\end{enumerate}
\end{Ejer}


\begin{Ejer}
Resolver por el método de Gauss-Jordan los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
x_1+2x_2+3x_3&=&1\\
-0.4x_1+2x_2-x_3&=&10\\
0.5x_1-3x_2+x_3&=&15
\end{eqnarray*}

\item \begin{eqnarray*}
2x_1-0.9x_2+3x_3&=&-3.61\\
-0.5x_1+0.1x_2-x_3&=&2.035\\
x_1-6.35x_2-0.45x_3&=&15.401
\end{eqnarray*}

\item \begin{eqnarray*}
0.7x_1+2.7x_2-6x_3+0.7x_4&=&1.6487\\
2x_1-0.8x_2+3x_3-x_4&=&-2.342\\
-x_1-1.5x_2+1.4x_3+3x_4&=&-4.189\\
7x_2-1.56x_3+x_4=15.792
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.1x_2-0.2x_3&=&7.85\\
0.1x_1+7x_2-0.3x_3&=&-19.3\\
0.3x_1-0.2x_2+10x_3&=&71.4
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
10x_1+2x_2-x_3&=&27\\
-3x_1-6x_2+2x3&=&-61.5\\
x_1+x_2+5x_3&=&-21.5
\end{eqnarray*}

\item $A=\left(\begin{array}{ccccc}\\
1 &3 & -2 & 1\\
1  &3 & -1 & 2\\
0  &1 & -1 & 4\\
2  &6 & 1 & 2\\
\end{array}\right)$ y $b=\left(\begin{array}{c}
4 \\
1 \\
5\\ 
2\\\end{array}\right)$

\item \begin{eqnarray*}
6x_1-x_2-x_3+4x_4&=&17\\
x_1-10x_2+2x_3-x_4&=&-17\\
3x_1-2x_2+8x_3-x_4&=&19\\
x_1+x_2+x_3-5x_4&=&-14
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
x+2y+3z+4w&=&1\\
x-4y+z+11w&=&2\\
-x+8y+7z+6w&=&-2\\
16x+8y-5z+6w&=&11
\end{eqnarray*}


\item \textbf{*}\begin{eqnarray*}
x_1+x_2&=&3\\
x_1+2x_2+x_3&=&-1\\
x_2+3x_3+x_4&=&2\\
x_3+4x_4+x_5&=&1\\
x_4+5x_5&=&3
\end{eqnarray*}

\item \begin{eqnarray*}
15x_1-18x_2+15x_3-3x_4&=&11\\
-18x_1+24_2-18x_3+4x_4&=&10\\
15x_1-18x_2+18x_3-3x_4&=&11\\
-3x_1+4x_2-3x_3+x_4&=&13
\end{eqnarray*}

\end{enumerate}
\end{Ejer}

\begin{Ejer}
Resolver por el método de Gauss-Seidel los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item \begin{eqnarray*}
3x_1-0.2x_2-0.5x_3&=&8\\
0.1x_1+7x_2+0.4x_3&=&-19.5\\
0.4x_1-0.1x_2+10x_3&=&72.4
\end{eqnarray*}

\item \begin{eqnarray*}
-5x_1+1.4x_2-2.7x_3&=&94.2\\
0.7x_1-2.5x_2+15x_3&=&-6\\
3.3x_1-11x_2+4.4x_3&=&-27.5
\end{eqnarray*}

\item \begin{eqnarray*}
3x_1-0.5x_2+0.6x_3&=&5.24\\
0.3x_1-4x_2-x_3&=&-0.387\\
-0.7x_1+2x_2+7x_3&=&14.803
\end{eqnarray*}

\item \begin{eqnarray*}
5x_1-0.2x_2+x_3&=&1.5\\
0.1x_1+3x_2-0.5x_3&=&-2.7\\
-0.3x_1+x_2-7x_3&=&9.5
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
-3x_2+7x_3&=&2\\
x_1+2x_2-x_3&=&3\\
12x_1+2x_2+2x_3&=&6
\end{eqnarray*}

\item \begin{eqnarray*}
0.15_1+2.11x_2+30.75x_3&=&-26.38\\
0.64x_1+1.21x_2+2.05x_3&=&1.01\\
3.21x_1+1.53x_2+1.04x_3&=&5.23
\end{eqnarray*}


\item \textbf{*}\begin{eqnarray*}
x_1+x_2-x_3&=&-3\\
6x_1+2x_2+2x_3&=&2\\
-3x_1+4x_2+x_3&=&1
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
2x_1+x_2-x3&=&1\\
5x_1+2x_2+2x_3&=&-4\\
3x_1+x_2+x_3&=&5
\end{eqnarray*}

\item \begin{eqnarray*}
3x-0.1y-0.2z&=&7.85\\
0.1x+7y-0.3z&=&-19.3\\
0.3x_1-0.2x_2+10x_3=71.4
\end{eqnarray*}


\item \begin{eqnarray*}
17x_1-2x_2-3x_3=500\\
-5x_1+21x_2-2x_3&=&200\\
-5x_1-5x_2+22x_3&=&30
\end{eqnarray*}

\end{enumerate}
\end{Ejer}



\begin{Ejer}
Aplicar el método de Jacobi para resolver los siguientes sistemas de ecuaciones lineales

\begin{enumerate}
\item $A=\left(\begin{array}{cccc|c}\\
10 & 2 &  -1 &  0 &  26\\
1 & 20 & -2 & 3 & -15\\
-2 & 1 & 30 & 0 & 53\\
1 & 2 & 3 & 20 & 47
\end{array}\right)$


\item \textbf{*}$A=\left(\begin{array}{ccc|c}\\
-1 & 2 & 10 & 11\\ 
11 & -1 & 2 & 12\\
1 & 5 & 2 & 8
\end{array}\right)$

\item $A=\left(\begin{array}{ccc|c}\\
8 & 2 & 3 & 51\\
2 & 5 & 1 & 23\\
-3 & 1 & 6 & 20
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
2 & -1 & 1 & 3 & 10\\
2 & 2 & 2 & 2 & 1\\
-1 & -1 & 2 & 2 & -5\\
3 & 1 & -1 & 4 & 6
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
3 & 1 & 1 & -1 & 5\\
0 & 2 & 1 & 4 & 0\\
1 & 1 & -1 & 9 & 1\\
2 & 4 & 6 & 3 & 0
\end{array}\right)$

\item $A=\left(\begin{array}{cccc|c}\\
10 & -1 & 2 & 0 & 6\\
-1 & 11 & -1 & 3 & 25\\
2 & -1 & 10 & -1 & -11\\
0 & 2 & -1 & 8 & 15
\end{array}\right)$

\item \textbf{*}\begin{eqnarray*}
x_1+2x_2-2x_3&=&7\\
x_1+x_2+x_3&=&2\\
2x_1+2x_2+x_3&=&5
\end{eqnarray*}

\item \begin{eqnarray*}
-4x_1+14x_2=10\\
-5x_1+13x_2&=8\\
-x_1+2x_3&=&1
\end{eqnarray*}

\item \textbf{*}\begin{eqnarray*}
x+y+2z&=&1\\
x+2y+z&=&1\\
2x+y+z&=&1
\end{eqnarray*}

\item \begin{eqnarray*}
6x_1-2x_2+2x_3+4x_4&=&10\\
12x_1-8x_2+6x_3+10x_4&=&20\\
3x_1-13x_2+9x_3+3x_4&=&2\\
-6x_1+4x_2+x_3-18x_4&=&-19
\end{eqnarray*}


\end{enumerate}
\end{Ejer}

\begin{Ejer} La siguiente serie de ejercicios hay que resolverlos con el apoyo de R, excepto los indicados por un \textbf{*}

\begin{enumerate}
  \item \textbf{*} Resuelva el sistema
  \begin{eqnarray*}
  \begin{cases}
  2x+y-z=1,\\
  -x+3y+2z=12,\\
  x+2y+3z=7
  \end{cases}
  \end{eqnarray*}
  mediante eliminación gaussiana simple (sin pivoteo).

  \item Resuelva el mismo sistema anterior pero ahora aplicando eliminación gaussiana con pivoteo parcial. Compare los pasos con el ejercicio anterior.

  \item Aplique eliminación gaussiana con pivoteo y escalamiento al sistema
  \begin{eqnarray*}
  \begin{cases}
  10x+2y+z=7,\\
  2x+20y+2z=9,\\
  x+2y+30z=12
  \end{cases}
  \end{eqnarray*}
  y analice la importancia del escalamiento.

  \item \textbf{*}Utilice el método de Gauss--Jordan para calcular la inversa de
  \begin{eqnarray*}
  A=\begin{bmatrix}
  1 & 2 & 1\\
  0 & 1 & -1\\
  2 & 3 & 4
  \end{bmatrix}.
  \end{eqnarray*}

  \item \textbf{*}Resuelva $Ax=b$ con $A$ y $b$ dados por
  \begin{eqnarray*}
  A=\begin{bmatrix}
  4 & -2 & 1\\
  -2 & 4 & -2\\
  1 & -2 & 3
  \end{bmatrix},\qquad
  b=\begin{bmatrix}1\\4\\2\end{bmatrix},
  \end{eqnarray*}
  utilizando factorización $LU$ y sustitución hacia adelante y hacia atrás.

  \item Calcule la factorización de Cholesky de
  \begin{eqnarray*}
  A=\begin{bmatrix}
  25 & 15 & -5\\
  15 & 18 & 0\\
  -5 & 0 & 11
  \end{bmatrix}
  \end{eqnarray*}
  y resuelva $Ax=b$ con $b=(35,33,6)^\top$.
\end{enumerate}


\begin{enumerate}\setcounter{enumi}{6}
  \item \textbf{*}Resuelva mediante sustitución hacia atrás el sistema triangular superior:
  \begin{eqnarray*}
  \begin{cases}
  2x+3y-z=5,\\
  -y+2z=4,\\
  3z=6.
  \end{cases}
  \end{eqnarray*}

  \item \textbf{*}Resuelva mediante sustitución hacia adelante el sistema triangular inferior:
  \begin{eqnarray*}
  \begin{cases}
  x=3,\\
  2y+x=5,\\
  z-y+2x=10.
  \end{cases}
  \end{eqnarray*}

\end{enumerate}


\begin{enumerate}\setcounter{enumi}{9}
  \item Aplique el método de Jacobi para resolver
  \begin{eqnarray*}
  \begin{cases}
  10x-y+2z=6,\\
  -x+11y-z+3w=25,\\
  2x-y+10z-w=-11,\\
  3y-z+8w=15,
  \end{cases}
  \end{eqnarray*}
  realizando 3 iteraciones con $x^{(0)}=\mathbf{0}$.

  \item Repita el ejercicio anterior con el método de Gauss--Seidel. Compare la velocidad de convergencia con Jacobi.

  \item Aplique el método SOR con $\omega=1.25$ al mismo sistema y compare las tres trayectorias de convergencia.

  \item Escriba la matriz de iteración $T_J$ y el vector $c_J$ del método de Jacobi para el sistema
  \begin{eqnarray*}
  \begin{cases}
  4x+y=9,\\
  x+3y=7.
  \end{cases}
  \end{eqnarray*}
  Verifique si $\rho(T_J)<1$.

  \item Investigue experimentalmente en R cuál es el valor óptimo aproximado de $\omega$ para el método SOR en el sistema $4\times 4$ del ejercicio 10.
\end{enumerate}


\begin{enumerate}\setcounter{enumi}{14}
  \item Genere una matriz aleatoria simétrica definida positiva $5\times 5$ en R y resuelva $Ax=b$:
 \begin{itemize}
    \item[(a)] Usando factorización $LU$.  
    \item[(b)] Usando factorización de Cholesky.  
    \item[(c)] Usando Gauss--Seidel con 20 iteraciones.  
  \end{itemize}
  Compare el tiempo de cómputo y la precisión de cada método.
\end{enumerate}


\end{Ejer}



\begin{Ejer}
Genera un archivo tipo Rmd con las prácticas realizadas en el laboratorio, en las que deberá de desplegarse una barra lateral izquierda con un nivel de profundidad de 4: seccion, subseccion, subsubseccion y subsubsubseccion.
\end{Ejer}




\begin{Ejer}

Revisa en R los códigos para resolver sistemas de ecuaciones lineales, esto en un archivo tipo Rmd, mismo que deberás de entregar.



\end{Ejer}

\newpage



\section{Apendice A: Breve historia de los M\'etodos Num\'ericos}


Un \textit{m\'etodo num\'erico} es un proceso matem\'atico \textit{iterativo} cuyo objetivo es encontrar la aproximaci\'on a una soluci\'on espec\'ifica con un cierto error previamente determinado. Los m\'etodos num\'ericos requieren de una aproximaci\'on a la soluci\'on real al problema, misma que es corregida a trav\'es de la repetici\'on de un cierto proceso que debe arrojar soluciones cada vez m\'as cercanas al valor real. Cada correcci\'on de un valor inicial se conoce como \textit{iteraci\'on}. El proceso es controlado por medio de la medici\'on de una cantidad de error predefinido entre dos aproximaciones sucesivas.

La historia de los m\'etodos num\'ericos es la colecci\'on de acontecimientos matem\'aticos en los que se resuelven problemas sin el uso de la matem\'atica anal\'itica. Algunos de los m\'etodos m\'as utilizados en la actualidad fueron creados mucho antes de la invenci\'on de la computadora; su aplicaci\'on era extenuante y complicada porque cada iteraci\'on requer\'ia de una diversidad de operaciones aritm\'eticas que se realizaban por grupos enteros de calculistas, evidentemente, de forma manual. La historia de los m\'etodos num\'ericos es paralela, al menos desde la mitad del siglo XIX, a la historia de la computaci\'on. Las contribuciones m\'as actuales radican en la creaci\'on de software que minimiza los errores y mejora las aproximaciones de los resultados \cite{Isaacson}.

\begin{itemize}
    \item 1650 a.C. Se crean los Papiros de Rhynd en los que se escribe un m\'etodo para resolver expresiones matem\'aticas sin \'algebra.
    \item 250 a.C. Euclides crea el M\'etodo de Exhausti\'on, que consiste en aproximar figuras geom\'etricas (tri\'angulos, cuadrados, pent\'agonos, etc.) consecutivamente dentro de un c\'irculo para obtener una aproximaci\'on a $\pi$.
    \item Siglo IX d.C. Al Juarismi crea los \textit{algoritmos}.
    \item 1623. John Napier inventa los \textit{huesos de Napier}, que son arreglos pr\'acticos de logaritmos en tablas.
    \item Siglo XVII. Isaac Newton crea los procesos de interpolaci\'on polinomial.
    \item Siglo XVIII. Leibnitz crea el C\'alculo diferencial.
    \item 1768. Euler crea soluciones aproximadas a ecuaciones diferenciales con el principio de la integraci\'on num\'erica. Jacob Stirling y Brook Taylor presentan el C\'alculo de diferencias finitas.
    \item 1822. Charles Babbage inventa la \textit{M\'aquina diferencial}.
    \item 1843. Ada, condesa de Lovelace, publica sus notas sobre la m\'aquina anal\'itica de Charles Babbage.
    \item 1890. (IBM) Tabula el censo estadounidense empleando las m\'aquinas de tarjetas perforadas de Herman Hollerith.
    \item 1931. Vannebar Bush dise\~na el analizador diferencial, un computador anal\'ogico electromec\'anico. En 1945 publicar\'a el art\'iculo \textit{C\'omo podremos pensar} en el que describe la computaci\'on personal.    \item 1937. Alan Turing publica \textit{Sobre los n\'umeros computables}, en el que describe un computador universal. En este mismo a\~no, Howard Aiken propone la construcci\'on de un gran computador y descubre partes de la m\'aquina diferencial de Babbage en Harvard; tambi\'en John Vincent Atanasoff conceptualiza el computador electr\'onico (la cual completar\'a en 1939).
    
    \item 1938. William Hewlett y David Packard crean su empresa en Palo Alto, California, Estados Unidos.
    
    \item 1939. Turing comienza a descifrar los c\'odigos secretos alemanes.
    
    \item 1944. John Von Neumann redacta el primer informe sobre EDVAC. En distintas universidades de Estados Unidos se desarrollan proyectos sobre computadoras cuya aplicaci\'on (secreta) ser\'a apoyar a la milicia en c\'alculos bal\'isticos (ecuaciones diferenciales).
    
    \item 1950. Turing crea su famosa prueba sobre la inteligencia artificial; se suicidar\'a en 1954. J.H. Wilkinson acudi\'o al Laboratorio Nacional de F\'isica de Reino Unido para construir una versi\'on m\'as simple de la m\'aquina de Turing; construy\'o la \textit{ACE (Automatic Computing Engine)} para resolver c\'alculos con matrices.
    
    \item 1953. John W. Backus, empleado de IBM, desarrolla \textit{FORTRAN (Formulae Translating)}, como una alternativa al uso del lenguaje ensamblador; se us\'o por primera vez en una IBM 704.
    
    \item 1958. Se anuncia la creaci\'on de la Agencia de Proyectos de Investigaci\'on Avanzada (ARPA).
    
    \item 1962. Doug Engelbart publica \textit{Aumentar el intelecto humano}; en 1963, junto con Bill English inventar\'a el rat\'on.
    
    \item 1968. Noyce y Moore fundan \textit{INTEL}.
    
    \item 1969. Misi\'on Apolo 11. Katherine Johnson calcula la trayectoria del cohete Mercurio. Dorothy Vaughan se convierte en la supervisora de IBM dentro de la NASA. Mary Jackson es la primera ingeniera aeroespacial en Estados Unidos. Margaret Hamilton escribe el c\'odigo del programa que control\'o la nave. Todas ellas tuvieron una participaci\'on fundamental para que la misi\'on fuera un \'exito.
    
    \item 1970. Investigadores visitantes en el \textit{Argone National Laboratory} de Estados Unidos traducen c\'odigos de \textit{ALGOL} para obtener eigenvalores planteados por Wilkinson para incluirlos en \textit{FORTRAN}. De esta labor nace \textit{EISPACK} en 1976 y posteriormente \textit{LINPACK} en 1976.
    
    \item 1973. Vint Cerf y Bob Kahn completan los protocolos TCP/IP.
    
    \item 1975. Bill Gates y Paul Allen desarrollan el lenguaje de programaci\'on \textit{BASIC}; fundan \textit{Microsoft}. Steve Jobs y Steve Wozniak lanzan el \textit{Apple I}.
    
    \item 1983. Richard Stallman empieza a desarrollar el proyecto \textit{GNU}.
    
    \item 1986. Cleve Moler, a partir de \textit{EISPACK} y \textit{LINPACK}, crea \textit{MATLAB}; funda la empresa \textit{MathWorks}.
    
    \item 1991. Linus Torvalds lanza la primera versi\'on de \textit{Linux}. Tim Berbers-Lee anuncia la \textit{World Wide Web}.
\end{itemize}


\section{Clases}

\subsection{M\'etodos Iterativos}

\subsubsection{Gauss-Jacobi}
\begin{equation*}
A =
\begin{pmatrix}
10 & -1 & 2 & 0\\
-1 & 11 & -1 & 3\\
2 & -1 & 10 & -1\\
0 & 3 & -1 & 8
\end{pmatrix}, \qquad
\mathbf{b} =
\begin{pmatrix}
6\\ 25\\ -11\\ 15
\end{pmatrix}, \qquad
\mathbf{x}^{\*} = (1,2,-1,1)^{T}.
\end{equation*}

Iteraci\'on 
\begin{eqnarray*}
x_1^{(k+1)} &=& \frac{6 + x_2^{(k)} - 2x_3^{(k)}}{10},\\
x_2^{(k+1)} &=& \frac{25 + x_1^{(k)} + x_3^{(k)} - 3x_4^{(k)}}{11},\\
x_3^{(k+1)} &=& \frac{-11 - 2x_1^{(k)} + x_2^{(k)} + x_4^{(k)}}{10},\\
x_4^{(k+1)} &=& \frac{15 - 3x_2^{(k)} + x_3^{(k)}}{8}.
\end{eqnarray*}


Tomamos $\mathbf{x}^{(0)}=(0,0,0,0)^\top$ y actualizamos cada $x_i^{(k+1)}$ usa sólo valores del paso $k$.

\paragraph{Iteración 1:}
\begin{eqnarray*}
x_1^{(1)}&=&\frac{6 + 0 - 20}{10}=0.6, \\
x_2^{(1)}&=&\frac{25 + 0+ 0 - 3\cdot0}{11}=\frac{25}{11}\approx 2.272727,\\
x_3^{(1)}&=&\frac{-11 - 2\cdot0 + 0 + 0}{10}=-1.1, \\
x_4^{(1)}&=&\frac{15 - 3\cdot0 + 0}{8}=\frac{15}{8}=1.875.
\end{eqnarray*}

\paragraph{Iteración 2} Con $\mathbf{x}^{(1)}=(0.6,\,2.272727,\,-1.1,\,1.875)$,
\begin{eqnarray*}
x_1^{(2)}&=&\frac{6 + 2.272727 - 2(-1.1)}{10}=\frac{10.472727}{10}=1.047273,\\
x_2^{(2)}&=&\frac{25 + 0.6 + (-1.1) - 3(1.875)}{11}=\frac{18.875}{11}\approx 1.715909,\\
x_3^{(2)}&=&\frac{-11 - 2(0.6) + 2.272727 + 1.875}{10}=\frac{-8.052273}{10}\approx -0.805227,\\
x_4^{(2)}&=&\frac{15 - 3(2.272727) + (-1.1)}{8}=\frac{7.081819}{8}\approx 0.885227.
\end{eqnarray*}

\paragraph{Iteración 3} Con $\mathbf{x}^{(2)}=(1.047273,\,1.715909,\,-0.805227,\,0.885227)$,
\begin{eqnarray*}
x_1^{(3)}&=&\frac{6 + 1.715909 - 2(-0.805227)}{10}
=\frac{9.326363}{10}=0.932636,\\
x_2^{(3)}&=&\frac{25 + 1.047273 + (-0.805227) - 3(0.885227)}{11}
=\frac{22.586365}{11}=2.053306,\\
x_3^{(3)}&=&\frac{-11 - 2(1.047273) + 1.715909 + 0.885227}{10}
=\frac{-10.493410}{10}=-1.049341,\\
x_4^{(3)}&=&\frac{15 - 3(1.715909) + (-0.805227)}{8}
=\frac{9.047046}{8}=1.130881.
\end{eqnarray*}

\paragraph{Iteración 4} Con $\mathbf{x}^{(3)}=(0.932636,\,2.053306,\,-1.049341,\,1.130881)$,
\begin{eqnarray*}
x_1^{(4)}&=&\frac{6 + 2.053306 - 2(-1.049341)}{10}
=\frac{10.151988}{10}=1.015199,\\
x_2^{(4)}&=&\frac{25 + 0.932636 + (-1.049341) - 3(1.130881)}{11}
=\frac{21.490652}{11}=1.953696,\\
x_3^{(4)}&=&\frac{-11 - 2(0.932636) + 2.053306 + 1.130881}{10}
=\frac{-9.681085}{10}=-0.968109,\\
x_4^{(4)}&=&\frac{15 - 3(2.053306) + (-1.049341)}{8}
=\frac{7.790741}{8}=0.973843.
\end{eqnarray*}

\paragraph{Iteración 5} Con $\mathbf{x}^{(4)}=(1.015199,\,1.953696,\,-0.968109,\,0.973843)$,
\begin{eqnarray*}
x_1^{(5)}&=&\frac{6 + 1.953696 - 2(-0.968109)}{10}
=\frac{9.889914}{10}=0.988991,\\
x_2^{(5)}&=&\frac{25 + 1.015199 + (-0.968109) - 3(0.973843)}{11}
=\frac{22.125561}{11}=2.011415,\\
x_3^{(5)}&=&\frac{-11 - 2(1.015199) + 1.953696 + 0.973843}{10}
=\frac{-10.102859}{10}=-1.010286,\\
x_4^{(5)}&=&\frac{15 - 3(1.953696) + (-0.968109)}{8}
=\frac{8.170803}{8}=1.021351.
\end{eqnarray*}

Tabla de resultados

\begin{center}
\begin{tabular}{c|rrrr}
%%\toprule
k & $x_1^{(k)}$ & $x_2^{(k)}$ & $x_3^{(k)}$ & $x_4^{(k)}$\\
%%\midrule
0 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\
1 & 0.6000 & 2.2727 & -1.1000 & 1.8750 \\
2 & 1.0473 & 1.7159 & -0.8052 & 0.8852 \\
3 & 0.9326 & 2.0533 & -1.0493 & 1.1309 \\
4 & 1.0152 & 1.9537 & -0.9681 & 0.9738 \\
5 & 0.9890 & 2.0114 & -1.0103 & 1.0214 \\
%%\bottomrule
\end{tabular}
\end{center}

Tabla de errores

\begin{center}
\begin{tabular}{c|cc}
%\toprule
k & $\|x^{(k)}-x^{*}|_\infty$ & $\|x^{(k)}-x^{(k-1)}\|_\infty$\\
%\midrule
0 & 2.0000 & -- \\
1 & 0.8750 & 2.2727 \\
2 & 0.2841 & 0.9898 \\
3 & 0.1309 & 0.3374 \\
4 & 0.0463 & 0.1570 \\
5 & 0.0214 & 0.0577 \\
%\bottomrule
\end{tabular}
\end{center}

Ejercicios:instrucciones: usa $x^{(0)}=\mathbf{0}$ y detén cuando $\|x^{(k+1)}-x^{(k)}\|_\infty<10^{-3}$. 
Comprueba dominancia diagonal, escribe las fórmulas de Jacobi e itera con tabla.

\begin{Ejer}
\begin{equation*}
\begin{pmatrix}
5 & 1\\
2 & 6
\end{pmatrix}
\begin{pmatrix}x_1\\x_2\end{pmatrix}
=
\begin{pmatrix}3\\-10\end{pmatrix}.
\end{equation*}
\end{Ejer}


\begin{Ejer}
\begin{equation*}
\underbrace{\begin{pmatrix}
9 & -1 & 0 & 0\\
-1 & 12 & -2 & 1\\
0 & -1 & 11 & -2\\
0 & 2 & -1 & 10
\end{pmatrix}}_{A_{4\times4}}\,
\underbrace{\begin{pmatrix}x_1\\x_2\\x_3\\x_4\end{pmatrix}}_{x}
=
\underbrace{\begin{pmatrix}7\\26\\-15\\15\end{pmatrix}}_{b},
\end{equation*}
\end{Ejer}


\begin{Ejer}

\begin{equation*}
\underbrace{\begin{pmatrix}
10 & -1 & 2 & 0 & 0\\
-1 & 12 & -2 & 3 & 0\\
2 & -1 & 11 & -2 & 1\\
0 & 3 & -1 & 10 & -2\\
0 & 0 & 2 & -1 & 9
\end{pmatrix}}_{A_{5\times5}}\,
\underbrace{\begin{pmatrix}x_1\\x_2\\x_3\\x_4\\x_5\end{pmatrix}}_{x}
=
\underbrace{\begin{pmatrix}15\\-17\\26\\-7\\13\end{pmatrix}}_{b},
\end{equation*}

\end{Ejer}

\subsubsection{Gauss-Seidel}

Considerar

\[
A =
\begin{pmatrix}
9 & -1 & 0 & 2\\
-1 & 10 & -2 & 1\\
0 & -1 & 8 & -1\\
2 & 1 & -1 & 11
\end{pmatrix},\quad
\mathbf{b} =
\begin{pmatrix}
10\\ 8\\ 6\\ 13
\end{pmatrix},\quad
\mathbf{x}^{*}=(1,1,1,1)^\top.
\]
$A$ es diagonalmente dominante por renglones, por lo que Gauss--Seidel converge.

Iteraci\'on general
Con ecuaciones por rengl\'on:
\[
\begin{aligned}
9x_1 - x_2 + 2x_4 &= 10,&
\Rightarrow\;& x_1 \;=\; \frac{10 + x_2 - 2x_4}{9},\\
- x_1 + 10x_2 - 2x_3 + x_4 &= 8,&
\Rightarrow\;& x_2 \;=\; \frac{8 + x_1 + 2x_3 - x_4}{10},\\
- x_2 + 8x_3 - x_4 &= 6,&
\Rightarrow\;& x_3 \;=\; \frac{6 + x_2 + x_4}{8},\\
2x_1 + x_2 - x_3 + 11x_4 &= 13,&
\Rightarrow\;& x_4 \;=\; \frac{13 - 2x_1 - x_2 + x_3}{11}.
\end{aligned}
\]
\textbf{Regla de GS:} cada $x_i^{(k+1)}$ usa los \emph{valores m\'as recientes disponibles} dentro del mismo paso $k+1$.


Partimos de $\mathbf{x}^{(0)}=(0,0,0,0)^\top$.

\paragraph{Iteraci\'on 1}
\[
\begin{aligned}
x_1^{(1)}&=\frac{10+0-20}{9}=1.111111,\\
x_2^{(1)}&=\frac{8+\underline{x_1^{(1)}}+20-0}{10}
=\frac{8+1.111111}{10}=0.911111,\\
x_3^{(1)}&=\frac{6+\underline{x_2^{(1)}}+0}{8}
=\frac{6+0.911111}{8}=0.863889,\\
x_4^{(1)}&=\frac{13-2\underline{x_1^{(1)}}-\underline{x_2^{(1)}}+\underline{x_3^{(1)}}}{11}
=\frac{13-2(1.111111)-0.911111+0.863889}{11}=0.975505.
\end{aligned}
\]

\paragraph{Iteraci\'on 2}
\[
\begin{aligned}
x_1^{(2)}&=\frac{10+x_2^{(1)}-2x_4^{(1)}}{9}
=\frac{10+0.911111-2(0.975505)}{9}=0.995567,\\
x_2^{(2)}&=\frac{8+\underline{x_1^{(2)}}+2x_3^{(1)}-x_4^{(1)}}{10}
=\frac{8+0.995567+2(0.863889)-0.975505}{10}=0.974784,\\
x_3^{(2)}&=\frac{6+\underline{x_2^{(2)}}+x_4^{(1)}}{8}
=\frac{6+0.974784+0.975505}{8}=0.993786,\\
x_4^{(2)}&=\frac{13-2\underline{x_1^{(2)}}-\underline{x_2^{(2)}}+\underline{x_3^{(2)}}}{11}
=\frac{13-2(0.995567)-0.974784+0.993786}{11}=1.002534.
\end{aligned}
\]

\paragraph{Iteraci\'on 3}
\[
\begin{aligned}
x_1^{(3)}&=\frac{10+0.974784-2(1.002534)}{9}=0.996635,\\
x_2^{(3)}&=\frac{8+\underline{0.996635}+2(0.993786)-1.002534}{10}=0.998167,\\
x_3^{(3)}&=\frac{6+\underline{0.998167}+1.002534}{8}=1.000088,\\
x_4^{(3)}&=\frac{13-2\underline{0.996635}-\underline{0.998167}+\underline{1.000088}}{11}=1.000786.
\end{aligned}
\]

\paragraph{Iteraci\'on 4}
\[
\begin{aligned}
x_1^{(4)}&=\frac{10+0.998167-2(1.000786)}{9}=0.999622,\\
x_2^{(4)}&=\frac{8+\underline{0.999622}+2(1.000088)-1.000786}{10}=0.999901,\\
x_3^{(4)}&=\frac{6+\underline{0.999901}+1.000786}{8}=1.000086,\\
x_4^{(4)}&=\frac{13-2\underline{0.999622}-\underline{0.999901}+\underline{1.000086}}{11}=1.000086.
\end{aligned}
\]

\paragraph{Iteraci\'on 5}
\[
\begin{aligned}
x_1^{(5)}&=\frac{10+0.999901-2(1.000086)}{9}=0.999970,\\
x_2^{(5)}&=\frac{8+\underline{0.999970}+2(1.000086)-1.000086}{10}=1.000006,\\
x_3^{(5)}&=\frac{6+\underline{1.000006}+1.000086}{8}=1.000011,\\
x_4^{(5)}&=\frac{13-2\underline{0.999970}-\underline{1.000006}+\underline{1.000011}}{11}=1.000006.
\end{aligned}
\]

Tabla de resultados

\begin{center}
\begin{tabular}{c|rrrr}
%\toprule
k & $x_1^{(k)}$ & $x_2^{(k)}$ & $x_3^{(k)}$ & $x_4^{(k)}$\\
%\midrule
0 & 0.000000 & 0.000000 & 0.000000 & 0.000000 \\
1 & 1.111111 & 0.911111 & 0.863889 & 0.975505 \\
2 & 0.995567 & 0.974784 & 0.993786 & 1.002534 \\
3 & 0.996635 & 0.998167 & 1.000088 & 1.000786 \\
4 & 0.999622 & 0.999901 & 1.000086 & 1.000086 \\
5 & 0.999970 & 1.000006 & 1.000011 & 1.000006 \\
%\bottomrule
\end{tabular}
\end{center}

Tabla de errores

\begin{center}
\begin{tabular}{c|cc}
%\toprule
k & $\|x^{(k)}-x^{*}\|_\infty$ & $\|x^{(k)}-x^{(k-1)}\|_\infty$\\
%\midrule
0 & 1.000000 & -- \\
1 & 0.136111 & 1.111111 \\
2 & 0.025216 & 0.129897 \\
3 & 0.003365 & 0.023383 \\
4 & 0.000378 & 0.002986 \\
5 & 0.000030 & 0.000348 \\
%\bottomrule
\end{tabular}
\end{center}


Instrucciones: usa $x^{(0)}=\mathbf{0}$ y det\'en cuando $\|x^{(k+1)}-x^{(k)}\|_\infty<10^{-3}$. Escribe las f\'ormulas de GS e itera con tabla.


\begin{Ejer}
\[
\underbrace{\begin{pmatrix}
6 & -1\\
1 & 5
\end{pmatrix}}_{A_{2\times2}}
\begin{pmatrix}x_1\\x_2\end{pmatrix}
=
\underbrace{\begin{pmatrix}13\\-3\end{pmatrix}}_{b}.
\]

\end{Ejer}


\begin{Ejer}\[
\underbrace{\begin{pmatrix}
8 & 1 & -1 & 0\\
2 & 10 & -2 & 1\\
1 & -1 & 9 & -1\\
0 & 2 & -1 & 7
\end{pmatrix}}_{A_{4\times4}}
\begin{pmatrix}x_1\\x_2\\x_3\\x_4\end{pmatrix}
=
\underbrace{\begin{pmatrix}6\\-3\\20\\-9\end{pmatrix}}_{b}.
\]

\end{Ejer}


\begin{Ejer}
\[
\underbrace{\begin{pmatrix}
10 & -1 & 0 & 0 & 2\\
-1 & 11 & -1 & 0 & 0\\
0 & -1 & 12 & -2 & 1\\
0 & 0 & -2 & 9 & -1\\
2 & 0 & 1 & -1 & 8
\end{pmatrix}}_{A_{5\times5}}
\begin{pmatrix}x_1\\x_2\\x_3\\x_4\\x_5\end{pmatrix}
=
\underbrace{\begin{pmatrix}10\\21\\1\\-10\\11\end{pmatrix}}_{b}.
\]
\end{Ejer}


\section{Programas y rutinas en R}


\paragraph{Sesión 1}


\subsection{Operadores lógicos}

\begin{verbatim}
17<5
17>5
17<=5
17>=5
17!=5
17==5
\end{verbatim}

\subsection{OPERADORES ARITMETICOS}

\subsubsection{SUMA, RESTA, MULTIPLICACION, DIVISION, POTENCIA, MODULO, DIVISION ENTERA}
\begin{verbatim}
17+5
17*5
17*5
17^5
17%/%5
17%%5
\end{verbatim}

\subsubsection{LOGARITMOS Y EXPONENCIALES}
\begin{verbatim}
log(1)
log(12)
log(12,2)
exp(12)
exp(1)
\end{verbatim}
\subsubsection{FUNCIONES TRIGONOMETRICAS}
\begin{verbatim}
sin(45)
cos(45)
tan(45)
asin(0.96)
acos(0.97)
atan(0.45)
\end{verbatim}
\subsubsection{FUNCIONES VARIAS}
\begin{verbatim}
abs(-34)
sqrt(8)
floor(1.56)
ceiling(1.56)
max(4,7,2,12)
min(4,7,2,12)
sign(-45)
\end{verbatim}
\subsubsection{EJERCICIOS DE PRACTICA}
\begin{verbatim}
# calcular la expresion cos(pi/6+pi/2)+e^2
# calcular la expresion cos(pi/6+pi/2)+e^2*log(5)+arc cos(1/raiz(2))
# introducir las siguientes expresiones: 
# a) 1/7
# b) options(digits=3); 1/7
# c) options(digits=6); 1/7
# d) round(67.45)
# e) round(75.324568,2)
# f) options(digits=7);
# g) signif(56.345458234234,2)
# h) signif(56.345458234234)
# i) exp(-30)
# j) options(scipen= 999)
# k) exp(-30)
# l) options(scipen=0)
\end{verbatim}

\paragraph{Sesión 2}

\subsection{EJERCICIOS DE PRACTICA}

\subsubsection{DEFINICION DE CONSTANTES}
\begin{verbatim}
e = exp(1); 
x = 0.0034
e <- exp(1)
x <- 0.034;
x0 = e^(2*x)
\end{verbatim}
\subsubsection{CONCATENAR Y PEGAR EXPRESIONES}
\begin{verbatim}
txt = "El valor de x0 es _"
cat(txt, x0)
paste(txt,x0)
paste0(txt,x0)
\end{verbatim}
\subsubsection{ASIGNACION E IMPRESION}
\begin{verbatim}
x0 <- 1
x1 <- x0 - pi*x0 + 1 
(x1 <- x0 - pi*x0 + 1 ) 
print(x1)
\end{verbatim}
\subsubsection{LISTADO DE OBJETOS DEFINIDOS}
\begin{verbatim}
ls()
# Eliminar todos los objetos
rm(list= ls())
ls()
\end{verbatim}
\subsubsection{IMPRIMIR PEGAR AVANZADO}
\begin{verbatim}
x0 <- 1
x1 <- x0 - pi*x0 + 1
cat("x0 =", x0, "\n","x1 =", x1) 
\end{verbatim}
\subsubsection{EJERCICIOS DE PRACTICA}



\paragraph{Sesión 3}


\subsubsection{DEFINICION DE FUNCIONES}
\begin{verbatim}
# nombre_funcion <- function(param1,param2,param3,...,paramn){
# instruccion 1
# instruccion 2
# return(valor_de_retorno)
#}
\end{verbatim}

\subsubsection{Ejemplo 1}
\begin{verbatim}
fun1 <- function(x,a,b,h,k){
  res <- a+b*cos(hx+k)
  return(res)
}
\end{verbatim}
\subsubsection{Ejemplo 2}
\begin{verbatim}
Discriminante <- function(a,b,c){
  res <- b^2-4*a*c
  return(res)
}
\end{verbatim}
\subsubsection{GRAFICAS}
\begin{verbatim}
fun2 <- function(x,h,k){
  res <- 1/h*sin(k*x)
  return(res)
}

f2 <- fun2(1:100,2,3)
plot(f2,type="l", col= "red", lwd=2,
     main= "Grafico de la funcion f2",
     xlab= "x",
     ylab="f(x)=1/h*sin(k*x)",
     axes= TRUE)
\end{verbatim}
\subsubsection{EJEMPLOS DE PRACTICA}
Graficar: rectas, parabolas, cubicas, polinomios, exponenciales, logaritmos


\subsection{Introducción a Factorización LU}


\subsubsection{Inversa de una matriz}
\begin{verbatim}

```{r}
A=matrix(c(1,3,-2,1,1,4,-1,2,0,1,-1,4,2,6,1,2),nrow = 4,byrow = TRUE)
(A)
Aorig<- A
```
\end{verbatim}
Paso 1)

\begin{verbatim}

```{r}
I = diag(4);(I)
```
\end{verbatim}

Paso 2)

\begin{verbatim}

```{r}
AInv <- cbind(A,I); (AInv)
A <- AInv
```
\end{verbatim}

Paso 3)

\begin{verbatim}
```{r}
l21 <- A[2,1]/A[1,1];(l21)
l31 <- A[3,1]/A[1,1];(l31)
l41 <- A[4,1]/A[1,1];(l41)
```
\end{verbatim}

Paso 4)

\begin{verbatim}
```{r}
A[2,] <- A[2,]-l21*A[1,]; 
A[3,] <- A[3,]-l31*A[1,]; 
A[4,] <- A[4,]-l41*A[1,]; 
(A)
```
\end{verbatim}

Paso 5)
\begin{verbatim}
```{r}
Atmp <- A[2,]
A[2,] <- A[3,]
A[3,] <- Atmp
(A)
```
\end{verbatim}

Paso 6)

\begin{verbatim}
```{r}
l32 <- A[3,2]/A[2,2];(l32)
l42 <- A[4,2]/A[2,2];(l42)
```

\end{verbatim}

Paso 7)

\begin{verbatim}
```{r}
A[3,] <- A[3,]-l32*A[2,]; 
A[4,] <- A[4,]-l42*A[2,]; 
(A)
```

\end{verbatim}

Paso 8)

\begin{verbatim}
```{r}
esc <- 1/A[3,3]
A[3,] <- esc*A[3,]
(A)
```

\end{verbatim}

Paso 9)

\begin{verbatim}
```{r}
l43 <- A[4,3]/A[3,3];(l43)
```

\end{verbatim}

Paso 10)

\begin{verbatim}
```{r}
A[4,] <- A[4,]-l43*A[3,];
(A)
```

\end{verbatim}

Paso 11)

\begin{verbatim}
```{r}
esc <- 1/A[4,4]
A[4,] <- esc*A[4,]
(A)
```
\end{verbatim}

Paso 12)

\begin{verbatim}
```{r}
l34 <- A[3,4]/A[4,4];(l34); A[3,] <- A[3,]-l34*A[4,]
l24 <- A[2,4]/A[4,4];(l24); A[2,] <- A[2,]-l24*A[4,]
l14 <- A[1,4]/A[4,4];(l14); A[1,] <- A[1,]-l14*A[4,]
(A)
```

\end{verbatim}

Paso 13)

\begin{verbatim}
```{r}
l23 <- A[2,3]/A[3,3];(l23); A[2,] <- A[2,]-l23*A[3,]
l13 <- A[1,3]/A[3,3];(l13); A[1,] <- A[1,]-l13*A[3,]
(A)

```

\end{verbatim}

Paso 14)

\begin{verbatim}
```{r}
l12 <- A[1,2]/A[2,2];(l12); A[1,] <- A[1,]-l12*A[2,]
(A)
```

\end{verbatim}

Paso 15)
\begin{verbatim}
```{r}
AInv <- A[,5:8]; (AInv)
```

\end{verbatim}

Paso 16)
\begin{verbatim}
```{r}
IdCalc <- Aorig%*%AInv; (IdCalc)
```
\end{verbatim}


\subsubsection{Factorización LU}

\begin{Ejem}

\begin{verbatim}
```{r}
A=matrix(c(1,1,1,1,2,3,1,5,-1,1,-5,3,3,1,7,-2),byrow = TRUE,nrow = 4); (A)
```

```{r}
b=matrix(c(10,31,-2,18),nrow = 4);(b)
```

```{r}
Ab <- cbind(A,b); (Ab)
```
\end{verbatim}

Los pivotes se definen como $a_{kk}^{(k)}$, luego construimos los multiplicadores: $l_{i,k}=a_{ik}^{(k)}/a_{kk}^{(k)}$, $l_{21}=a_{21}/a_{aa}$ y $l_{31}=a_{31}/a_{11}$ y $l_{41}=a_{41}/a_{11}$

\begin{verbatim}
```{r}
A <- Ab;
l21 <- A[2,1]/A[1,1];(l21)
l31 <- A[3,1]/A[1,1];(l31)
l41 <- A[4,1]/A[1,1];(l41)
```
\end{verbatim}

Construimos la matriz $U$, donde las entradas $a_{ij}^{(k+1)}=a_{ik}^{(k)}-l_{ik}\times a_{kj}^{(k)}$

\begin{verbatim}
```{r}
A[2,] <- A[2,]-l21*A[1,]; (A[2,])
A[3,] <- A[3,]-l31*A[1,]; (A[3,])
A[4,] <- A[4,]-l41*A[1,]; (A[4,])
```
\end{verbatim}

Es decir la matriz resultante es:

\begin{verbatim}
```{r}
(A)
```
\end{verbatim}

entonces el pivote es $A_{22} = 1$, calculemos los $l_{32}$ y $l_{42}$

\begin{verbatim}
```{r}
l32 <- A[3,2]/A[2,2];(l32)
l42 <- A[4,2]/A[2,2];(l42)
```
\end{verbatim}

Haciendo cero debajo del pivote

\begin{verbatim}
```{r}
A[3,] <- A[3,]-l32*A[2,];(A[3,])
A[4,] <- A[4,]-l42*A[2,];(A[4,])
```
\end{verbatim}

La matriz $A$ queda de la forma:

\begin{verbatim}
```{r}
(A)
```
\end{verbatim}

por tanto el pivote es $A_{33}=-2$, y resta calcylar $l_{43}$

\begin{verbatim}
```{r}
l43 <- A[4,3]/A[3,3];(l43)
```
\end{verbatim}

haciendo cero debajo del pivote

\begin{verbatim}
```{r}
A[4,] <- A[4,]-l43*A[3,];(A[4,])
```
\end{verbatim}

Por lo tanto la matriz $A$ resultante es

\begin{verbatim}
```{r}
(A)
```
\end{verbatim}

entonces podemos construir la matriz $L$ con los valores $l_{ij}$ calculados en los pasos anteriores

\begin{verbatim}
```{r}
L=diag(4);(L)
```

```{r}
L[2,1] <- l21
L[3,1] <- l31
L[4,1] <- l41
L[3,2] <- l32
L[4,2] <- l42
L[4,3] <- l43
(L)
```

```{r}
U <- A[,1:4];(U)
```
\end{verbatim}

Verifiquemos que efectivamente $A=LU$
\begin{verbatim}
```{r}
Acalculada <- L%*%U; (Acalculada)
```
\end{verbatim}
\end{Ejem}


\begin{Ejem}

Apliquemos lo desarrollado para la siguiente matriz $A$
\begin{verbatim}
```{r}
A=matrix(c(4,0,1,1,3,1,3,1,0,1,2,0,3,2,4,1),nrow = 4,byrow = TRUE); (A)
```

```{r}
b=matrix(c(5,6,13,1),nrow = 4);(b)
```
\end{verbatim}

Paso 1)

\begin{verbatim}
```{r}
Ab <- cbind(A,b)
A <- Ab;
```

\end{verbatim}

Paso 2) 

\begin{verbatim}
```{r}
l21 <- A[2,1]/A[1,1];(l21)
l31 <- A[3,1]/A[1,1];(l31)
l41 <- A[4,1]/A[1,1];(l41)
```

\end{verbatim}

Paso 3)
\begin{verbatim}
```{r}
A[2,] <- A[2,]-l21*A[1,]; (A[2,])
A[3,] <- A[3,]-l31*A[1,]; (A[3,])
A[4,] <- A[4,]-l41*A[1,]; (A[4,])
```

\end{verbatim}

Paso 4)
\begin{verbatim}
```{r}
l32 <- A[3,2]/A[2,2];(l32)
l42 <- A[4,2]/A[2,2];(l42)
```

\end{verbatim}

Paso 5)
\begin{verbatim}
```{r}
A[3,] <- A[3,]-l32*A[2,];(A[3,])
A[4,] <- A[4,]-l42*A[2,];(A[4,])
```

\end{verbatim}

Paso 6)
\begin{verbatim}
```{r}
l43 <- A[4,3]/A[3,3];(l43)
```

\end{verbatim}

Paso 7)
\begin{verbatim}
```{r}
A[4,] <- A[4,]-l43*A[3,];(A[4,])
```

\end{verbatim}

Paso 8)
\begin{verbatim}
```{r}
L=diag(4);(L)
```

\end{verbatim}

Paso 9)
\begin{verbatim}
```{r}
L[2,1] <- l21
L[3,1] <- l31
L[4,1] <- l41
L[3,2] <- l32
L[4,2] <- l42
L[4,3] <- l43; 
(L)
```

\end{verbatim}

Paso 10)
\begin{verbatim}
```{r}
U <- A[,1:4];(U)
```

\end{verbatim}

Paso 11)
\begin{verbatim}
```{r}
Acalculada <- L%*%U; (Acalculada)
```
\end{verbatim}

\end{Ejem}

\begin{Ejer}

Aplicar la factorización $LU$ a la matriz A definida por
\begin{verbatim}
```{r}
A=matrix(c(2,3,2,4,
           4,10,-4,0,
           -3,-2,-5,-2,
           -2,4,4,-7),nrow = 4,byrow = TRUE)
(A)
```
\end{verbatim}
\end{Ejer}

\subsubsection{Notas importantes}

\begin{Note}
Si se fija $l_{ii}=1$ se le denomina factorizacion \textit{Doolitle}.
\end{Note}

\begin{Note}
**Nota 2** Una matriz $A$ tiene factorización de Doolitle si y sólo sí se le puede aplicar el metodo de eliminación de Gauss sin pivoteo.
\end{Note}

\begin{Note}
Si $U=L^{T}$ entonces la factorización se le denomina \textit{factorización de Cholesky}.
\end{Note}

\begin{Note}
Si la matriz es simétrica y definida positiva, entonces $A=LL^{T}$.
\end{Note}

\subsection{M\'etodos de Eliminaci\'on directa}

\subsubsection{Gaussiana Simple}


\begin{verbatim}

```{r,echo=FALSE}
gauss_simple <- function(A, b, tolerancia, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  if (!is.numeric(b)) stop("b debe ser numérico.")
  n <- nrow(A)
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (length(b) != n) stop("Longitud de b debe ser igual al número de filas de A.")
  Ab <- cbind(A, b);  
  numcols = ncol(Ab)
  for (k in 1:(n - 1)) {
    pivote <- Ab[k, k]
    if (abs(pivote) < tolerancia) {
      stop(sprintf("Pivote casi cero en fila %d (%.3e). 
      El método sin pivoteo falla.", k, pivote))
    }
    if (k + 1 <= n) {
      for (i in (k + 1):n) {
        m <- Ab[i, k] / pivote;
        Ab[i, k:numcols] <- Ab[i, k:numcols] - m * Ab[k, k:numcols]
        if (abs(Ab[i, k]) < tolerancia) Ab[i, k] <- 0
        if (verbose) {
        cat(sprintf("k=%d, i=%d, m=%.6g\n", k, i, m));
        print(Ab)}
      }
    }
  }
  x <- numeric(n)
  for (i in n:1) {
    if (i == n) {
      suma <- 0
    } else {suma <- sum(Ab[i, (i + 1):n] * x[(i + 1):n])}
    x[i] <- (Ab[i, n + 1] - suma) / Ab[i, i]
  }
  list(x=x,U=Ab[, 1:n],Ab=Ab)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}

```{r,echo=FALSE}
A <- matrix(c(
  2,  1, -1,
 -3, -1,  2,
 -2,  1,  2
), nrow = 3, byrow = TRUE)
b <- c(8, -11, -3);
tolerancia <- 1e-12;
res <- gauss_simple(A, b,tolerancia);
res <- gauss_simple(A, b,tolerancia, verbose = TRUE)
res$x;res$U;res$Ab;A %*% res$x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Gaussiana con pivoteo parcial}

\begin{verbatim}

```{r}
gauss_piv_parcial <- function(A, b, tolerancia, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); 
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (length(b) != n) stop("Dimensiones de b incorrectas.")
  Ab <- cbind(A, b)
  for (k in 1:(n-1)) {
    # Selección del pivote (máximo en valor absoluto en la 
    #columna k desde la fila k)
    max_row <- which.max(abs(Ab[k:n, k])) + (k - 1)
    if (abs(Ab[max_row, k]) < tolerancia){
    stop(sprintf("Pivote casi nulo en columna %d", k))}
    # Intercambio de filas si es necesario
    if (max_row != k) {
      Ab[c(k, max_row), ] <- Ab[c(max_row, k), ]
      if (verbose) {
      cat(sprintf("Intercambio de fila %d con fila %d\n", k, max_row));
      print(Ab)}
    }
    for (i in (k + 1):n) {
      m <- Ab[i, k] / Ab[k, k];
      Ab[i, k:ncol(Ab)] <- Ab[i, k:ncol(Ab)] - m * Ab[k, k:ncol(Ab)]
      if (abs(Ab[i, k]) < tolerancia) Ab[i, k] <- 0
      if (verbose) {
      cat(sprintf("k=%d, i=%d, m=%.6g\n", k, i, m));print(Ab)}
    }
  }
  x <- numeric(n)
  for (i in n:1) {
    if (i == n) {
      suma <- 0
    } else {
    suma <- sum(Ab[i, (i + 1):n] * x[(i + 1):n])
    }
    x[i] <- (Ab[i, n + 1] - suma) / Ab[i, i]
  }
  list(x = x,U = Ab[, 1:n],Ab = Ab)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}

```{r,echo=FALSE}
A <- matrix(c(
  0, 2, 1,
  1, -2, -3,
  -1, 1, 2), nrow = 3, byrow = TRUE)
b <- c(3, -3, -1);
tolerancia <- 1e-12
res <- gauss_piv_parcial(A, b, tolerancia, verbose = TRUE);
res$x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Gauss Jordan}

\begin{verbatim}
```{r}
gauss_jordan <- function(A, b, tolerancia, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); 
  m <- ncol(A)
  if (!is.null(b) && length(b) != n) 
  stop("Dimensiones incompatibles entre A y b.")
  Ab <- cbind(A, as.matrix(b)); 
   ncols <- ncol(Ab); row <- 1
  for (col in 1:m) {
    if (row > n) break
    pivot_row_rel <- which.max(abs(Ab[row:n, col]))
    pivot_row <- row + pivot_row_rel - 1
    if (abs(Ab[pivot_row, col]) < tolerancia) {
      if (verbose) cat(sprintf("Sin pivote usable en col=%d (|piv <tol). Se omite columna.\n", col))
      next
    }
    if (pivot_row != row) {
      Ab[c(row, pivot_row), ] <- Ab[c(pivot_row, row), ]
      if (verbose) {
      cat(sprintf("Swap filas %d <-> %d (col=%d)\n", row,
      pivot_row, col));
      print(Ab)}
    }
    pivote <- Ab[row, col];    
    Ab[row, ] <- Ab[row, ] / pivote
    if (verbose) {
    cat(sprintf("Normaliza fila %d por pivote %.6g (col=%d)\n", 
    row, pivote, col));
    print(Ab)}
    for (r in 1:n) {
      if (r == row) next
      factor <- Ab[r, col]
      if (abs(factor) > tolerancia) {
        Ab[r, ] <- Ab[r, ] - factor * Ab[row, ]
        if (verbose) {
        cat(sprintf("R%d := R%d - (%.6g)*R%d (col=%d)\n", 
        r, r, factor, row, col));print(Ab)}
      }
    }
    row <- row + 1
  }
  Ab[abs(Ab) < tolerancia] <- 0;  
  out <- list(RREF = Ab);  
  X <- Ab[, (m + 1):ncols, drop = FALSE]
  out$x <- if (ncol(X) == 1) as.vector(X) else X
  return(out)
}
```
\end{verbatim}

\begin{Ejem} 
\begin{verbatim}
```{r,echo=FALSE}
A <- matrix(c(
  2,  1, -1,
 -3, -1,  2,
 -2,  1,  2), nrow = 3, byrow = TRUE)
b <- c(8, -11, -3);
tolerancia <- 1e-12;
res <- gauss_jordan(A, b, tolerancia, verbose = TRUE);
res$x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Cálculo de Inversa}
\begin{verbatim}
```{r}
gauss_jordan_inversa <- function(A,tolerancia, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); m <- ncol(A)
  if (n != m) stop("Para invertir, A debe ser cuadrada.")
  Ab <- cbind(A, diag(n));
  ncols <- ncol(Ab);
  row <- 1
  for (col in 1:m) {
    if (row > n) break
    pivot_row_rel <- which.max(abs(Ab[row:n, col]))
    pivot_row <- row + pivot_row_rel - 1
    if (abs(Ab[pivot_row, col]) < tolerancia) {
      stop(sprintf("No hay pivote en la columna %d (|piv|<tol).", 
      col))
    }
    if (pivot_row != row) {
      Ab[c(row, pivot_row), ] <- Ab[c(pivot_row, row), ]
      if (verbose) {
      cat(sprintf("Intercambia filas %d <-> %d (col=%d)\n", 
      row, pivot_row, col)); 
      print(Ab)
      }
    }
    pivote <- Ab[row, col];
    Ab[row, ] <- Ab[row, ] / pivote
    if (verbose) {
    cat(sprintf("Normaliza fila %d por pivote %.6g (col=%d)\n",
    row, pivote, col));
    print(Ab)}
    for (r in 1:n) {
      if (r == row) next
      factor <- Ab[r, col]
      if (abs(factor) > tolerancia) {
        Ab[r, ] <- Ab[r, ] - factor * Ab[row, ]
        if (verbose){
        cat(sprintf("R%d := R%d - (%.6g)*R%d (col=%d)\n", 
        r, r, factor, row, col)); 
        print(Ab)
        }
      }
    }
    row <- row + 1
  }
  Ab[abs(Ab) < tolerancia] <- 0;
  LHS <- Ab[, 1:m, drop = FALSE]
  if (!all(LHS == diag(n))) {
  stop("La matriz es singular o la tolerancia es muy estricta.")
  }
  Ainv <- Ab[, (m + 1):ncols, drop = FALSE]
  list(Ainv = Ainv, RREF = Ab)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
tolerancia <- 1e-12
A <- matrix(c(
  1, 2, 3,
  0, 1, 4,
  5, 6, 0), nrow = 3, byrow = TRUE)
res <- gauss_jordan_inversa(A, tolerancia,
 verbose = TRUE);
 res$Ainv;
 round(A %*% res$Ainv, 6)
```
\end{verbatim}
\end{Ejem}


\subsubsection{Factorización LU}

\subsubsection*{Sin Pivoteo}

\begin{verbatim}
```{r}
lu_simple <- function(A, tol = 1e-12, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); m <- ncol(A)
  if (n != m) stop("A debe ser cuadrada.")
  U <- A;  
  L <- diag(n)
  for (k in 1:(n - 1)) {
    piv <- U[k, k]
    if (abs(piv) < tol) {
    stop(sprintf("Pivote casi cero en k=%d. No se puede continuar.",
    k))
    }
    for (i in (k + 1):n) {
      m <- U[i, k] / piv;
      L[i, k] <- m;
      U[i, k:n] <- U[i, k:n] - m * U[k, k:n]
      if (verbose) {
      cat(sprintf("k=%d, i=%d, m=%.6g\n", k, i, m));
      print(U)
      }
    }
  }
  list(L = L, U = U)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
A <- matrix(c(
  2,  1, -1,
 -3, -1,  2,
 -2,  1,  2
), 3, 3, byrow = TRUE)
lu <- lu_simple(A);lu$L; lu$U
```
\end{verbatim}
\end{Ejem}

\subsubsection*{Con Pivoteo}
\begin{verbatim}

```{r}
lu_piv_parcial <- function(A, tol = 1e-12, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A); m <- ncol(A); 
  if (n != m) stop("A debe ser cuadrada.")
  U <- A;
  L <- diag(n);
  P <- diag(n)
  for (k in 1:(n - 1)) {
    max_row <- which.max(abs(U[k:n, k])) + (k - 1)
    if (abs(U[max_row, k]) < tol) {
    stop(sprintf("Matriz singular (pivote ~ 0).", k))
    }
    if (max_row != k) {
      U[c(k, max_row), ] <- U[c(max_row, k), ];
      P[c(k, max_row), ] <- P[c(max_row, k), ]
      if (k > 1) {
      L[c(k, max_row), 1:(k - 1)] <- L[c(max_row, k), 1:(k - 1)]
      }
      if (verbose) {
      cat(sprintf("Intercambia filas %d <-> %d\n",
      k, max_row)); 
      print(U)
      }
    }
    for (i in (k + 1):n) {
      m <- U[i, k] / U[k, k];
      L[i, k] <- m;
      U[i, k:n] <- U[i, k:n] - m * U[k, k:n];
      if (verbose) {
      cat(sprintf("k=%d, i=%d, m=%.6g\n", k, i, m));
      print(U)
      }
    }
  }
  list(P = P, L = L, U = U)
}
```
\end{verbatim}


\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
A <- matrix(c(
  0,  2,  1,
  1, -2, -3,
 -1,  1,  2), 3, 3, byrow = TRUE)
lu <- lu_piv_parcial(A, verbose = FALSE);
lu$P;
lu$L; 
lu$U
```
\end{verbatim}
\end{Ejem}

\subsubsection*{Solucion vía LU}

\begin{verbatim}
```{r}
forward_sub <- function(L, b) {
  n <- nrow(L);  
  y <- numeric(n)
  for (i in 1:n) {
  y[i] <- (b[i] - sum(L[i, 1:(i - 1)] * y[1:(i - 1)]))
  }
  y
}
```


```{r}
# x en Ux = y
back_sub <- function(U, y, tol = 1e-12) {
  n <- nrow(U);  
  x <- numeric(n)
  for (i in n:1) {
    s <- if (i == n) 0 else sum(U[i, (i + 1):n] * x[(i + 1):n])
    if (abs(U[i, i]) < tol) stop(sprintf("Pivote ~0 en U[%d,%d].", 
    i, i))
    x[i] <- (y - s) / U[i, i]
  }
  x
}
```
\end{verbatim}


\subsubsection*{Resolucion sin pivoteo}

\begin{verbatim}
```{r}
solve_lu_simple <- function(A, b, tol = 1e-12) {
  lu <- lu_simple(A, tol = tol);
  y <- forward_sub(lu$L, b)
  x <- back_sub(lu$U, y, tol = tol)
  x
}
```
\end{verbatim}

\subsubsection{Resolucion con pivoteo}

\begin{verbatim}
```{r}
solve_lu_piv_parcial <- function(A, b, tol = 1e-12) {
  lu <- lu_piv_parcial(A, tol = tol);
  Pb <- lu$P %*% b
  y  <- forward_sub(lu$L, as.vector(Pb))
  x  <- back_sub(lu$U, y, tol = tol)
  x
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
A <- matrix(c(
  0,  2,  1,
  1, -2, -3,
 -1,  1,  2), 3, 3, byrow = TRUE)
b <- c(3, -3, -1); 
x <- solve_lu_piv_parcial(A, b);
x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Factorización de Cholesky}

\begin{verbatim}
```{r}
tolerancia <- 1e-12
cholesky_fact <- function(A,tolerancia) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A)
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (!all(abs(A - t(A)) < tolerancia)) stop("A debe ser simétrica.")
  L <- matrix(0, n, n)
  for (j in 1:n) {
    suma <- sum(L[j, 1:(j-1)]^2)
    val <- A[j, j] - suma
    if (val <= 0) stop("A no es definida positiva (falló Cholesky).")
    L[j, j] <- sqrt(val)
    if (j < n) {
      for (i in (j+1):n) {
        suma <- sum(L[i, 1:(j-1)] * L[j, 1:(j-1)])
        L[i, j] <- (A[i, j] - suma) / L[j, j]
      }
    }
    cat(sprintf("Paso j=%d\n", j))
    print(L)
  }
  return(L)
}
```
\end{verbatim}


\begin{Ejem}
\begin{verbatim}
```{r,echo=FALSE}
# Matriz simétrica definida positiva
A <- matrix(c(
  4,  12, -16,
 12,  37, -43,
-16, -43,  98), nrow = 3, byrow = TRUE)

L <- cholesky_fact(A,tolerancia)
L
```
\end{verbatim}
\end{Ejem}

\subsubsection{Solución vía Cholesky}

\begin{verbatim}
```{r}
tolerancia <- 1e-12
solve_cholesky <- function(A, b, tolerancia) {
  L <- cholesky_fact(A, tolerancia)
  y <- forward_sub(L, b)
  x <- back_sub(t(L), y, tolerancia)
  x
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}

```{r,echo=FALSE}
b <- c(1, 2, 3)
x <- solve_cholesky(A, b,tolerancia)
x
A %*% x   
```
\end{verbatim}
\end{Ejem}

\subsection{Métodos Iterativos}

\subsubsection{Gauss Jacobi}

\begin{verbatim}
```{r}
jacobi <- function(A, b, x0 = NULL, tol = 1e-8, maxiter = 1000) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A)
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (length(b) != n) stop("Dimensiones de b incompatibles.")
  if (is.null(x0)) x0 <- rep(0, n)
  x <- x0;  
  D <- diag(diag(A));  
  R <- A - D;
  for (k in 1:maxiter) {
    x_new <- (b - R %*% x) / diag(D)
    cat(sprintf("Iter %d: %s\n", k, 
    paste(round(x_new, 6), collapse = " ")))
    if (sqrt(sum((x_new - x)^2)) < tol) {
    return(list(x = as.vector(x_new), 
    iter = k, convergencia = TRUE))}
    x <- x_new
  }
  list(x = as.vector(x), 
  iter = maxiter, 
  convergencia = FALSE)
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}


```{r,echo=FALSE}
A <- matrix(c(
  4, -1,  0,
 -1,  4, -1,
  0, -1,  3), 3, 3, byrow = TRUE)

b <- c(15, 10, 10)
res_jacobi <- jacobi(A, b, x0 = c(0, 0, 0), tol = 1e-8); 
res_jacobi$x
```
\end{verbatim}
\end{Ejem}

\subsubsection{Gauss Seidel}

\begin{verbatim}
```{r}
gauss_seidel <- function(A, b, x0 = NULL, 
tol = 1e-8, maxiter = 1000, verbose = FALSE) {
  if (!is.matrix(A)) stop("A debe ser una matriz.")
  n <- nrow(A)
  if (ncol(A) != n) stop("A debe ser cuadrada.")
  if (length(b) != n) stop("Dimensiones de b incompatibles.")
  x <- if (is.null(x0)) rep(0, n) else as.numeric(x0)
  if (length(x) != n) stop("Dimensión de x0 incompatible con A.")
  for (k in 1:maxiter) {
    x_old <- x
    for (i in 1:n) {
      aii <- A[i, i]
      if (abs(aii) < .Machine$double.eps) {
      stop(sprintf("A[%d,%d] = 0: Gauss–Seidel No se puede aplicar",
       i, i))}
      s1 <- if (i > 1) sum(A[i, 1:(i - 1)] * x[1:(i - 1)]) else 0
      s2 <- if (i < n) sum(A[i, (i + 1):n] * x_old[(i + 1):n]) else 0
      x[i] <- (b[i] - s1 - s2) / aii
    }
    dx  <- sqrt(sum((x - x_old)^2));
    res <- sqrt(sum((b - A %*% x)^2))
    cat(sprintf("Iter %4d |dx|=%.3e  |res|=%.3e   x=%s\n",
    k, dx, res, paste(round(x, 6), collapse=" ")))
    if (dx < tol && res < tol) {
      return(list(x = as.vector(x), iter = k,
      convergencia = TRUE,delta = dx, residuo = res))
    }
  }
  list(x = as.vector(x), iter = maxiter, convergencia = FALSE,
       delta = sqrt(sum((x - x)^2)), 
       residuo = sqrt(sum((b - A %*% x)^2)))
}
```
\end{verbatim}

\begin{Ejem}
\begin{verbatim}

```{r,echo=FALSE}
A <- matrix(c(
  4, -1,  0,
 -1,  4, -1,
  0, -1,  3
), 3, 3, byrow = TRUE)
b <- c(15, 10, 10)
res_gs <- gauss_seidel(A, b, x0 = c(0,0,0), tol = 1e-8);
res_gs$x
```
\end{verbatim}
\end{Ejem}





\begin{thebibliography}{9}

\bibitem{Isaacson} 
Isaacson, W. (2014). 
\textit{Los innovadores: Los genios que inventaron el futuro}. 
Debate.

\end{thebibliography}

\end{document}

