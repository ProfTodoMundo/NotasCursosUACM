% !TEX root = NotasCursoProbabilidad.tex

%===========================================
\section{Fundamentos}
%===========================================

\begin{Def}
Par\'ametro es una Caracter\'istica de la poblaci\'on medias y varianzas en la
dist. normal o binomial. Si se conocen sus valores
o se puede aproximar con suficiente precisi\'on, se puede responder
cualquier pregunta sobre la probabilidad.
\end{Def}

\begin{Def}
Estad\'istico: Cualquier funci\'on de la muestra, p.e. media o varianza
muestral.
\end{Def}

\begin{Def} 
Estimadores: Son estad\'isticos indep. de los par\'ametros de la pob, y se utilizan para aprox. Si $\theta$ es el par\'ametro de inter\'es, su estimador es denotado por $\hat{\theta}$.

Por ejemplo: dist normal para $\mu \leftarrow \bar{X} = \hat{\mu}$ y
$\sigma^2 \leftarrow S^2 = \hat{\sigma}^2$. $\bar{X}$, $S^2$ son estimadores puntuales de $\mu$ y $\sigma^2$
p/dist. normal.
\end{Def}

\begin{Def}

Muestreo: Se considerar\'a el muestreo aleatorio simple para
determinar el tama\~no de la muestra, $n$.
\end{Def}


Los estimadores se obtienen a partir de una MAS:
$X_1, X_2, \ldots, X_n$. de  la v.a. $\bar{X}$. P/cada muestreo los res. ser\'an dif.,
por ser realizados a partir de una muestra aleatoria,
los estimadores son aleatorios y por tanto tienen una dist.,
se le denomina dist. de muestreo.

\begin{Ejem}
Dist. Binomial:

Sup. $X \sim \textrm{Bin}(n,p)$, $p = P\{X=1\}$, $n$ = tama\~no de la muestra.

Para determinar $p$ se selecciona una MAS
$X_1, X_2, \ldots, X_n$ de v.a. $\textrm{Bin}(1,p)=\text{Bern}(p)$.
La proporci\'on muestral est\'a dada por:

\[
\hat{p} = \sum_{i=1}^n \frac{X_i}{n}
\]

Proporci\'on muestral $\hat{p}$ es una v.a. para
$n$ suf. grande:
\[
\hat{p} \sim N\left(p,\frac{p(1-p)}{n}\right)
\]
(Tmas l\'imite)

El error t\'ipico: estimador
\[
ET(\hat{p}) = \sqrt{\frac{p(1-p)}{n}},
\]
$p$ desconocido y por tanto
$ET(\hat{p})$ tambi\'en.

P.e. Si $X \sim \text{Bin}(15,p)$ y se quiere estimar $p$,
lo es. P/ aprox. se puede sustituir $p$ por $\hat{p}$.

Tomar 100 muestras de tama\~no 100
$(X_1, X_2, \ldots, X_{100})$ y se calcula la prop. muestral
en c/u de ellas, obt. 100 valores
para $\hat{p}$.

\[
\hat{\mu} = 0.7 \qquad \text{y} \qquad \hat{\sigma}^2 = \frac{0.7 \times 0.3}{100}
\]
\end{Ejem}

\begin{Ejem}
2) Dist. Normal \quad $N(\mu,\sigma^2)$

Def.: $X \sim N(\mu,\sigma^2)$. Sup. $X_1, X_2, \ldots, X_n$ MAS,
$X \sim N(\mu,\sigma^2)$.

\[
\hat{\mu} = \bar{X} = \frac{1}{n}\sum_{i=1}^n X_i \quad \text{T.O.}
\]

\[
\bar{X} \sim N\left(\mu,\frac{\sigma^2}{n}\right)
\quad \Rightarrow \quad
Z_1 = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)
\tag{1}
\]

Esto es v\'alido si la varianza pob. $\sigma^2$ es conocida.

\[
E(\bar{X})=\mu
\qquad
ET(\bar{X})=\frac{\sigma}{\sqrt{n}},
\quad \text{ejemplo } n=20,100 \text{ y } 500
\]

\[
\Rightarrow \quad \bar{X} \sim N(\mu=5,\sigma^2=4/n)
\]

Si $\sigma^2$ es desconocida,

\[
\hat{\sigma}^2 \leftarrow S^2
= \frac{1}{n}\sum_{i=1}^n (X_i-\bar{X})^2
\tag{2}
\]

\[
s^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2
\tag{3}
\]

En (1) $\sigma^2 \to s^2$:

\[
\frac{\bar{X}-\mu}{S/\sqrt{n}}
\sim
\begin{cases}
t_{n-1}, & n \le 30 \\
N(0,1), & n > 30
\end{cases}
\]

y

\[
ET(\bar{X})=\frac{S}{\sqrt{n-1}} \simeq \frac{S}{\sqrt{n}}
\quad \text{(error residual)}
\]

1) $\sigma^2$ conocida:
\[
\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)
\]

2) $\sigma^2$ desc., $n>30$:
\[
\frac{\bar{X}-\mu}{S/\sqrt{n}} \sim N(0,1)
\]

3) $\sigma^2$ desc., $n\le 30$:
\[
\frac{\bar{X}-\mu}{S/\sqrt{n}} \sim t_{n-1}
\]
3) Est. de varianza $\sigma^2$:

Si $X_1, X_2, \ldots, X_n$ MAS de v.a. $N(\mu,\sigma^2)$, entonces

\[
\frac{nS^2}{\sigma^2} \sim \chi^2_n
\qquad \text{o} \qquad
\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}
\]

Para $n$ peque\~na.

Si $n$ es suf. grande se puede aprox $\chi^2_n$
por $N(n,2n)$.
\end{Ejem}


Un estimador de un par\'ametro pob es funci\'on de la muestra.

Es preciso dar una definici\'on de las prop. para que
el estimador sea bueno.

Sea $X_1, X_2, \ldots, X_n$ una MAS de tama\~no $n$, se dice que
$\hat{\theta}$ es un estimador de $\theta$ si es estad\'istico calculado
p/ cualquier dicho par\'ametro desconocido es ese.

\[
\hat{\theta} = h(X_1, X_2, \ldots, X_n)
\]


3) Sea $X$ v.a. con dist. $F_X(x)$, $F_X(x;\theta)$,
$\theta$ par\'ametro desconocido.

Sea $X_1, X_2, \ldots, X_n$ MAS de tama\~no $n$.
A $\hat{\theta} = h(X_1, X_2, \ldots, X_n)$ se le
llama estimador puntual de $\theta$, $\hat{\theta}$
es una v.a. por ser funci\'on de v.a.
y por tanto tiene dist.

Al seleccionar una muestra, $\hat{\theta}$ toma un valor
llamado estimaci\'on puntual.

Se dice que $\hat{\theta}$ es un estimador insesgado de
$\theta$ si
\[
E(\hat{\theta})=\theta
\]

Si $\hat{\theta}$ no es insesgado $\Rightarrow$
\[
E(\hat{\theta})-\theta
\]
es el sesgo del estimador.

De los posibles estimadores, el de menor varianza es
considerado mejor estimador (MVUE).
Si es insesgado $\Rightarrow$ (UMVUE) es mejor estimador de
$\theta$.

Para $X_1, X_2, \ldots, X_n$ MAS de $N(\mu,\sigma^2)$,
$\mu$ es UMVUE.

Para $X_1, X_2, \ldots, X_n$ MAS de $N(\mu,\sigma^2)$,
\[
\hat{\sigma}_{\hat{\theta}} = \sqrt{V(\hat{\theta})}
\]

El error est\'andar.

Para MAS $N(\mu,\sigma^2)$,
\[
\bar{X} \sim N\left(\mu,\frac{\sigma^2}{n}\right)
\quad \text{con error est\'andar}
\]

\[
\hat{\sigma}_{\bar{X}} = \frac{S}{\sqrt{n}}
\]
\end{Ejem}


Def. 1. Dado un espacio de probabilidad
\[
(\Omega,\mathcal{A},P),
\]
una variable aleatoria.

Una v.a. $X$ es una funci\'on
\[
X:(\Omega,\mathcal{A}) \to \mathbb{R}
\quad \text{t.q.} \quad
A_x=\{\omega: X(\omega)\le x\}\in\mathcal{A},
\ \forall x\in\mathbb{R}.
\]

Def. 2. Sea $\Omega$ espacio muestral, y v.a.
La funci\'on de prob inducida por $Y$ es
\[
P_Y(y=y_i)=P[Y=y_i]
= P_\Omega\{\omega\in\Omega: Y(\omega)=y_i\}.
\]

El espacio muestral para $Y$ es
\[
S_Y=\{y_i\in\mathbb{R}\mid \exists s\in\Omega,\ Y(s)=y_i\}.
\]

Def. La poblaci\'on es el conjunto (grupo) de
elementos de los cuales se desea tener informaci\'on.
La muestra es una parte de la poblaci\'on bajo
estudio.

Def. La funci\'on distribuci\'on acumulada (CDF)
de la v.a. $X$ es
\[
F_X(x)=P[X\le x], \quad \forall x\in\mathbb{R}.
\]

\[
F_X(x)=P[X\le x]
= P\{\omega: X(\omega)\le x\}, \quad \forall x\in\mathbb{R}.
\]

Prop. de la FDA (CDF) $F_X(\cdot)$:

a) 
\[
F_X(-\infty)=\lim_{x\to -\infty}F_X(x)=0,
\qquad
F_X(\infty)=\lim_{x\to \infty}F_X(x)=1.
\]

b) $F_X(\cdot)$ es mon\'otona no decreciente, i.e.
\[
F_X(a)\le F_X(b), \quad \text{para } a<b.
\]

c) $F_X(\cdot)$ es continua por la derecha:
\[
\lim_{h\to 0^+}F_X(x+h)=F_X(x).
\]


Def. Sea $X$ v.a.

a) $X$ es una v.a. discreta si s\'olo
puede tomar una cantidad finita o
numerable de valores distintos, i.e.
su rango es discreto.

Def. de prob.: Sea $\mathcal{A}$ $\sigma$-\'algebra en $\Omega$,
$P:\mathcal{A}\to[0,1]$ es una funci\'on de conjuntos t.q.

1) $P(A)\ge 0 \quad \forall A\in\mathcal{A}$

2) $P(\Omega)=1$

3) Si $\{A_\alpha\}_{\alpha=1}^\infty$ son eventos
mutuamente excluyentes,
\[
A_\alpha\cap A_{\alpha^{'}}=\varnothing \quad \text{para } \alpha\neq\alpha^{'},
\]
entonces
\[
P\left(\bigcup_{\alpha=1}^\infty A_\alpha\right)
= \sum_{\alpha=1}^\infty P(A_\alpha).
\]

Def. Si $X$ es una v.a. discreta con valores
$x_1,x_2,\ldots,x_n,\ldots$, la funci\'on $f_X(\cdot)$
definida por

\[
f_X(x)=
\begin{cases}
P[X=x_j], & x=x_j,\quad j=1,2,\ldots \\
0, & x\neq x_j
\end{cases}
\]

es llamada funci\'on de densidad discreta de $X$.

Los valores $x_j$ son llamados puntos de masa de $X$
y $f_X(x_j)$ son las masas asociadas a los puntos de
masa $x_j$.


Teo. Si $X$ es una v.a. discreta, $F_X(\cdot)$
puede obtenerse de $f_X(\cdot)$ y viceversa.

Nota: La CDF $F_X(\cdot)$ es una funci\'on escal\'on.

Def.

Def. Una v.a. $X$ es continua si existe una
funci\'on $f_X(\cdot)$ t.q.
\[
F_X(x)=\int_{-\infty}^{x} f_X(u)\,du,
\quad \forall x\in\mathbb{R}.
\]

Def. Si $X$ es una v.a. continua, la funci\'on
\[
f_X(x)=\frac{d}{dx}F_X(x)
\]
es llamada funci\'on de densidad de probabilidad de $X$.

Teo. Si $X$ tiene densidad $f_X(\cdot)$, entonces
\[
f_X(x)=\frac{d}{dx}F_X(x)=F'_X(x).
\]

Def.

Def. Sea $X$ v.a., la media de $X$, $\mu_X$,
$E[X]$ se define por

\[
E[X]=\sum_{j=1}^{\infty} x_j f_X(x_j)
\qquad \text{VAD}
\]

\[
E[X]=\int_{-\infty}^{\infty} x f_X(x)\,dx
\qquad \text{VAC}
\]



Si $g$ es una funci\'on de $\bar{X}$, entonces

\[
E[g(X)] = \sum_{x:g(x)\ge 0} g(x)\,f_X(x)
\]

\[
E[g(X)] = \int_{-\infty}^{\infty} g(x)\,f_X(x)\,dx
\]

siempre que la integral exista.

Def. Si $E[X^2]$ existe, entonces la varianza de la v.a. $X$ es

\[
\mathrm{Var}(X)=V[X]=E\big[X-(E[X])^2\big]
\]

y la desviaci\'on est\'andar

\[
SD(X)=\sqrt{V[X]}.
\]

Si $E[X^2]$ no existe, la varianza no existe.

Teo.
\[
V[Y]=E[Y^2]-(E[Y])^2.
\]

Def. Sea $X$ v.a., $\mu_X=E[X]$, la varianza $\sigma_X^2$
se define por

\[
\mathrm{Var}[X]
= \sum_{j=1}^{\infty} (x_j-\mu_X)^2\,f_X(x_j)
\]

\[
\mathrm{Var}[X]
= \int_{-\infty}^{\infty} (x-\mu_X)^2\,f_X(x)\,dx.
\]


Teo. Sea $X$ v.a. entonces

i) $E[c]=c$, $c$ constante.

ii) $E[cg(X)] = c\,E[g(X)]$.

iii) $E[c_1 g_1(X)+c_2 g_2(X)]
= c_1 E[g_1(X)] + c_2 E[g_2(X)]$.

iv) $E[g_1(X)] \le E[g_2(X)]$ si $g_1(x)\le g_2(x)$
$\forall x$.

Teo. Sea $X$ v.a. con $g(\cdot)\ge 0$, $g:\mathbb{R}\to\mathbb{R}$,

\[
P[g(X)\ge k] \le \frac{E[g(X)]}{k},
\qquad \forall k>0.
\]

(Chebyshov)

Corolario: (Des. Chebyshev)
Si $X$ v.a. con varianza finita,

\[
P\left(|X-\mu_X|\ge r\sigma_X\right)
= P\left((X-\mu_X)^2 \ge r^2\sigma_X^2\right)
\le \frac{1}{r^2},
\quad r>0.
\]

Obs.
\[
P\left(|X-\mu_X|<r\sigma_X\right)
\ge 1-\frac{1}{r^2}.
\]

Teo.
\[
V[aY+b]=a^2V[Y].
\]

Def. La FGM de la v.a. $X$ es

\[
M(t)=E[e^{tY}]
\]

y por Taylor alrededor,

\[
\mu_r = E[X^r]
= \sum \int e^{ty} f(y)
= \int_{-\infty}^{\infty} e^{ty} f_Y(y)\,dy.
\]


La funci\'on caracter\'istica (CF) de $\bar{X}$

\[
C(t)=E\!\left[e^{itX}\right],
\qquad i=\sqrt{-1}.
\]

Prop. Sea $X$ v.a. con MGF $M_X(t)$ que existe para
$|t|<b$, $b>0$. Entonces

\[
C(t)=M_X(it).
\]

Def. Sean $X,Y$ v.a., $X,Y$ son v.a.i.d.
$X\sim Y$ o $Y\sim F_X$ si $F_Y(y)=F_X(y)$,
$\forall y\in\mathbb{R}$.

Prop. Sean $X,Y$ v.a., $X$ y $Y$ son id\'enticamente
distribuidas si se cumple cualquiera de las sig.
condiciones:

a) $F_X(y)=F_Y(y)$ \quad $\forall y$

b) $f_X(y)=f_Y(y)$ \quad $\forall y$

c) $C_X(t)=C_Y(t)$ \quad $\forall t$

d) $M_X(t)=M_Y(t)$ \quad $\forall t$ en una vecindad
de cero.

Def. El $k$-\'esimo momento de $X$ es $E[X^k]$
y su $k$-\'esimo momento central es

\[
E\!\left[(X-E[X])^k\right].
\]


Teo. Sup. MGF $m(t)$ existe para $|t|<b$,
$b$ constante positiva. Sup. $k$-\'esima derivada
$m^{(k)}(t)$ existe para $|t|<b$. Entonces

\[
E[X^k]=m^{(k)}(0).
\]

Def. El $q$-\'esimo cuantil de una v.a. $\bar{X}$,
$q_\alpha$, es el m\'inimo valor $\xi$ t.q.
\[
F_X(\xi)\ge q.
\]

Def. La mediana de una v.a. $\bar{X}$,
$\mathrm{Med}_X=\xi_{0.5}$.

Nota. El 3er momento sobre la media es llamada
medida de simetr\'ia o sesgo.

\[
\gamma=\frac{\mu_3}{\sigma^3}
\]
es el coeficiente de sesgo.

\[
\frac{\mu_4}{\sigma^4}-3
\]
coeficiente de exceso de kurtosis.

Def. Sea $f(y)=f_Y(y\mid\theta)$ la PDF de $Y$ v.a.
\[
\mathcal{Y}=\{y\mid f(y)>0\}
\]
es el soporte de valores de $Y$.

Sea $\Theta$ el conjunto de par\'ametros
$\theta$ de inter\'es.

$\mathcal{H}$ es el espacio de par\'ametros de $Y$.


Def. La funci\'on indicadora

\[
\mathbf{1}_A(x)=\mathbf{1}(x\in A)=
\begin{cases}
1, & x\in A,\\
0, & x\notin A.
\end{cases}
\]

Nota: Algunas PMF / PDF / CDF est\'an
restringidas solamente en el soporte.
Ap\'endice 2.

Def. Sea $f_Y(y)$ PDF de $Y$ v.a.
Sup.
\[
f_Y(y\mid \theta)=c(\theta)\,k(y\mid \theta),
\]
entonces $k(y\mid \theta)$ es el kernel de $f_Y$
y $c(\theta)>0$ es el t\'ermino constante que hace
que $f_Y$ sume / integre $1$. As\'i,

\[
\int_{-\infty}^{\infty} k(y\mid \theta)\,dy
= \frac{1}{c(\theta)}.
\]

\[
\sum_{y\in \mathcal{Y}} k(y\mid \theta)
= \frac{1}{c(\theta)}.
\]

Sup. $Y$ es v.a. con $f_Y(\cdot)$ PDF,

\[
E[g(Y)]
= \int_{-\infty}^{\infty} g(y)\,f(y\mid \theta)\,dy
= \int_{y\in \text{soporte}} g(y)\,f(y\mid \theta)\,dy.
\]

Sup. que desp. de operaciones,
\[
E[g(Y)]
= a\,c(\theta)\int_{-\infty}^{\infty} k(y\mid \theta)\,dy,
\quad a \ \text{cte}.
\]


De manera similar si $f_Y$ es PMF,

\[
E[g(Y)] = \sum_{y\in \mathcal{Y}} g(y)\,f(y\mid \theta),
\]
y soporte de $Y$.

Sup. que desp. operaciones algebraicas,
\[
E[g(Y)] = a\,c(\theta)\sum_{y\in \mathcal{Y}} k(y\mid \tau),
\quad a \ \text{cte}.
\]

\[
\Rightarrow \quad
E[g(Y)]
= a\,c(\theta)\sum_{y\in \mathcal{Y}} \frac{c(\tau)}{c(\tau)}\,k(y\mid \tau)
\]

\[
= a\,\frac{c(\theta)}{c(\tau)}
\sum_{y\in \mathcal{Y}} c(\tau)\,k(y\mid \tau)
= a\,\frac{c(\theta)}{c(\tau)}.
\]

Ej. La funci\'on $\zeta(v)$ tiene PMF

\[
f(y)=P[Y=y]=\frac{1}{y^v\,\zeta(v)},
\qquad v>1,\; y=1,2,3,\ldots
\]

\[
\zeta(v)=\sum_{y=1}^{\infty}\frac{1}{y^v}.
\]

Luego, para $v>1$,

\[
E[Y]
= \sum_{y=1}^{\infty} y\,\frac{1}{\zeta(v)}\,\frac{1}{y^v}
= \frac{1}{\zeta(v)}\sum_{y=1}^{\infty} y^{1-v}
\]

\[
= \frac{\zeta(v-1)}{\zeta(v)}
\sum_{y=1}^{\infty}\frac{1}{y^{v-1}}
= \frac{\zeta(v-1)}{\zeta(v)}.
\]



Mezclas de distribuciones

Def. La dist. de una v.a. $\bar{X}$ es una mezcla
si la CDF de $\bar{X}$ es de la forma

\[
F_{\bar{X}}(x)=\sum_{i=1}^{k}\alpha_i\,F_{X_i}(x),
\qquad 0<\alpha_i<1,\quad \sum_{i=1}^{k}\alpha_i=1,
\quad k\ge 2,
\]

y $F_{X_i}(x)$ es la CDF de una v.a. $X_i$,
$i=1,2,\ldots,k$.

Def. Sea $\bar{X}$ v.a. con CDF $F_{\bar{X}}(x)$.
Sea $h$ funci\'on t.q. $E[h(\bar{X})]$ existe.
Entonces

\[
E[h(\bar{X})]
= \int_{-\infty}^{\infty} h(x)\,dF_{\bar{X}}(x).
\]

Prop.

a) Si $\bar{X}$ es una v.a. discreta con PMF
$f_{\bar{X}}(x)$, tiene soporte $\mathcal{Y}$, entonces

\[
E[h(\bar{X})]
= \int_{-\infty}^{\infty} h(x)\,dF_{\bar{X}}(x)
= \sum_{y\in \mathcal{Y}} h(y)\,f_{\bar{X}}(y).
\]

b) Si $\bar{X}$ es una v.a. continua con CDF
$F_{\bar{X}}(x)$ y PDF $f_{\bar{X}}(x)$, entonces

\[
E[h(\bar{X})]
= \int_{-\infty}^{\infty} h(x)\,dF_{\bar{X}}(x)
= \int_{-\infty}^{\infty} h(x)\,f_{\bar{X}}(x)\,dx.
\]

c) Si $\bar{X}$ es una v.a. con dist. mezcla y

\[
F_{\bar{X}}(x)=\sum_{i=1}^{k}\alpha_i\,F_{X_i}(x),
\]

entonces

\[
E[h(\bar{X})]
= \int_{-\infty}^{\infty} h(x)\,dF_{\bar{X}}(x)
= \sum_{i=1}^{k}\alpha_i\,E_{X_i}[h(X_i)].
\]


Ej. Sup. CDF de $X$ es
\[
F_X(x)=(1-\varepsilon)\,\Phi(x)
+ \varepsilon\,\Phi(x\mid k),
\]
$0<\varepsilon<1$ y $\Phi(x)$ es la CDF de
$W_1\sim N(0,1)$, entonces
\[
\Phi(x\mid k)
\]
es la CDF de $W_2\sim N(0,k^2)$.

$E[Y]=?$ Usar $h(y)=y$. Entonces

\[
E[Y]=(1-\varepsilon)E[W_1]+\varepsilon E[W_2]
=(1-\varepsilon)0+\varepsilon(0)=0.
\]

Para encontrar $E[Y^2]$ utilizar $h(y)=y^2$.

\[
E[Y^2]
=(1-\varepsilon)E[W_1^2]+\varepsilon E[W_2^2]
=(1-\varepsilon)1+\varepsilon k^2
=1-\varepsilon+\varepsilon k^2.
\]

Entonces
\[
V[Y]=E[Y^2]-E^2[Y]
=1-\varepsilon+\varepsilon k^2.
\]

\bigskip
\textbf{Distribuci\'on discreta.}

1) 
\[
f(x)=f(x;N)=
\begin{cases}
\frac{1}{N}, & x=1,2,\ldots,N,\\
0, & \text{otro caso}.
\end{cases}
\]

$X$ v.a. uniforme.

\[
E[X]=\frac{N+1}{2},
\qquad
V[X]=\frac{N^2-1}{12}.
\]

\[
M_X(t)=E[e^{tX}]
=\sum_{x=1}^{N} e^{tx}\,\frac{1}{N}.
\]


2) Si $X$ v.a. con

\[
f_X(x)=f_X(x;p)=
\begin{cases}
p\,x^{p-1}, & 0<x<1,\\
0, & \text{otro caso},
\end{cases}
\qquad 0\le p\le 1,
\]

\[
F_X(x)=\int_{0}^{x} p\,t^{p-1}\,dt
= x^{p}, \qquad 0<x<1.
\]

Sea $X\sim \text{Beta}(p)$.

\[
E[X]=p,
\qquad
V[X]=\frac{p(1-p)}{(p+1)}.
\]

3) Sea $X$ v.a. con

\[
f_X(x)=f_X(x;n,p)
=\binom{n}{x}p^x(1-p)^{n-x},
\quad x=0,1,\ldots,n.
\]

$X\sim \text{Bin}(n,p)$.

\[
E[X]=np,
\qquad
V[X]=np(1-p).
\]

4) Sea $X$ v.a. con

\[
f_X(x)=f_X(x;\lambda)
=\frac{(\lambda x)^k e^{-\lambda x}}{k!},
\quad x=0,1,2,\ldots
\]

$X\sim \text{Poisson}(\lambda)$.

\[
E[X]=\lambda,
\qquad
V[X]=\lambda.
\]

5) Sea $X$ v.a. con

\[
f_X(x;\mu,n)
=\frac{\left(\frac{n}{\mu}\right)^n x^{n-1}
e^{-nx/\mu}}{(n-1)!},
\quad x>0.
\]

$X\sim \text{Gamma}(n,\mu)$.

\[
E[X]=\mu,
\qquad
V[X]=\frac{\mu^2}{n}.
\]

6) Sea $X$ v.a. con

\[
f_X(x;\alpha,\beta)
=\frac{1}{\beta}
\left(\frac{x}{\beta}\right)^{\alpha-1}
e^{-(x/\beta)^\alpha},
\quad x>0.
\]

$X\sim \text{Weibull}(\alpha,\beta)$.

\[
E[X]=\beta\,\Gamma\!\left(1+\frac{1}{\alpha}\right),
\qquad
V[X]=\beta^2
\left[
\Gamma\!\left(1+\frac{2}{\alpha}\right)
-\Gamma^2\!\left(1+\frac{1}{\alpha}\right)
\right].
\]


Distribuciones continuas

6) Binomial negativa

\[
P(X=x\mid r,p)
= \binom{x-1}{r-1} p^{\,r}(1-p)^{x-r},
\qquad x=r,r+1,\ldots
\]

\[
E[X]=\frac{r}{p},
\qquad
\mathrm{Var}[X]=\frac{r(1-p)}{p^{2}},
\]

\[
M_X(t)=\left(\frac{p}{1-(1-p)e^{t}}\right)^{r}.
\]

7) Distribuci\'on geom\'etrica

\[
P(X=x\mid p)=p(1-p)^{x-1},
\qquad x=1,2,\ldots
\]

\[
E[X]=\frac{1}{p},
\qquad
\mathrm{Var}[X]=\frac{1-p}{p^{2}},
\]

\[
M_X(t)=\frac{pe^{t}}{1-(1-p)e^{t}}.
\]

8) Distribuci\'on uniforme

\[
f(x\mid a,b)=
\begin{cases}
\frac{1}{b-a}, & a<x<b,\\
0, & \text{otro caso}.
\end{cases}
\]

\[
E[X]=\frac{a+b}{2},
\qquad
\mathrm{Var}[X]=\frac{(b-a)^{2}}{12},
\]

\[
M_X(t)=\frac{e^{bt}-e^{at}}{(b-a)t}.
\]

9) Distribuci\'on gamma

\[
f(x\mid \alpha,\beta)
=\frac{1}{\Gamma(\alpha)\beta^{\alpha}}
x^{\alpha-1}e^{-x/\beta},
\qquad x>0.
\]

\[
E[X]=\alpha\beta,
\qquad
\mathrm{Var}[X]=\alpha\beta^{2},
\]

\[
M_X(t)=(1-\beta t)^{-\alpha},
\qquad t<\frac{1}{\beta}.
\]

10) Distribuci\'on exponencial \quad ($\alpha=1$)

\[
f(x\mid \beta)=\frac{1}{\beta}e^{-x/\beta},
\qquad x>0.
\]

\[
E[X]=\beta,
\qquad
\mathrm{Var}[X]=\beta^{2},
\]

\[
M_X(t)=\frac{1}{1-\beta t},
\qquad t<\frac{1}{\beta}.
\]
5) Distribuci\'on Weibull

Sea $Y=X^{1/r}$.

\[
f_Y(y\mid \beta)
= \frac{r}{\beta^{r}}\, y^{r-1} e^{-(y/\beta)^r},
\qquad y>0,\ r>0,\ \beta>0
\]

\[
E[Y]=\beta\,\Gamma\!\left(1+\frac{1}{r}\right),
\qquad
\mathrm{Var}[Y]
= \beta^{2}\!\left[
\Gamma\!\left(1+\frac{2}{r}\right)
-\Gamma^{2}\!\left(1+\frac{1}{r}\right)
\right]
\]

6) Distribuci\'on normal general

\[
X\sim N(\mu,\sigma^{2})
\]

\[
f(x\mid \mu,\sigma^{2})
= \frac{1}{\sqrt{2\pi\sigma^{2}}}
\exp\!\left(
-\frac{(x-\mu)^{2}}{2\sigma^{2}}
\right)
\]

\[
E[X]=\mu,
\qquad
\mathrm{Var}[X]=\sigma^{2},
\qquad
M_X(t)=\exp\!\left(\mu t + \frac{\sigma^{2}t^{2}}{2}\right)
\]

7) Distribuci\'on Beta

\[
f(x\mid \alpha,\beta)
= \frac{1}{B(\alpha,\beta)}\,
x^{\alpha-1}(1-x)^{\beta-1},
\qquad 0<x<1,\ \alpha>0,\ \beta>0
\]

\[
B(\alpha,\beta)
= \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
\]

\[
E[X]=\frac{\alpha}{\alpha+\beta},
\qquad
\mathrm{Var}[X]
= \frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}
\]

8) Distribuci\'on Gamma

\[
f(x\mid \alpha,\beta)
= \frac{1}{\Gamma(\alpha)\beta^{\alpha}}
x^{\alpha-1} e^{-x/\beta},
\qquad x>0
\]

\[
E[X]=\alpha\beta,
\qquad
\mathrm{Var}[X]=\alpha\beta^{2}
\]

\[
M_X(t)=(1-\beta t)^{-\alpha},
\qquad t<\frac{1}{\beta}
\]

9) Dist. Lognormal

Sea $X$ v.a. y sea $Y=\log X$ v.a.

\[
\log X \sim N(\mu,\sigma^{2})
\]

\[
f(x\mid \mu,\sigma^{2})
= \frac{1}{x\sqrt{2\pi\sigma^{2}}}
\exp\!\left(
-\frac{(\log x-\mu)^{2}}{2\sigma^{2}}
\right),
\qquad x>0
\]

\[
E[X]=e^{\mu+\frac{\sigma^{2}}{2}},
\qquad
\mathrm{Var}[X]
= \big(e^{\sigma^{2}}-1\big)e^{2\mu+\sigma^{2}}
\]

10) Dist. Doble exponencial

\[
f(x\mid \mu,b)
= \frac{1}{2b} e^{-|x-\mu|/b},
\qquad x\in\mathbb{R},\ b>0
\]

\[
E[X]=\mu,
\qquad
\mathrm{Var}[X]=2b^{2}
\]

11) Distribuci\'on log\'istica

\[
f(x\mid \alpha,\beta)
= \frac{e^{-(x-\alpha)/\beta}}
{\beta\big(1+e^{-(x-\alpha)/\beta}\big)^{2}},
\qquad x\in\mathbb{R},\ \beta>0
\]

\[
E[X]=\alpha,
\qquad
\mathrm{Var}[X]=\frac{\pi^{2}}{3}\beta^{2}
\]

12) Dist. Pareto

\[
f(x\mid x_{0},\theta)
= \frac{\theta x_{0}^{\theta}}{x^{\theta+1}},
\qquad x\ge x_{0},\ \theta>0
\]

\[
E[X]=\frac{\theta x_{0}}{\theta-1},
\qquad \theta>1
\]

\[
\mathrm{Var}[X]
= \frac{\theta x_{0}^{2}}
{(\theta-1)^{2}(\theta-2)},
\qquad \theta>2
\]

13) Dist. Gumbel

\[
f_X(x;\alpha,\beta)
= \frac{1}{\beta}
\exp\!\left(
-\frac{x-\alpha}{\beta}
- e^{-(x-\alpha)/\beta}
\right),
\qquad \beta>0
\]

Distribuci\'on Normalizada

Sea
\[
I = \int_{-\infty}^{\infty} e^{-z^{2}/2}\,dz
\]

\[
I^{2}
= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
e^{-(z_{1}^{2}+z_{2}^{2})/2}\,dz_{1}\,dz_{2}
\]

Cambio a coordenadas polares:

\[
z_{1}=r\cos\theta,
\qquad
z_{2}=r\sin\theta,
\qquad
dz_{1}\,dz_{2}=r\,dr\,d\theta
\]

\[
I^{2}
= \int_{0}^{2\pi}\int_{0}^{\infty}
e^{-r^{2}/2} r\,dr\,d\theta
\]

\[
= \int_{0}^{2\pi} d\theta
\int_{0}^{\infty} r e^{-r^{2}/2}\,dr
\]

Sea
\[
u=\frac{r^{2}}{2}
\quad \Rightarrow \quad
du=r\,dr
\]

\[
\int_{0}^{\infty} r e^{-r^{2}/2}\,dr
= \int_{0}^{\infty} e^{-u}\,du = 1
\]

\[
I^{2}=2\pi
\quad \Rightarrow \quad
I=\sqrt{2\pi}
\]

\[
\int_{-\infty}^{\infty}
\frac{1}{\sqrt{2\pi}}e^{-z^{2}/2}\,dz = 1
\]

Sea $X$ v.a. con f.d.p. $f(x)$.

I)
\[
f(x)=x e^{-x^{2}/2}, \qquad x>0
\]

II)
\[
f(x)=\frac{1}{\sqrt{\pi}} e^{-x^{2}}, \qquad x\in\mathbb{R}
\]

III)
\[
f(x)=\frac{4}{\pi} x^{2} e^{-x^{2}}, \qquad x>0
\]

IV)
\[
f(x)=\frac{4}{\sqrt{\pi}} x^{2} e^{-x^{2}}, \qquad x>0
\]

V)
\[
f(x)=2x e^{-x^{2}}, \qquad x>0
\]

VI)
\[
f(x)=\frac{2}{\sqrt{\pi}} e^{-x^{2}}, \qquad x>0
\]

VII)
\[
f(x)=\frac{4}{\sqrt{\pi}} x e^{-x^{2}}, \qquad x>0
\]

VIII)
\[
f(x)=2x^{3} e^{-x^{2}}, \qquad x>0
\]

IX)
\[
f(x)=c e^{-x^{2}}, \qquad x\in\mathbb{R}
\]

X)
\[
f(x)=k x^{2} e^{-x^{2}}, \qquad x>0
\]

---

Sea
\[
I=\int_{0}^{\infty} e^{-z^{2}}\,dz
\]

\[
I^{2}
= \int_{0}^{\infty}\int_{0}^{\infty}
e^{-(z_{1}^{2}+z_{2}^{2})}\,dz_{1}\,dz_{2}
\]

Cambio a coordenadas polares:

\[
z_{1}=r\cos\theta,
\qquad
z_{2}=r\sin\theta,
\qquad
dz_{1}dz_{2}=r\,dr\,d\theta
\]

\[
I^{2}
= \int_{0}^{\pi/2}\int_{0}^{\infty}
e^{-r^{2}} r\,dr\,d\theta
\]

\[
= \int_{0}^{\pi/2} d\theta
\int_{0}^{\infty} r e^{-r^{2}}\,dr
\]

Sea
\[
u=r^{2}
\quad \Rightarrow \quad
du=2r\,dr
\]

\[
\int_{0}^{\infty} r e^{-r^{2}}\,dr
= \frac{1}{2}\int_{0}^{\infty} e^{-u}\,du
= \frac{1}{2}
\]

\[
I^{2}=\frac{\pi}{4}
\quad \Rightarrow \quad
I=\frac{\sqrt{\pi}}{2}
\]

$(X)\,dx \sim X$ i.s.

\[
\frac{k}{T}=\theta
=\left(\frac{2k}{T},\frac{k}{T}\right)
=\theta
\le \frac{2k}{T}=0
\qquad k=n
\]

\[
\frac{2k}{T}=0
\le \frac{2k}{T}
= \frac{2k}{2}
= \left(\frac{y}{T}\right)
-\frac{2k}{2}
= E[Y]
\]

\[
\frac{2k}{2}
= \frac{k}{T}
= M_X(1)
\]

\[
M_X(t)
= \frac{2(x-t)}{2x(x-t)}
= \frac{(x-t)}{2x(x-t)}
\]

\[
\frac{2(x-t)}{x}
\]

\[
M_X(t)
= \frac{x-t}{x}
\]

\[
M_X(t)
= \frac{2(x-t)}{x}
\]

\[
M_X(t)
= \frac{x-t}{x}
\]

\[
M_X(t)
= \frac{x-t}{x}
\]

\[
M_X'(t)
= -\frac{1}{x}
\]

\[
M_X''(t)
= \frac{2(x-t)}{x^2}
\]

\[
M_X'(1)
= -\frac{1}{x}
\]

\[
\int_0^\infty e^{-(x-t)x}\,dx
\]

\[
\int_0^\infty x e^{-x^2}\,dx
\]

\[
\int_0^\infty x^2 e^{-x^2}\,dx
\]

\[
E[X]
= \int_0^\infty x e^{-x^2}\,dx
\]

\[
E[X^2]
= \int_0^\infty x^2 e^{-x^2}\,dx
\]

\[
K(X|\theta)
\sim \text{exp}(\theta)
\]

\[
K(X|\theta)=\lambda e^{-\lambda x}
\]

\[
f_X(x)=\lambda e^{-\lambda x},\quad x>0
\]

\[
\int_0^\infty \lambda e^{-\lambda x}\,dx
\]

\[
E[X]
=\int_0^\infty x\lambda e^{-\lambda x}\,dx
\]

\[
=\frac{1}{\lambda}
\]

\[
E[X^2]
=\int_0^\infty x^2\lambda e^{-\lambda x}\,dx
\]

\[
=\frac{2}{\lambda^2}
\]

\[
V[X]
=\frac{1}{\lambda^2}
\]

\[
f(x)=k e^{-2x},\quad x>0
\]

\[
\int_0^\infty k e^{-2x}\,dx
\]

\[
k=\frac{1}{2}
\]

\[
E[X]
=\int_0^\infty x\frac{1}{2}e^{-2x}\,dx
\]

\[
=\frac{1}{4}
\]

\[
\theta=\frac{1}{\lambda}
\]

\[
\theta=\frac{X}{2}
\]

\[
\theta=\frac{X}{2}
\]

\[
K(\theta|X)
\sim \text{exp}(k)
\]

(24)

\[
\theta = (\mu,\sigma^2)
\]

\[
\lambda = \frac{1}{2}
\]

\[
X \sim \text{Exp}\left(\frac{1}{2}\right)
\]

\[
f_X(x;\theta)
\]

\[
\Phi =
\begin{pmatrix}
\mu = \frac{1}{\lambda} \\
\sigma^2 = \frac{1}{\lambda^2}
\end{pmatrix}
\]

\[
f(x) = \frac{1}{2} e^{-x/2}, \quad x>0
\]

\[
\int_0^\infty f(x)\,dx
= \int_0^\infty \frac{1}{2} e^{-x/2}\,dx
\]

\[
u = \frac{x}{2}
\quad \Rightarrow \quad
du = \frac{1}{2} dx
\]

\[
\int_0^\infty \frac{1}{2} e^{-x/2}\,dx
= \int_0^\infty e^{-u}\,du
= 1
\]

\[
E[X]
= \int_0^\infty x f(x)\,dx
= \int_0^\infty x \frac{1}{2} e^{-x/2}\,dx
\]

\[
u = \frac{x}{2}
\quad \Rightarrow \quad
x = 2u
\]

\[
E[X]
= \int_0^\infty 2u e^{-u}\,du
\]

\[
= 2 \int_0^\infty u e^{-u}\,du
\]

\[
= 2
\]

\[
E[X^2]
= \int_0^\infty x^2 f(x)\,dx
= \int_0^\infty x^2 \frac{1}{2} e^{-x/2}\,dx
\]

\[
= \int_0^\infty 4u^2 e^{-u}\,du
\]

\[
= 4 \int_0^\infty u^2 e^{-u}\,du
\]

\[
= 8
\]

\[
\operatorname{Var}(X)
= E[X^2] - (E[X])^2
= 8 - 4
= 4
\]

\[
\mu = 2,
\qquad
\sigma^2 = 4
\]

\[
M_X(t)
= \int_0^\infty e^{tx} \frac{1}{2} e^{-x/2}\,dx
\]

\[
= \int_0^\infty \frac{1}{2} e^{x(t - 1/2)}\,dx
\]

\[
= \int_0^\infty \frac{1}{2} e^{-x(1/2 - t)}\,dx
\]

\[
= \frac{1}{2} \int_0^\infty e^{-x(1/2 - t)}\,dx
\]

\[
= \frac{1}{2} \cdot \frac{1}{(1/2 - t)}
\]

\[
M_X(t)
= \frac{1}{1 - 2t}
\]

\[
t < \frac{1}{2}
\]

% P\'agina 22

\[
f_X(x) = x^{n-1} e^{-x/\lambda}
\]

\[
f_X(x) = x^{n-1} e^{-x/\lambda}
\]

\[
X \sim \text{Gamma}(n,\lambda)
\]

\[
\mathbb{E}[X] = n\lambda,
\qquad
\mathrm{Var}(X) = n\lambda^2
\]

\[
M_X(t) = \frac{1}{(1-\lambda t)^n}
\]

Supongamos ahora
\[
f_X(x) = c\, x e^{-x^2}, \qquad x \ge 0
\]

Averiguar $c$.

\[
\int_0^{\infty} c\, x e^{-x^2}\,dx = 1
\]

Sea $u = x^2$, entonces $du = 2x\,dx$.

\[
\int_0^{\infty} x e^{-x^2}\,dx
= \frac{1}{2}\int_0^{\infty} e^{-u}\,du
= \frac{1}{2}
\]

Por lo tanto,
\[
c = 2
\]

Luego,
\[
f_X(x) = 2x e^{-x^2}, \qquad x \ge 0
\]

\[
\int_0^{\infty} e^{-x^2}\,dx = \frac{\sqrt{\pi}}{2}
\]

\[
\int_{-\infty}^{\infty} e^{-x^2}\,dx = \sqrt{\pi}
\]

\[
\int_0^{\infty}\int_0^{\infty} e^{-(x^2+y^2)}\,dx\,dy
= \frac{\pi}{4}
\]

Pasando a coordenadas polares:
\[
x = r\cos\theta,
\qquad
y = r\sin\theta
\]

\[
\int_0^{2\pi}\int_0^{\infty} e^{-r^2} r\,dr\,d\theta
\]

\[
= 2\pi \int_0^{\infty} r e^{-r^2}\,dr
\]

\[
= 2\pi \cdot \frac{1}{2}
= \pi
\]

% P\'agina 23

\textbf{Def.} Sea $X$ una v.a. con funci\'on de distribuci\'on $F_X(x)$.
Se define la funci\'on de probabilidad acumulada para $X$ como
\[
F_X(x) = \mathbb{P}(X \le x).
\]

Definici\'on:
\[
F_X(a,b) = \mathbb{P}(a < X \le b).
\]

Las funciones acumuladas $F_X(x)$ satisfacen las siguientes propiedades:
\[
F_X(b) = \mathbb{P}(X \le b),
\qquad
F_X(a) = \mathbb{P}(X \le a).
\]

Entonces
\[
F_X(a,b) = F_X(b) - F_X(a).
\]

Caso discreto

\textbf{Def.} Sean $(X_1,X_2)$ v.a. discretas.
Se define la funci\'on de distribuci\'on conjunta como
\[
F_{X_1,X_2}(x,y)
= \mathbb{P}(X_1 \le x,\; X_2 \le y).
\]

\textbf{Def.} La funci\'on de densidad de probabilidad conjunta de
$X_1,X_2$ se denota por $f_{X_1,X_2}(x,y)$.

La funci\'on de densidad marginal de $X_1$ es
\[
f_{X_1}(x) = \sum_y f_{X_1,X_2}(x,y).
\]

La funci\'on de densidad marginal de $X_2$ es
\[
f_{X_2}(y) = \sum_x f_{X_1,X_2}(x,y).
\]

\textbf{Def.} La funci\'on de densidad condicional de $X_1$ dado $X_2=y$ es
\[
f_{X_1|X_2}(x|y)
= \frac{f_{X_1,X_2}(x,y)}{f_{X_2}(y)},
\qquad f_{X_2}(y) > 0.
\]

An\'alogamente,
\[
f_{X_2|X_1}(y|x)
= \frac{f_{X_1,X_2}(x,y)}{f_{X_1}(x)},
\qquad f_{X_1}(x) > 0.
\]

% P\'agina 24

\textbf{Def.} Decimos que $X,Y$ son continuas
si existen funciones $f_{X,Y}(x,y)$ tales que
\[
\forall A \subset \mathbb{R}^2,
\qquad
\mathbb{P}\{(X,Y)\in A\}
= \int\!\!\!\int_A f_{X,Y}(x,y)\,dx\,dy.
\]

$f_{X,Y}(x,y)$ es funci\'on de densidad de probabilidad
conjunta de $X$ y $Y$.

La densidad de $X$ se obtiene observando
\[
\mathbb{P}\{X \in \Delta\}
= \int_{-\infty}^{\infty}\!\!
\left( \int_{\Delta} f_{X,Y}(x,y)\,dx \right) dy
= \int_{\Delta} f_X(x)\,dx.
\]

De donde
\[
f_X(x)
= \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dy.
\]

An\'alogamente,
\[
f_Y(y)
= \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dx.
\]

\textbf{Def.} La densidad condicional de $X$ dado $Y=y$ es
\[
f_{X|Y}(x|y)
= \frac{f_{X,Y}(x,y)}{f_Y(y)},
\qquad f_Y(y)>0.
\]

La densidad condicional de $Y$ dado $X=x$ es
\[
f_{Y|X}(y|x)
= \frac{f_{X,Y}(x,y)}{f_X(x)},
\qquad f_X(x)>0.
\]

Nota:
La variable $X_1,\ldots,X_k$ se dice
$k$-dimensional si
\[
(x_1,\ldots,x_k) \in \mathbb{R}^k.
\]

Si existe una funci\'on de densidad conjunta
$f_{X_1,\ldots,X_k}(x_1,\ldots,x_k)$, entonces
\[
\mathbb{P}\{(X_1,\ldots,X_k)\in A\}
= \int\!\!\cdots\!\!\int_A
f_{X_1,\ldots,X_k}(x_1,\ldots,x_k)\,
dx_1\cdots dx_k.
\]

\textbf{Def.} La variable aleatoria $(X_1,\ldots,X_k)$
es absolutamente continua si existe
$f_{X_1,\ldots,X_k}(x_1,\ldots,x_k)$ tal que
\[
\mathbb{P}\{(X_1,\ldots,X_k)\in A\}
= \int\!\!\cdots\!\!\int_A
f_{X_1,\ldots,X_k}(x_1,\ldots,x_k)\,
dx_1\cdots dx_k,
\quad \forall A.
\]

% P\'agina 25

\textbf{Nota:} Sean $(X_1,X_2)$ v.a. continuas en $\mathbb{R}^2$,
sea $f_{X_1,X_2}(x_1,x_2)$ su densidad conjunta.
\[
\mathbb{P}\{(X_1,X_2)\in A\}
= \int\!\!\int_A f_{X_1,X_2}(x_1,x_2)\,dx_1\,dx_2.
\]

Sea
\[
R=\{(x_1,x_2): a_1 \le x_1 \le b_1,\; a_2 \le x_2 \le b_2\}.
\]

Entonces
\[
\mathbb{P}\{a_1 \le X_1 \le b_1,\; a_2 \le X_2 \le b_2\}
= \int_{a_2}^{b_2}\int_{a_1}^{b_1}
f_{X_1,X_2}(x_1,x_2)\,dx_1\,dx_2.
\]

\textbf{Ej.}
Considere
\[
f(x,y)=k(x+y),
\qquad 0<x<1,\; 0<y<1,
\]
y $f(x,y)=0$ en otro caso.

Primero,
\[
1=\int_0^1\int_0^1 k(x+y)\,dx\,dy.
\]

Luego,
\[
\int_0^1\int_0^1 (x+y)\,dx\,dy
= \int_0^1\left(\int_0^1 x\,dx + \int_0^1 y\,dx\right)dy.
\]

Se obtiene
\[
k=\frac{1}{\frac{1}{2}}=2.
\]

Por lo tanto,
\[
f(x,y)=2(x+y).
\]

Calcular:
\[
\mathbb{P}\{X<\tfrac{1}{2}\}
= \int_0^{1/2}\int_0^1 2(x+y)\,dy\,dx.
\]

Adem\'as,
\[
\int_0^{1/2}\int_0^1 2(x+y)\,dy\,dx
= \frac{3}{8}.
\]

\[
\text{Vale } y=x+y.
\]

% P\'agina 26

\textbf{Teo:} Sean $X,Y$ v.a.c.c. y $f_{X,Y}(x,y)$ su densidad conjunta.
Se procede a obtener $f_X(x)$ y $f_Y(y)$.

\textbf{Def:}
\[
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dy,
\qquad
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dx.
\]

\textbf{Def:} Sean $X,Y$ v.a.c.c. con densidad conjunta $f_{X,Y}(x,y)$.
Entonces, para todo conjunto medible $A \subset \mathbb{R}$,
\[
\mathbb{P}\{X \in A\}
= \int_A \left( \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dy \right) dx.
\]

An\'alogamente,
\[
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dx.
\]

\textbf{Teo:}
Sean $X_1, X_2, \ldots, X_n$ v.a. independientes.
Entonces la densidad conjunta es
\[
f_{X_1,\ldots,X_n}(x_1,\ldots,x_n)
= \prod_{i=1}^n f_{X_i}(x_i),
\qquad x_i \ge 0.
\]

\textbf{Ej:}
Sea
\[
f_{X,Y}(x,y) = 2(x+y), \qquad 0<x<1,\; 0<y<1.
\]

Entonces
\[
f_X(x) = \int_0^1 2(x+y)\,dy,
\qquad
f_Y(y) = \int_0^1 2(x+y)\,dx.
\]

Adem\'as,
\[
\int_0^1 2(x+y)\,dy
= 2x + 1.
\]

\[
\int_0^1 2(x+y)\,dx
= 2y + 1.
\]

\textbf{Obs:}
\[
\int_0^1 f_X(x)\,dx = 1,
\qquad
\int_0^1 f_Y(y)\,dy = 1.
\]

% P\'agina 27

\[
\int_0^\infty \left( \int_0^\infty (x+y) \, dy \right) dx
= \int_0^\infty \left[ x + \frac{y^2}{2} \right]_{0}^{\infty} dx
\]

\[
f_X(x) = \int_0^\infty (x+y)\,dy
\]

\[
= \int_0^\infty x\,dy + \int_0^\infty y\,dy
\]

\[
= x \int_0^\infty dy + \int_0^\infty y\,dy
\]

\[
f_Y(y) = \int_0^\infty (x+y)\,dx
\]

\[
= \int_0^\infty x\,dx + y \int_0^\infty dx
\]

\[
\Rightarrow f_{X,Y}(x,y) = f_X(x) f_Y(y)
\]

\textbf{Ej:}
Una f\'abrica produce cuerdas que se venden y que se miden en metros.
Sean $X$ y $Y$ las longitudes respectivas.

\[
f_{X,Y}(x,y) = \frac{2}{3}(2x+3y), \quad 0<x<2,\; 0<y<1
\]

a)

\[
f_X(x) = \int_0^1 \frac{2}{3}(2x+3y)\,dy
\]

\[
= \frac{2}{3} \left[ 2x y + \frac{3}{2} y^2 \right]_0^1
\]

\[
= \frac{2}{3} \left( 2x + \frac{3}{2} \right)
\]

\[
f_Y(y) = \int_0^2 \frac{2}{3}(2x+3y)\,dx
\]

\[
= \frac{2}{3} \left[ x^2 + 3xy \right]_0^2
\]

\[
= \frac{2}{3} (4 + 6y)
\]

\textbf{Sol:}

a)

\[
\mathbb{P}(X+Y \le 1)
\]

% P\'agina 28

\textbf{Ej:}

Se seleccionan al azar 2 refrescos.
Probabilidad de que ambos sean rojos.

Sean $X = \#$ de refrescos verdes,
$Y = \#$ de refrescos rojos.

a)

\[
\mathbb{P}\big( (X,Y) \in \Delta \big) = ?
\]

\[
\Delta = \{(x,y) : x+y \le 1\}
\]

\textbf{Sol:}

\[
\mathbb{R}^2 = \{(0,0),(0,1),(0,2),(1,0),(1,1),(2,0)\}
\]

\[
C_2^8 = \frac{8!}{2!6!}
\]

\[
f(0,1) = \frac{(2)(1)(3)}{(3)(7)}
\]

\[
= \frac{6}{21}
\]

\[
f(0,0) = \frac{(3)(2)}{(5)(7)}
\]

\[
= \frac{6}{35}
\]

\[
\frac{(3)(2)}{(5)(7)} = \frac{6}{35}
\]

\[
f_{X,Y}(x,y)
\]

\[
\begin{array}{c|ccc|c}
X \backslash Y & 0 & 1 & 2 & I \\
\hline
0 & \frac{7}{28} & \frac{3}{14} & \frac{7}{28} & \frac{1}{2} \\
1 & \frac{4}{28} & \frac{3}{14} & 0 & \frac{15}{28} \\
2 & \frac{3}{28} & 0 & 0 & \frac{3}{28} \\
\hline
 & \frac{5}{14} & \frac{3}{7} & \frac{1}{28} &
\end{array}
\]

\[
f_X(x)
\]

\[
\sum_{x,y} f_{X,Y}(x,y)
\]

\[
\sum_{x} f_X(x)
\]

\[
f_{X,Y}(x,y)
\]

% P\'agina 29

\textbf{Ej:} Definamos

\[
f_{X,Y}(x,y) = \frac{2}{5}(2x+3y), \qquad \mathbf{1}_{\{0 \le x \le 1\}} \mathbf{1}_{\{0 \le y \le 1\}}
\]

\[
f_X(x) = \int_0^\infty f_{X,Y}(x,y)\,dy
\]

\[
f_X(x) = \int_0^1 \frac{2}{5}(2x+3y)\,dy
\]

\[
f_Y(y) = \int_0^\infty f_{X,Y}(x,y)\,dx
\]

\[
f_Y(y) = \int_0^1 \frac{2}{5}(2x+3y)\,dx
\]

\textbf{Def:}

Sea $X,Y$ v.a. con $\Delta$ dist. continua.

Dado que $X=x$ se tiene

\[
f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}, \qquad f_X(x) > 0
\]

\[
f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}, \qquad f_Y(y) > 0
\]

\textbf{Notas:}

\[
\mathbb{P}(a < X < b \mid Y = y) =
\int_a^b f_{X|Y}(x|y)\,dx
\]

\textbf{Ej:} 3.18 (libro)

\[
f_{X,Y}(x,y) = \frac{3}{7}
\]

\[
f_{X,Y}(x,1) = \frac{f_{X,Y}(x,1)}{f_Y(1)}
\]

\[
f_{X|Y}(x|1) =
\frac{f_{X,Y}(x,1)}{f_Y(1)} =
\frac{3}{7}
\]

\[
f_{X|Y}(x|1) =
f_{X,Y}(x,1) + f_{X,Y}(2,1)
\]

\[
= \frac{3}{7}
\]

% P\'agina 30

\textbf{Nota:}

\[
\begin{array}{c|ccc}
x & 0 & 1 & 2 \\
\hline
f_X(x) & \frac{1}{2} & \frac{1}{2} & 0
\end{array}
\]

\[
f_{X,Y}(x,y) = 10xy^2 \, \mathbf{1}_{\{0 \le x \le 1\}} \mathbf{1}_{\{0 \le y \le 1\}}
\]

\[
f_X(x) = ?
\]

\[
f_Y(y) = ?
\]

\textbf{Ej:} Sea $X,Y$ v.a.

\[
f_X(x) = \int_0^\infty f_{X,Y}(x,y)\,dy
\]

\[
f_X(x) = \int_0^1 10x y^2 \, dy
\]

\[
f_Y(y) = \int_0^\infty f_{X,Y}(x,y)\,dx
\]

\[
f_Y(y) = \int_0^1 10x y^2 \, dx
\]

\[
\Rightarrow f_{X|Y}(x|y) =
\frac{f_{X,Y}(x,y)}{f_Y(y)}
\]

\[
\mathbb{P}\left(\frac{1}{2} < X < 1\right)
= \int_{1/2}^1 f_{X|Y}(x|y)\,dx
\]

\textbf{Observa:}

\[
f_{X|Y}(x|0)
\]

\[
f_{X|Y}(x|1)
\]

\[
f_{X|Y}(x|2)
\]

\[
f_Y(y) =
\begin{cases}
\frac{15}{28}, & y=0 \\
\frac{3}{7}, & y=1 \\
\frac{3}{28}, & y=2
\end{cases}
\]

\[
f_X(0) = \frac{15}{28}
\]

\[
f_X(1) = \frac{3}{7}
\]

\[
f_X(2) = \frac{3}{28}
\]

% P\'agina 31

\[
f_{X|Y}(y) = f_X(x|y)
\]

\[
f_{X|Y}(x|y)
= \frac{f_{X,Y}(x,y)}{f_Y(y)}
\]

\[
f_X(x) = \int f_{X,Y}(x,y)\,dy
\qquad
f_Y(y) = \int f_{X,Y}(x,y)\,dx
\]

\[
\mathbb{P}\left\{\frac{1}{2} < X < 1 \mid Y = \frac{1}{2}\right\}
= \int_{1/2}^{1} f_{X|Y}(x|y)\,dx
\]

\textbf{Ejercicios}

\[
3.40,\; 3.41,\; 3.42,\; 3.43,\; 3.44
\]

\[
3.45,\; 3.49,\; 3.50,\; 3.53,\; 3.63
\]

\[
3.68
\]

\textbf{Marque verdaderas}

\[
f_{X,Y}(x,y) = \frac{1}{500}
\quad
0 \le x \le y \le 200
\]

\textbf{Ejercicio}

\[
\mathbb{P}\{1 \le X \le 2\}
\]

\[
f_X(x) = \int_0^\infty f_{X,Y}(x,y)\,dy
\]

\[
f_Y(y) = \int_0^\infty f_{X,Y}(x,y)\,dx
\]

\[
f_{X|Y}(x|y)
= \frac{f_{X,Y}(x,y)}{f_Y(y)}
\]

\textbf{Ej:}

\[
f_X(x) = \int_0^\infty f_{X,Y}(x,y)\,dy
\]

\[
f_Y(y) = \int_0^\infty f_{X,Y}(x,y)\,dx
\]

\[
f_{X|Y}(x|y)
\]

\textbf{Contin\'ua Nota 1}

\[
\begin{array}{c|ccc}
 & f_{Y|X}(y|x) & f_X(x) & f_{X,Y}(x,y) \\
\hline
x=0 & \frac{3}{10} & \frac{3}{14} & \frac{3}{28} \\
x=1 & \frac{3}{5}  & \frac{15}{28} & \frac{15}{28} \\
x=2 & \frac{1}{10} & 0 & 0
\end{array}
\]

\[
f_X(0) = \frac{3}{14}
\]

\[
f_X(1) = \frac{15}{28}
\]

\[
f_X(2) = \frac{3}{28}
\]

% P\'agina 32

\[
f_{X|Y}(x|y) = \frac{2x}{3}, 
\qquad x \in (0,2)
\]

\[
f_X(x) = \int_0^2 f_{X|Y}(x|y)\,dy,
\qquad
f_Y(y) = \int_0^2 f_{X|Y}(x|y)\,dx
\]

\[
f_{X|Y}(x|y)
\]

\[
f_{Y|X}(y|x)
\]

\textbf{Teorema}

\[
f_{X|Y}(x|y)
= \frac{f_{X,Y}(x,y)}{f_Y(y)},
\qquad
f_Y(y) \neq 0
\]

\[
f_{Y|X}(y|x)
= \frac{f_{X,Y}(x,y)}{f_X(x)},
\qquad
f_X(x) \neq 0
\]

\textbf{Ejemplos, Muestreos y Ejercicios}

\[
\textrm{Ejs: } 4.4,\; 4.6,\; 4.8,\; 4.9,\; 4.10
\]

\textbf{Proposici\'on}

\[
f_{Y|X}(y|x) \text{ es una funci\'on de densidad condicional}
\]

\[
\text{cuando } x \text{ es un n\'umero fijo}
\]

\[
\text{y para todo } x \text{ cumple con las propiedades de una densidad.}
\]

\textbf{Proposici\'on}

\[
f_{Y|X}(\cdot|x) \text{ es positiva (no negativa)}
\]

\[
\int_{-\infty}^{\infty} f_{Y|X}(y|x)\,dy = 1
\]

\[
\int_{-\infty}^{\infty} \frac{f_{X,Y}(x,y)}{f_X(x)}\,dy = 1
\]

\[
\frac{1}{f_X(x)} \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dy = 1
\]

\[
f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dy
\]


% P\'agina 33

\textbf{Nota:} En $f_{Y|X}(\cdot|x)$, $x$ es fija y permite considerarla
como una distribuci\'on acumulada.

\textbf{Def:} Funci\'on de dist. acumulada condicional

Si $X,Y$ son V.A.C.C., la dist. acumulada condicional de $Y$ dado $X=x$
se define por
\[
F_{Y|X}(y|x) = \int_{-\infty}^{y} f_{Y|X}(t|x)\,dt,
\qquad x \in \mathbb{R},\ f_X(x)>0.
\]

\textbf{Sup:}
\[
f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
\]

\textbf{Ej:}

\[
f_{Y|X}(y|x) = \int_0^{y} \frac{2x+t}{x+\tfrac{1}{2}}\,dt,
\qquad 0 \le y \le 1.
\]

\[
f_{Y|X}(y|x)
= \frac{1}{x+\tfrac{1}{2}} \int_0^{y} (2x+t)\,dt
= \frac{1}{x+\tfrac{1}{2}}\left(2xy + \frac{y^2}{2}\right).
\]

\textbf{Recordando:}

\[
f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}.
\]

\[
f_{X,Y}(x,y) = f_{Y|X}(y|x) f_X(x).
\]

Si $X$ y $Y$ son independientes, entonces
\[
f_{X,Y}(x,y) = f_X(x) f_Y(y).
\]

\[
\Rightarrow f_{Y|X}(y|x) = f_Y(y).
\]

\[
f_{Y|X}(y|x)\,dy
= \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dy.
\]

\[
f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y)\,dx.
\]


% P\'agina 34

\[
f_{X,Y}(x,y) = f_X(x)\,f_Y(y)
\]

\textbf{Def:} Sean $X,Y$ V.A.D.C. o V.A.C.C. con densidades
$f_{X,Y}(x,y)$ y marginales $f_X(x)$, $f_Y(y)$.

$X$ y $Y$ son estoc\'asticamente independientes si y solo si
\[
f_{X,Y}(x,y) = f_X(x)\,f_Y(y).
\]

\textbf{Ej:} Retomar Ej. 3 p.\,20 y 3 p.\,24 y verificar si $X,Y$
son independientes.

\textbf{Def:} Sean $(X_1,X_2,\dots,X_k)$ V.A. discretas o continuas.
$X_1,X_2,\dots,X_k$ son estoc\'asticamente independientes si
\[
f_{X_1,X_2,\dots,X_k}(x_1,x_2,\dots,x_k)
= f_{X_1}(x_1)\,f_{X_2}(x_2)\cdots f_{X_k}(x_k).
\]

\textbf{Def:} Sean $(X_1,X_2,\dots,X_k)$ V.A. discretas o continuas.
$X_1,X_2,\dots,X_k$ son estoc\'asticamente independientes si
\[
f_{X_i|X_j}(x_i|x_j) = f_{X_i}(x_i),
\qquad i \neq j.
\]

\textbf{Ej:} Inferir
\[
f_{X,Y}(x,y) = e^{-(2x)}\,e^{-(y)},
\qquad x>0,\ y>0.
\]

\textbf{Luego:}
\[
f_{X,Y}(x,y) = f_X(x)\,f_Y(y)
\qquad (\text{VERIFICAR})
\]

\[
f_{X,Y}(x,y)=f_X(x)f_Y(y)
\]

\textbf{Def:}  
Sean \(X,Y\) VADC o VACC con \(f_{X,Y}(x,y)\) y densidades marginales
\(f_X(x)\) y \(f_Y(y)\).  
\(X,Y\) son estoc\'asticamente independientes si y solo si
\[
f_{X,Y}(x,y)=f_X(x)f_Y(y).
\]

\textbf{Ej:}  
Probar si 3 R.V. \(X_1,X_2,X_3\) son independientes.  
Verificar si
\[
f_{X_1,X_2,X_3}(x_1,x_2,x_3)
=
f_{X_1}(x_1)\,f_{X_2}(x_2)\,f_{X_3}(x_3).
\]

\textbf{Def:}  
Sean \((X_1,X_2,\ldots,X_k)\) un vector aleatorio.  
Son estoc\'asticamente independientes si
\[
f_{X_1,\ldots,X_k}(x_1,\ldots,x_k)
=
\prod_{i=1}^k f_{X_i}(x_i).
\]

\textbf{Def:}  
Sean \((X_1,X_2,\ldots,X_k)\) V.A. discretas.  
Son estoc\'asticamente independientes si
\[
p_{X_1,\ldots,X_k}(x_1,\ldots,x_k)
=
\prod_{i=1}^k p_{X_i}(x_i).
\]

\textbf{Ej:}  
Sea
\[
f_{X,Y}(x,y)=e^{-(x+y)}.
\]

\textbf{Teo:}  
Sean \(X_1,X_2,\ldots,X_k\) V.A. independientes y  
\(g_1(\cdot),g_2(\cdot),\ldots,g_k(\cdot)\) sean funciones tales que  
\(Y_i=g_i(X_i)\), \(i=1,2,\ldots,k\).  
Entonces \(Y_1,\ldots,Y_k\) son independientes.

\section*{Esperanza}

\textbf{Def:}  
Sean \(X,Y\) V.A. con densidad conjunta \(f_{X,Y}(x,y)\).  
Se define
\[
\mathbb{E}[g(X,Y)]
=
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
g(x,y)\,f_{X,Y}(x,y)\,dx\,dy.
\]

\textbf{Teo:}  
Si \(g(X,Y)=X\), entonces
\[
\mathbb{E}[X]
=
\int_{-\infty}^{\infty} x\,f_X(x)\,dx.
\]

\textbf{Ej:}  
Sea
\[
f_{X,Y}(x,y)=e^{-(x+y)}, \quad x>0,\; y>0.
\]

Calcular
\[
\mathbb{E}[X].
\]

\[
\mathbb{E}[X]
=
\int_0^{\infty}\int_0^{\infty}
x\,e^{-(x+y)}\,dx\,dy
=
\int_0^{\infty} x e^{-x}
\left(
\int_0^{\infty} e^{-y}\,dy
\right)
dx
=
\int_0^{\infty} x e^{-x}\,dx.
\]

\[
\mathbb{E}[X]=1.
\]

\textbf{Ej:}  
Sea
\[
g(X,Y)=X+Y.
\]

\[
\mathbb{E}[X+Y]
=
\mathbb{E}[X]+\mathbb{E}[Y].
\]

\[
\mathbb{E}[X+Y]
=
\int_0^{\infty}\int_0^{\infty}
(x+y)\,f_{X,Y}(x,y)\,dx\,dy
=
\int_0^{\infty}\int_0^{\infty}
(x+y)e^{-(x+y)}\,dx\,dy
=2.
\]

\textbf{Prop:}  
\[
\mathbb{E}[aX+bY]
=
a\,\mathbb{E}[X]+b\,\mathbb{E}[Y].
\]

\textbf{Teo:}  
Sean \(X,Y\) V.A. independientes.  
Sea \(g(X,Y)=g(X)h(Y)\).  
Entonces
\[
\mathbb{E}[g(X)h(Y)]
=
\mathbb{E}[g(X)]\,\mathbb{E}[h(Y)].
\]

\textbf{Dem:}  
\[
\mathbb{E}[g(X)h(Y)]
=
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
g(x)h(y)\,f_{X,Y}(x,y)\,dx\,dy
\]
\[
=
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
g(x)h(y)\,f_X(x)f_Y(y)\,dx\,dy
\]
\[
=
\left(
\int_{-\infty}^{\infty} g(x)f_X(x)\,dx
\right)
\left(
\int_{-\infty}^{\infty} h(y)f_Y(y)\,dy
\right)
\]
\[
=
\mathbb{E}[g(X)]\,\mathbb{E}[h(Y)].
\]

\textbf{Sup:}  
\[
f_{X,Y}(x,y)=f_X(x)f_Y(y).
\]

\textbf{Ej:}  
\[
\mathbb{E}[X+Y]=
\]

\[
\mathbb{E}[X+Y]=
\mathbb{E}[X]+\mathbb{E}[Y].
\]

\textbf{Def:} (Covarianza)

Sean \(X,Y\) V.A. con varianzas finitas.
\[
\operatorname{Cov}(X,Y)
=
\mathbb{E}\big[(X-\mu_X)(Y-\mu_Y)\big],
\]
donde \(\mu_X=\mathbb{E}[X]\) y \(\mu_Y=\mathbb{E}[Y]\).

\textbf{Def:} (Correlaci\'on)

El coeficiente de correlaci\'on de \(X\) y \(Y\) se define por
\[
\rho_{X,Y}
=
\frac{\operatorname{Cov}(X,Y)}{\sigma_X\sigma_Y},
\]
donde \(\sigma_X>0\) y \(\sigma_Y>0\).

\textbf{Nota:}
\[
\operatorname{Cov}(X,Y)
=
\mathbb{E}[XY]
-
\mathbb{E}[X]\mathbb{E}[Y].
\]

\[
\mathbb{E}\big[(X-\mu_X)(Y-\mu_Y)\big]
=
\mathbb{E}[XY]
-
\mu_Y\mathbb{E}[X]
-
\mu_X\mathbb{E}[Y]
+
\mu_X\mu_Y
\]

\[
=
\mathbb{E}[XY]
-
\mathbb{E}[X]\mathbb{E}[Y].
\]

\textbf{Nota:}  
Si \(X,Y\) son independientes, entonces
\[
\operatorname{Cov}(X,Y)=0.
\]

\textbf{Ej:} Funcional para \(X,Y\).

\[
f_{X,Y}(x,y) = (x+y)\,\mathbf{1}_{(0,1)}(x)\,\mathbf{1}_{(0,1)}(y).
\]

\textbf{Def:}  
Sean \(X_1,X_2,\dots,X_k\) funciones aleatorias con
\[
\mathbb{E}[X_i] = \mu_i,\qquad i=1,\dots,k.
\]
Los momentos centrales est\'an definidos por
\[
\mathbb{E}\big[(X_1-\mu_1)\cdots(X_k-\mu_k)\big].
\]

\textbf{Def:} (Funci\'on generadora de momentos)
\[
M_{X_1,\dots,X_k}(t_1,\dots,t_k)
=
\mathbb{E}\!\left[
\exp\!\left(\sum_{i=1}^k t_i X_i\right)
\right].
\]

\textbf{Nota:}  
Las funciones generadoras satisfacen
\[
\mathbb{E}\big[g(X,Y)\big]
=
\iint g(x,y)\,f_{X,Y}(x,y)\,dx\,dy.
\]

\textbf{Ej:}

Sea \(X=\mathbf{1}_A\), con \(A\in\mathcal{F}\).
\[
\mathbb{E}(X)=\mathbb{E}[\mathbf{1}_A]=P(A).
\]

\[
X^2=X \;\Rightarrow\; \mathbb{E}(X^2)=\mathbb{E}(X).
\]

\[
\operatorname{Var}(X)
=
\mathbb{E}(X^2)-\big(\mathbb{E}(X)\big)^2
=
P(A)-P(A)^2
=
P(A)\big(1-P(A)\big).
\]

\textbf{Prop:}  
Sean \(X_1,\dots,X_n\) V.A. independientes.

\begin{enumerate}
\item[a)] \(\operatorname{Cov}(X,Y)=\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y)\).
\item[b)] \(\sigma_X^2=\operatorname{Var}(X)\).
\item[c)] \(\sigma_{X+Y}^2=\sigma_X^2+\sigma_Y^2\).
\end{enumerate}

\textbf{Prop:}
\[
\operatorname{Var}\!\left(\sum_{i=1}^n X_i\right)
=
\sum_{i=1}^n \operatorname{Var}(X_i).
\]

\[
\operatorname{Cov}(X_i,X_j)=0,\qquad i\neq j.
\]

\textbf{Corolario:}

Sean \(X_1,X_2,\ldots,X_n\) V.A. independientes.
Entonces
\[
\operatorname{Var}\!\left(\sum_{i=1}^n X_i\right)
=
\sum_{i=1}^n \operatorname{Var}(X_i).
\]

\textbf{Def:}
Sean \(X_1,X_2,\ldots,X_n\) V.A. independientes i.i.d.
Entonces
\[
\bar{X}
=
\frac{1}{n}\sum_{i=1}^n X_i
\]
es una V.A. y se llama \emph{media muestral}.

\textbf{Prop:}
Si \(X_1,X_2,\ldots,X_n\) son V.A. i.i.d. con
\[
\mathbb{E}(X)=\mu,
\qquad
\operatorname{Var}(X)=\sigma^2,
\]
entonces:
\begin{enumerate}
\item[a)] \(\mathbb{E}(\bar{X})=\mu\).
\item[b)] \(\operatorname{Var}(\bar{X})=\dfrac{\sigma^2}{n}\).
\item[c)] \(\operatorname{Cov}(X_i,\bar{X})=\dfrac{\sigma^2}{n}\).
\end{enumerate}

\textbf{Def:}
\begin{enumerate}
\item[b)] 
\[
\operatorname{Var}(\bar{X})
=
\operatorname{Var}\!\left(\frac{1}{n}\sum_{i=1}^n X_i\right)
=
\frac{1}{n^2}\operatorname{Var}\!\left(\sum_{i=1}^n X_i\right)
=
\frac{\sigma^2}{n}.
\]

\item[c)]
\[
\operatorname{Cov}(X_i,\bar{X})
=
\operatorname{Cov}\!\left(X_i,\frac{1}{n}\sum_{j=1}^n X_j\right)
=
\frac{1}{n}\operatorname{Cov}\!\left(X_i,X_i\right)
=
\frac{\sigma^2}{n}.
\]
\end{enumerate}

\textbf{Corolario:}

Sean \(X_1, X_2, \ldots, X_n\) V.A. independientes.
Entonces
\[
\operatorname{Var}\!\left(\sum_{i=1}^n X_i\right)
=
\sum_{i=1}^n \operatorname{Var}(X_i).
\]

\textbf{Def:}
Sean \(X_1, X_2, \ldots, X_n\) V.A. i.i.d.
Entonces
\[
\bar{X}
=
\frac{1}{n}\sum_{i=1}^n X_i
\]
es una V.A. y se llama \emph{media muestral}.

\textbf{Prop:}
Si \(X_1, X_2, \ldots, X_n\) son V.A. i.i.d. con
\[
\mu = \mathbb{E}(X),
\qquad
\sigma^2 = \operatorname{Var}(X),
\]
entonces:
\begin{enumerate}
\item[a)] \(\mathbb{E}(\bar{X}) = \mu\).
\item[b)] \(\operatorname{Var}(\bar{X}) = \dfrac{\sigma^2}{n}\).
\item[c)] \(\operatorname{Cov}(X_i,\bar{X}) = \dfrac{\sigma^2}{n}\).
\end{enumerate}

\textbf{Def:}
\begin{enumerate}
\item[b)]
\[
\operatorname{Var}(\bar{X})
=
\operatorname{Var}\!\left(\frac{1}{n}\sum_{i=1}^n X_i\right)
=
\frac{1}{n^2}\operatorname{Var}\!\left(\sum_{i=1}^n X_i\right)
=
\frac{\sigma^2}{n}.
\]

\item[c)]
\[
\operatorname{Cov}(X_i,\bar{X})
=
\operatorname{Cov}\!\left(X_i,\frac{1}{n}\sum_{j=1}^n X_j\right)
=
\frac{1}{n}\operatorname{Cov}(X_i,X_i)
=
\frac{\sigma^2}{n}.
\]
\end{enumerate}

\[
\iint h(y)\,f_{Y}(y)\,f_{X}(x)\,dx\,dy
=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}
h(y)\,f_{Y}(y)\,f_{X}(x)\,dx\,dy
\]

\[
=
\int_{-\infty}^{\infty}
h(y)\,f_{Y}(y)
\left(
\int_{-\infty}^{\infty}
f_{X}(x)\,dx
\right)
dy
\]

\[
=
\int_{-\infty}^{\infty}
h(y)\,f_{Y}(y)\,dy
\]

\textcolor{red}{Sea} \quad Z = X + Y.

\[
X,Y \ \text{V.A. indep.}
\quad\Rightarrow\quad
f_{X,Y}(x,y)=f_X(x)f_Y(y)
\]

\[
f_Z(z)
=
\int_{-\infty}^{\infty}
f_{X,Y}(x,z-x)\,dx
=
\int_{-\infty}^{\infty}
f_X(x)\,f_Y(z-x)\,dx
\]

\textbf{Propiedades}

\[
\text{Traslaci\'on}\quad u\ \text{unid.}
\]

\[
\text{Proporcionalidad}\quad u/n\ \text{unid.}
\]

\[
\text{Pr\'acticas}\quad 2q/n\ \text{unid.}
\]

\[
\frac{1}{n}\,\operatorname{Cov}\!\left(\sum_{i=1}^n X_i, X_j\right)
=
\frac{1}{n}\,\operatorname{Cov}(X_j,X_j)
=
\frac{\sigma^2}{n}
\]

\[
\frac{1}{n}\,\operatorname{Cov}\!\left(\sum_{i=1}^n X_i, X_i\right)
=
\frac{1}{n}\,\sigma^2
\]

\[
\frac{1}{n}\,\operatorname{Cov}\!\left(\sum_{i=1}^n X_i, X_i\right)
-
\frac{\sigma^2}{n}
=
0
\]

\[
\text{para todo valor v\'alido.}
\]

\[
\int_{-\infty}^{\infty} f_X(a-y)\, f_Y(y)\, dy
\]

\text{n.e. la f.d.p. de la convoluci\'on}

\[
X+Y
\]

\text{Definimos}

\[
f_{X+Y}(a)
=
\int_{-\infty}^{\infty}
f_X(a-y)\, f_Y(y)\, dy
\]

\[
=
\int_{-\infty}^{\infty}
f_X(a-y)\, f_Y(y)\, dy
\]

\textbf{Ej:}

\[
X,Y \sim U(0,1)
\]

\[
f_{X+Y}(a)
=
\int_{-\infty}^{\infty}
f_X(a-y)\, f_Y(y)\, dy
\]

\[
\text{si } 0<a<1
\]

\[
f_{X+Y}(a)
=
\int_{0}^{a} 1 \, dy
\]

\[
= a
\]

\[
\text{si } 1<a<2
\]

\[
f_{X+Y}(a)
=
\int_{a-1}^{1} 1 \, dy
\]

\[
= 2-a
\]

\[
f_{X+Y}(a)
=
\begin{cases}
a, & 0<a<1,\\[4pt]
2-a, & 1<a<2.
\end{cases}
\]

\[
a \in (0,1)\qquad a \in (1,2)
\]

\[
\int_{-\infty}^{\infty} f_X(a-y)\, f_Y(y)\, dy
\]

\text{n.e. la f.d.p. de la convoluci\'on}

\[
X+Y
\]

\text{Definimos}

\[
f_{X+Y}(a)
=
\int_{-\infty}^{\infty}
f_X(a-y)\, f_Y(y)\, dy
\]

\[
=
\int_{-\infty}^{\infty}
f_X(a-y)\, f_Y(y)\, dy
\]

\textbf{Ej:}

\[
X,Y \sim U(0,1)
\]

\[
f_{X+Y}(a)
=
\int_{-\infty}^{\infty}
f_X(a-y)\, f_Y(y)\, dy
\]

\[
\text{si } 0<a<1
\]

\[
f_{X+Y}(a)
=
\int_{0}^{a} 1 \, dy
\]

\[
= a
\]

\[
\text{si } 1<a<2
\]

\[
f_{X+Y}(a)
=
\int_{a-1}^{1} 1 \, dy
\]

\[
= 2-a
\]

\[
f_{X+Y}(a)
=
\begin{cases}
a, & 0<a<1,\\[4pt]
2-a, & 1<a<2.
\end{cases}
\]

\[
a \in (0,1)\qquad a \in (1,2)
\]

\textbf{Ej 2:} \quad X,Y \ \text{v.a. ind}

\[
f_{X+Y}(n)
\]

\[
0 \le k \le n
\]

\[
\text{Caso } X+Y = n \iff X = k,\ Y = n-k
\]

\[
P(X+Y=n)
=
\sum_{k=0}^{n} P(X=k,\ Y=n-k)
\]

\[
=
\sum_{k=0}^{n} P(X=k)\,P(Y=n-k)
\]

\[
=
\sum_{k=0}^{n}
\frac{\lambda_1^k}{k!} e^{-\lambda_1}
\frac{\lambda_2^{\,n-k}}{(n-k)!} e^{-\lambda_2}
\]

\[
=
e^{-(\lambda_1+\lambda_2)}
\sum_{k=0}^{n}
\frac{\lambda_1^k \lambda_2^{\,n-k}}{k!(n-k)!}
\]

\[
=
e^{-(\lambda_1+\lambda_2)}
\frac{1}{n!}
\sum_{k=0}^{n}
\binom{n}{k}
\lambda_1^k \lambda_2^{\,n-k}
\]

\[
=
e^{-(\lambda_1+\lambda_2)}
\frac{(\lambda_1+\lambda_2)^n}{n!}
\]

\[
X+Y \sim P(\lambda_1+\lambda_2)
\]

\[
\text{V.A.C.C.}
\]

X_1, X_2 \ \text{V.A.C.C. con densidad}
\[
f_{X_1,X_2}(x_1,x_2)
\]

\textbf{Sup.} \quad Y_1 = g_1(x_1,x_2), \quad Y_2 = g_2(x_1,x_2)
\quad \text{T--Q}

\[
y_1 = y_1(x_1,x_2), \quad y_2 = y_2(x_1,x_2)
\]

A) \quad \text{se pueden resolver las ecuaciones en forma \'unica}
\[
\text{para } x_1 \text{ y } x_2
\]

\[
x_1 = h_1(y_1,y_2), \quad x_2 = h_2(y_1,y_2)
\]

B) \quad \text{las funciones } y_1,y_2 \text{ tienen derivadas}
\[
\text{parciales } \forall (x_1,x_2)
\]

\[
J(x_1,x_2)
=
\begin{vmatrix}
\dfrac{\partial y_1}{\partial x_1} & \dfrac{\partial y_1}{\partial x_2} \\[1ex]
\dfrac{\partial y_2}{\partial x_1} & \dfrac{\partial y_2}{\partial x_2}
\end{vmatrix}
\neq 0
\]

\textbf{Entonces} \quad Y_1 \text{ y } Y_2 \ \text{son V.A.C.C.}

\[
f_{Y_1,Y_2}(y_1,y_2)
=
f_{X_1,X_2}\!\left(x_1,x_2\right)
\left| J(x_1,x_2) \right|
\]

\[
x_1 = h_1(y_1,y_2), \quad x_2 = h_2(y_1,y_2)
\]

\textbf{Ej:}

Sean \(X\) i.i.d. V.A. con
\[
X \sim \mathrm{Ga}(\alpha,\lambda),
\qquad
Y \sim \mathrm{Ga}(\beta,\lambda),
\qquad
U = X+Y,
\qquad
V = \frac{X}{X+Y}.
\]

\[
f_{X,Y}(x,y)
=
\frac{\lambda^\alpha e^{-\lambda x} x^{\alpha-1}}{\Gamma(\alpha)}
\,
\frac{\lambda^\beta e^{-\lambda y} y^{\beta-1}}{\Gamma(\beta)}
\]

Sea
\[
y_1 = g_1(x,y) = x+y,
\qquad
y_2 = g_2(x,y) = \frac{x}{x+y}.
\]

Entonces
\[
\frac{\partial y_1}{\partial x} = 1,
\qquad
\frac{\partial y_1}{\partial y} = 1,
\]

\[
\frac{\partial y_2}{\partial x}
=
\frac{y}{(x+y)^2},
\qquad
\frac{\partial y_2}{\partial y}
=
-\frac{x}{(x+y)^2}.
\]

\[
U = x+y,
\qquad
V = \frac{x}{x+y}
\;\Rightarrow\;
x = uv,
\qquad
y = u(1-v).
\]

\[
x = uv
\]

\[
v = \frac{uv}{uv+u(1-v)} = uv
\]

\[
uv = uv
\;\Rightarrow\;
v = uv + u(1-v)
\]

\[
uv = u
\]

\[
v = uv
\]

\[
\Rightarrow\;
v = uv + u(1-v)
\]

\[
\Rightarrow\;
y = u(1-v).
\]

\[
f_{Y_1,Y_2}(y_1,y_2)
=
f_{X_1,X_2}(x_1,x_2)\,
\left| J(x_1,x_2) \right|^{-1}
\]

Donde
\[
\left| J(x_1,x_2) \right|^{-1}
=
\begin{vmatrix}
1 & 1 \\
\dfrac{x}{(x+y)^2} & \dfrac{y}{(x+y)^2}
\end{vmatrix}^{-1}
=
\frac{xy}{(x+y)^2}
=
\frac{1}{x+y}
\]

\[
=
\frac{\lambda^{\alpha+\beta}}{\Gamma(\alpha)\Gamma(\beta)}
e^{-\lambda(x+y)} x^{\alpha-1} y^{\beta-1} (x+y)
\]

\[
=
\frac{\lambda^{\alpha+\beta}}{\Gamma(\alpha)\Gamma(\beta)}
e^{-\lambda u}
(uv)^{\alpha-1}
\bigl(u(1-v)\bigr)^{\beta-1}
\, u
\]

\[
=
\frac{1}{\Gamma(\alpha)\Gamma(\beta)}
\bigl(\lambda e^{-\lambda u}\bigr)
\bigl(u^{\alpha-1} u^{\beta}\bigr)
\bigl(v^{\alpha-1}(1-v)^{\beta-1}\bigr)
\]

\[
=
\frac{1}{\Gamma(\alpha)\Gamma(\beta)}
\bigl(\lambda e^{-\lambda u}\bigr)
(\lambda u)^{\alpha-1}
(\lambda u)^{\beta}
v^{\alpha-1}(1-v)^{\beta-1}
\]

\[
=
\frac{1}{\Gamma(\alpha)\Gamma(\beta)}
\lambda e^{-\lambda u}
(\lambda u)^{\alpha+\beta-1}
v^{\alpha-1}(1-v)^{\beta-1}
\]

\[
=
\frac{\lambda e^{-\lambda u}(\lambda u)^{\alpha+\beta-1}}{\Gamma(\alpha+\beta)}
\cdot
\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}
v^{\alpha-1}(1-v)^{\beta-1}
\]

\[
U \sim \mathrm{Ga}(\alpha+\beta,\lambda),
\qquad
V \sim \mathrm{Be}(\alpha,\beta).
\]

\[
\text{Sean } X,Y \text{ v.a. c.c. con densidad}
\]

\[
Z = XY
\]

\[
f_Z(z)
=
\int_{-\infty}^{\infty}
\frac{1}{|y|}
f_{X,Y}\!\left(\frac{z}{y},\,y\right)\,dy
\]

\[
\text{Sean } X,Y \text{ v.a. c.c. con densidad}
\]

\[
Z = X + Y,
\qquad
V = X - Y
\]

\[
x = \frac{z+v}{2},
\qquad
y = \frac{z-v}{2}
\]

\[
f_{Z,V}(z,v)
=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}
f_{X,Y}(x,y)\,dx\,dy
\]

\[
=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}
f_{X,Y}\!\left(\frac{z+v}{2},\,\frac{z-v}{2}\right)
\left|J\right|
\,dz\,dv
\]

\[
f_X(w)
=
\int_{-\infty}^{\infty}
f_{X,Y}(x,\,x-w)\,dx
\]

\[
=
\int_{-\infty}^{\infty}
f_{X,Y}(x,\,x-v)\,dx
\]

\[
\text{Sean } X,Y \text{ variables aleatorias i.i.d.}
\]

\[
\text{Var}(X-Y)
\]

\[
\text{Entonces:}
\]

\[
\mathbb{E}(X-Y)
=
\mathbb{E}(X)-\mathbb{E}(Y)
\]

\[
\text{y}
\]

\[
\text{Var}(X-Y)
=
\mathbb{E}\!\left[(X-Y)^2\right]
-
\left[\mathbb{E}(X-Y)\right]^2
\]

\[
=
\mathbb{E}(X^2 - 2XY + Y^2)
-
\left[\mathbb{E}(X-Y)\right]^2
\]

\[
=
\mathbb{E}(X^2)
-
2\mathbb{E}(XY)
+
\mathbb{E}(Y^2)
-
\left[\mathbb{E}(X-Y)\right]^2
\]

---

\[
\text{Def:}
\]

\[
\mathbb{E}(X^2)
=
\text{Var}(X) + [\mathbb{E}(X)]^2
\]

\[
\mathbb{E}(Y^2)
=
\text{Var}(Y) + [\mathbb{E}(Y)]^2
\]

\[
\mathbb{E}(XY)
=
\mathbb{E}(X)\mathbb{E}(Y)
\quad (\text{i.i.d.})
\]

---

\[
\text{As\'i:}
\]

\[
\text{Var}(X-Y)
=
\text{Var}(X)
+
\text{Var}(Y)
+
[\mathbb{E}(X)]^2
+
[\mathbb{E}(Y)]^2
-
2\mathbb{E}(X)\mathbb{E}(Y)
-
(\mathbb{E}(X)-\mathbb{E}(Y))^2
\]

\[
=
\text{Var}(X)
+
\text{Var}(Y)
\]

---

\[
\boxed{
\text{Var}(X-Y)
=
\text{Var}(X)
+
\text{Var}(Y)
}
\]

\[
\Rightarrow \ \mathbb{E}[XY]
=
\mu_X \mu_Y
+
\mu_Y \mathbb{E}[X-\mu_X]
+
\mu_X \mathbb{E}[Y-\mu_Y]
+
\mathbb{E}\!\left[(X-\mu_X)(Y-\mu_Y)\right]
\]

\[
\mathbb{E}\!\left[(X-\mu_X)(Y-\mu_Y)\right]
=
\mu_X \mu_Y
+
\text{Cov}(X,Y)
\]

\[
\mathbb{E}\!\left[(XY)^2\right]
=
\sim\sim\sim
\]

---

\[
\text{Cor: Si } X,Y \text{ son indep.}
\]

\[
\mathbb{E}[XY]
=
\mu_X \mu_Y
\]

\[
\text{y}
\]

\[
\text{Var}(XY)
=
\mu_Y^2 \, \text{Var}(X)
+
\mu_X^2 \, \text{Var}(Y)
+
\text{Var}(X)\text{Var}(Y)
\]

---

\[
\text{Tarea: } \mathbb{E}\!\left[\frac{X}{Y}\right]
\]

---

\[
\text{Ej: Sea } X \sim \mathcal{N}(0,1).
\quad
\text{Sea } Y = X^2 = g(X)
\]

\[
F_Y(y)
=
\mathbb{P}(Y \le y)
=
\mathbb{P}(X^2 \le y)
=
\mathbb{P}(-\sqrt{y} \le X \le \sqrt{y})
\]

\[
=
\Phi(\sqrt{y}) - \Phi(-\sqrt{y})
=
2 \int_{0}^{\sqrt{y}} \phi(u)\,du
\]

\[
=
\frac{2}{\sqrt{2\pi}}
\int_{0}^{\sqrt{y}} e^{-u^2/2}\,du
\]

---

\[
u = \sqrt{z}
\quad \Longleftrightarrow \quad
u^2 = z
\quad \Rightarrow \quad
2u\,du = dz
\]

\[
du = \frac{dz}{2\sqrt{z}}
\]

\[
0 \le u^2 \le y
\quad \Rightarrow \quad
0 \le z \le y
\]

\[
\frac{2}{\sqrt{2\pi}}
\int_{0}^{y}
\frac{e^{-z/2}}{2\sqrt{z}}\,dz
=
\frac{1}{\sqrt{\pi}}
\int_{0}^{y}
\frac{e^{-z/2}}{\sqrt{2z}}\,dz
\]

---

\[
\Gamma\!\left(\tfrac{1}{2}\right)
=
\sqrt{\pi}
\]

\[
X \sim \Gamma(\alpha,\beta)
\quad \Rightarrow \quad
f(x)
=
\frac{1}{\Gamma(\alpha)\beta^\alpha}
x^{\alpha-1} e^{-x/\beta}
\]

\[
\alpha = \tfrac{1}{2},
\quad
\beta = 2
\]

\[
\Rightarrow
\int_{0}^{y}
\frac{e^{-z/2}}{\Gamma(\tfrac{1}{2}) \sqrt{2z}}\,dz
\]

\[
\text{Sea } X \sim \mathcal{N}(0,1), \quad Y = X^2
\]

---

\[
\mathbb{E}(Y)
=
\mathbb{E}(X^2)
=
\int_{-\infty}^{\infty} x^2 e^{-x^2/2} \, dx
\]

\[
=
\int_{-\infty}^{\infty} e^{-x^2/2} x^2 \, dx
\]

\[
=
\left[
- x e^{-x^2/2}
\right]_{-\infty}^{\infty}
+
\int_{-\infty}^{\infty} e^{-x^2/2} \, dx
\]

\[
=
0 + \sqrt{2\pi}
\]

\[
\Rightarrow
\mathbb{E}(Y)
=
1
\]

---

\[
\text{Por } X \sim \mathcal{N}(0,1)
\quad \Rightarrow \quad
Y = X^2 \sim \chi^2_{(1)}
\]

---

\[
Y_1 = g_1(X_1,X_2)
\quad , \quad
Y_2 = X_1 + X_2
\]

\[
\text{(Transformaci\'on uno--a--uno)}
\]

---

\[
\mathbb{E}\!\left[
e^{t_1 Y_1 + t_2 Y_2}
\right]
=
\mathbb{E}\!\left[
e^{t_1(X_1^2+X_2^2) + t_2(X_1+X_2)}
\right]
\]

\[
=
\mathbb{E}\!\left[
e^{t_1 X_1^2 + t_2 X_1}
\right]
\mathbb{E}\!\left[
e^{t_1 X_2^2 + t_2 X_2}
\right]
\]

---

\[
M_{Y_1,Y_2}(t_1,t_2)
=
\mathbb{E}
\left[
e^{t_1(X_1^2+X_2^2) + t_2(X_1+X_2)}
\right]
\]

\[
=
\mathbb{E}
\left[
e^{t_1 X_1^2 + t_2 X_1}
\right]
\mathbb{E}
\left[
e^{t_1 X_2^2 + t_2 X_2}
\right]
\]

---

\[
M(t_1,t_2)
=
\exp\!\left(
\frac{t_2^2}{2(1-2t_1)}
\right)
\cdot
\frac{1}{\sqrt{1-2t_1}}
\]

---

\[
=
\exp\!\left(
\frac{t_2^2}{1-2t_1}
\right)
\cdot
\frac{1}{1-2t_1}
\]

---

\[
M(t_1,t_2)
=
\frac{1}{1-2t_1}
\exp\!\left(
\frac{t_2^2}{1-2t_1}
\right)
\]

---

\[
M(t)
=
\mathbb{E}(e^{tY})
=
e^{2t^2/2}
=
e^{t^2}
\]

\[
\Rightarrow \quad
Y \sim \mathcal{N}(0,2)
\]

\[
\text{Sea } X_1, X_2 \sim \mathcal{N}(0,1) \quad \text{independientes}
\]

\[
Z = \frac{X_2 - X_1}{\sqrt{2}}
\]

---

\[
\text{Sea } Y = X_2 - X_1
\]

\[
\Rightarrow \quad Y \sim \mathcal{N}(0,2)
\]

---

\[
\text{Por binomiales:}
\]

\[
M_Y(t)
=
M_{X_2-X_1}(t)
=
M_{X_2}(t)\,M_{-X_1}(t)
\]

\[
=
\left(e^{t^2/2}\right)
\left(e^{t^2/2}\right)
=
e^{t^2}
\]

---

\[
\Rightarrow \quad Y \sim \mathcal{N}(0,2)
\]

---

\[
\text{Sea } Z = \frac{X_2-X_1}{\sqrt{2}}
\]

\[
M_Z(t)
=
\mathbb{E}\!\left(e^{tZ}\right)
=
\mathbb{E}\!\left(e^{t(X_2-X_1)/\sqrt{2}}\right)
\]

\[
=
M_Y\!\left(\frac{t}{\sqrt{2}}\right)
=
e^{(t/\sqrt{2})^2}
=
e^{t^2/2}
\]

---

\[
\Rightarrow \quad Z \sim \mathcal{N}(0,1)
\]

---

\[
\text{Suma de normales}
\]

\[
M_{X+Y}(t)
=
M_X(t)\,M_Y(t)
=
e^{\mu_X t + \sigma_X^2 t^2/2}
\,
e^{\mu_Y t + \sigma_Y^2 t^2/2}
\]

\[
=
e^{(\mu_X+\mu_Y)t + (\sigma_X^2+\sigma_Y^2)t^2/2}
\]

---

\[
\Rightarrow \quad X+Y \sim \mathcal{N}(\mu_X+\mu_Y,\; \sigma_X^2+\sigma_Y^2)
\]

\textbf{Resultado:}

Sean $Z_1, Z_2, \ldots, Z_n \sim \mathcal{N}(0,1)$ independientes.

Sean $a_{ij} \in \mathbb{R}$,
\[
i = 1,2,\ldots,m,
\qquad
j = 1,2,\ldots,n
\]

Sea $\mu_i \in \mathbb{R}$, $i=1,2,\ldots,m$.

---

\textbf{Definici\'on:}

\[
X_1 = \sum_{j=1}^n a_{1j} Z_j + \mu_1
\]

\[
X_2 = \sum_{j=1}^n a_{2j} Z_j + \mu_2
\]

\[
\vdots
\]

\[
X_m = \sum_{j=1}^n a_{mj} Z_j + \mu_m
\]

En general,
\[
X_k = \sum_{j=1}^n a_{kj} Z_j + \mu_k,
\qquad
k = 1,2,\ldots,m
\]

---

\textbf{Entonces:}

\[
X_k \sim \mathcal{N}
\left(
\mu_k,\;
\sum_{j=1}^n a_{kj}^2
\right)
\]

---

\textbf{Luego:}

\[
M_{X_1 + X_2 + \cdots + X_m}(t_1,t_2,\ldots,t_m)
=
\mathbb{E}
\left[
e^{\,t_1 X_1 + t_2 X_2 + \cdots + t_m X_m}
\right]
\]

---

\[
M_{\sum_{j=1}^m X_j}(t_1,t_2,\ldots,t_m)
=
\mathbb{E}
\left[
e^{\sum_{j=1}^m t_j X_j}
\right]
\]

---

\textbf{A saber:}

\[
\mathbb{E}
\left[
\sum_{j=1}^m t_j X_j
\right]
=
\sum_{j=1}^m \mathbb{E}[t_j X_j]
=
\sum_{j=1}^m t_j \mathbb{E}[X_j]
\]

\[
=
\sum_{j=1}^m t_j \mu_j
\]

---

\[
\operatorname{Var}
\left(
\sum_{j=1}^m t_j X_j
\right)
=
\operatorname{Cov}
\left(
\sum_{j=1}^m t_j X_j,\;
\sum_{j=1}^m t_j X_j
\right)
\]

\[
=
\sum_{j=1}^m
\sum_{i=1}^m
\operatorname{Cov}(X_i, X_j)\, t_i t_j
\]

\textbf{Ejemplo:}

Sean $X_1, X_2$ v.a. independientes con distribuci\'on $\mathcal{N}(0,1)$.

Sea
\[
Y = \frac{(X_2 - X_1)^2}{2}
\]
y encontrar la distribuci\'on de $Y$.

---

\textbf{Funci\'on generadora de momentos de $Y$:}

\[
M_Y(t) = \mathbb{E}\left[e^{tY}\right]
= \mathbb{E}\left[
e^{\frac{(X_2 - X_1)^2}{2}t}
\right]
\]

\[
= \mathbb{E}
\left[
\exp\left(
\frac{X_2^2 - 2X_1X_2 + X_1^2}{2}\, t
\right)
\right]
\]

---

Como $X_1, X_2 \sim \mathcal{N}(0,1)$ independientes,
\[
f_{X_1,X_2}(x_1,x_2)
=
\frac{1}{2\pi}
\exp\left(
-\frac{x_1^2 + x_2^2}{2}
\right)
\]

---

\[
M_Y(t)
=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}
\frac{1}{2\pi}
\exp\left(
\frac{(x_2-x_1)^2}{2}t
\right)
\exp\left(
-\frac{x_1^2 + x_2^2}{2}
\right)
dx_1 dx_2
\]

---

\[
=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}
\frac{1}{2\pi}
\exp\left(
\frac{x_2^2(t-1) + x_1^2(t-1) - 2x_1x_2 t}{2}
\right)
dx_1 dx_2
\]

---

Reordenando t\'erminos:

\[
=
\int_{-\infty}^{\infty}
\frac{1}{2\pi}
\exp\left(
\frac{x_2^2(1-t)}{2}
\right)
\left[
\int_{-\infty}^{\infty}
\exp\left(
-\frac{(1-t)}{2}
\left(
x_1^2 + \frac{2x_1x_2 t}{1-t}
\right)
\right)
dx_1
\right]
dx_2
\]

\textbf{Ejemplo (continuaci\'on):}

Sean $X_1, X_2$ v.a. independientes con distribuci\'on $\mathcal{N}(0,1)$.
Sea
\[
Y = \frac{(X_2 - X_1)^2}{2}
\]
y encontrar la distribuci\'on de $Y$.

---

\textbf{Funci\'on generadora de momentos:}

\[
M_Y(t) = \mathbb{E}\left[e^{Yt}\right]
= \mathbb{E}\left[
e^{\frac{(X_2-X_1)^2}{2}t}
\right]
\]

\[
= \mathbb{E}
\left[
\exp\left(
\frac{X_2^2 - 2X_1X_2 + X_1^2}{2}t
\right)
\right]
\]

---

Como $X_1,X_2 \sim \mathcal{N}(0,1)$ independientes,

\[
=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}
\frac{1}{2\pi}
\exp\left(
\frac{(x_2-x_1)^2}{2}t
\right)
\exp\left(
-\frac{x_1^2+x_2^2}{2}
\right)
dx_1dx_2
\]

---

\[
=
\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty}
\frac{1}{2\pi}
\exp\left\{
\frac{x_2^2(t-1)+x_1^2(t-1)+2x_1x_2t}{2}
\right\}
dx_1dx_2
\]

---

Reescribiendo:

\[
=
\int_{-\infty}^{\infty}
\frac{1}{2\pi}
\exp\left(
\frac{x_2^2(1-t)}{2}
\right)
\left[
\int_{-\infty}^{\infty}
\exp\left(
-\frac{(1-t)}{2}
\left(
x_1^2 + \frac{2x_1x_2t}{1-t}
\right)
\right)
dx_1
\right]
dx_2
\]

---

Factorizando y completando cuadrados en la integral interna:

\[
=
\int_{-\infty}^{\infty}
\frac{1}{2\pi}
\exp\left(
\frac{x_2^2(1-t)}{2}
\right)
\left[
\int_{-\infty}^{\infty}
\exp\left(
-\frac{(1-t)}{2}
\left(
x_1 + \frac{x_2t}{1-t}
\right)^2
\right)
dx_1
\right]
dx_2
\]

---

Evaluando la integral gaussiana:

\[
=
\int_{-\infty}^{\infty}
\frac{1}{\sqrt{2\pi}}
\frac{1}{\sqrt{1-t}}
\exp\left(
-\frac{x_2^2}{2}\frac{1-t-t^2}{1-t}
\right)
dx_2
\]

---

\[
=
\frac{1}{\sqrt{1-t}}
\int_{-\infty}^{\infty}
\frac{1}{\sqrt{2\pi}}
\exp\left(
-\frac{x_2^2}{2(1-2t)}
\right)
dx_2
\]

---

Finalmente:

\[
M_Y(t) = (1-2t)^{-1/2},
\qquad t<\frac{1}{2}
\]

\textbf{Teorema.}  
Sean $X_1, X_2, \dots, X_n$ v.a. independientes t.q. su FGM existe
$\forall |t|<h$, p.a. $h>0$.  
Sea
\[
Y = \sum_{i=1}^n X_i .
\]

Entonces,
\[
M_Y(t) = \mathbb{E}\!\left[e^{\sum_{i=1}^n X_i t}\right]
= \prod_{i=1}^n M_{X_i}(t),
\qquad |t|<h.
\]

\textbf{Demostraci\'on.}

\[
M_Y(t)
= \mathbb{E}\!\left[\exp\!\left(\sum_{i=1}^n X_i t\right)\right]
= \mathbb{E}\!\left[\prod_{i=1}^n e^{X_i t}\right]
\]

\[
= \prod_{i=1}^n \mathbb{E}\!\left[e^{X_i t}\right]
= \prod_{i=1}^n M_{X_i}(t).
\]

---

\textbf{Ejemplo.}  
Sean $X_1, X_2, \dots, X_n$ v.a. independientes con distribuci\'on
$\mathrm{Ber}(p)$.

\[
M_X(t) = p e^t + q,
\]

entonces,
\[
M_{\sum X_i}(t)
= \prod_{i=1}^n M_{X_i}(t)
= (p e^t + q)^n.
\]

Se tiene que
\[
\sum_{i=1}^n X_i \sim \mathrm{Binomial}(n,p).
\]

---

\textbf{Ejemplo.}  
Sean $X_1, X_2, \dots, X_n$ v.a. independientes con
\[
X_i \sim \mathrm{Po}(\lambda_i).
\]

\[
M_{X_i}(t) = \mathbb{E}\!\left[e^{X_i t}\right]
= \exp\!\left(\lambda_i(e^t - 1)\right).
\]

Entonces,
\[
M_{\sum X_i}(t)
= \prod_{i=1}^n M_{X_i}(t)
= \prod_{i=1}^n \exp\!\left(\lambda_i(e^t - 1)\right)
\]

\[
= \exp\!\left(\sum_{i=1}^n \lambda_i (e^t - 1)\right),
\quad \text{i.e. } 
\sum_{i=1}^n X_i \sim \mathrm{Po}\!\left(\sum_{i=1}^n \lambda_i\right).

\textbf{Teorema.}  
Sean $X_1, X_2, \ldots, X_n$ v.a. independientes t.q. su FGM existe
$\forall |t| < h$, p.a. $h>0$.  
Sea
\[
Y = \sum_{i=1}^n X_i .
\]

Entonces,
\[
M_Y(t) = \mathbb{E}\!\left[e^{\sum_{i=1}^n X_i t}\right]
= \prod_{i=1}^n M_{X_i}(t),
\qquad |t| < h.
\]

\textbf{Demostraci\'on.}

\[
M_Y(t)
= \mathbb{E}\!\left[\exp\!\left(\sum_{i=1}^n X_i t\right)\right]
= \mathbb{E}\!\left[\prod_{i=1}^n e^{X_i t}\right]
\]

\[
= \prod_{i=1}^n \mathbb{E}\!\left[e^{X_i t}\right]
= \prod_{i=1}^n M_{X_i}(t).
\]

---

\textbf{Ejemplo.}  
Sean $X_1, X_2, \ldots, X_n$ v.a. independientes con distribuci\'on
$\mathrm{Ber}(p)$.

\[
M_X(t) = p e^t + q,
\]

entonces,
\[
M_{\sum X_i}(t)
= \prod_{i=1}^n M_{X_i}(t)
= (p e^t + q)^n,
\qquad \text{i.e. } Y = \sum_{i=1}^n X_i.
\]

Se tiene distribuci\'on
\[
\mathrm{Binomial}(n,p).
\]

---

\textbf{Ejemplo.}  
Sean $X_1, X_2, \ldots, X_n$ v.a. independientes con
\[
X_i \sim \mathrm{Po}(\lambda_i).
\]

\[
M_{X_i}(t) = \mathbb{E}\!\left[e^{X_i t}\right]
= \exp\!\left(\lambda_i (e^t - 1)\right).
\]

\[
M_{\sum X_i}(t)
= \prod_{i=1}^n M_{X_i}(t)
= \prod_{i=1}^n \exp\!\left(\lambda_i (e^t - 1)\right)
\]

\[
= \exp\!\left(\sum_{i=1}^n \lambda_i (e^t - 1)\right),
\quad \text{i.e. } 
Y \sim \mathrm{Po}\!\left(\sum_{i=1}^n \lambda_i\right).

\textbf{Ejemplo.}  
Sean $X_1, X_2, \ldots, X_n$ v.a. independientes con distribuci\'on
$\mathrm{Exp}(\lambda)$.

Entonces,
\[
M_{X_i}(t) = \frac{\lambda}{\lambda - t},
\]

y por lo tanto,
\[
M_Y(t)
= \prod_{i=1}^n M_{X_i}(t)
= \prod_{i=1}^n \left(\frac{\lambda}{\lambda - t}\right)
= \left(\frac{\lambda}{\lambda - t}\right)^n.
\]

i.e.
\[
Y \sim \mathrm{Ga}(n,\lambda).
\]

---

\textbf{Ejemplo.}  
Sean $X_1, X_2, \ldots, X_n$ v.a. independientes t.q.
\[
X_i \sim \mathcal{N}(\mu_i,\sigma_i^2),
\]
entonces
\[
a_i X_i \sim \mathcal{N}(a_i \mu_i, a_i^2 \sigma_i^2),
\]

y
\[
M_{a_i X_i}(t)
= \exp\!\left(a_i \mu_i t + \frac{1}{2} a_i^2 \sigma_i^2 t^2\right).
\]

Entonces,
\[
M_{\sum_{i=1}^n a_i X_i}(t)
= \prod_{i=1}^n M_{a_i X_i}(t)
= \exp\!\left(\sum_{i=1}^n a_i \mu_i t
+ \frac{1}{2} \sum_{i=1}^n a_i^2 \sigma_i^2 t^2\right).
\]

i.e.
\[
Y \sim \mathcal{N}\!\left(
\sum_{i=1}^n a_i \mu_i,
\sum_{i=1}^n a_i^2 \sigma_i^2
\right).
\]

---

\textbf{Resultado.}  
Si
\[
X \sim \mathcal{N}(\mu_x,\sigma_x^2),
\qquad
Y \sim \mathcal{N}(\mu_y,\sigma_y^2),
\]
y $X,Y$ son independientes, entonces

\[
X+Y \sim \mathcal{N}(\mu_x+\mu_y,\sigma_x^2+\sigma_y^2),
\]

\[
X-Y \sim \mathcal{N}(\mu_x-\mu_y,\sigma_x^2+\sigma_y^2).
\]

\textbf{Tema:} (Teorema Central del L\'imite) \hfill (Tema l\'imite central)

Sea $n \in \mathbb{N}$, $X_1, X_2, \ldots, X_n$ sean v.a. i.i.d.
con media $\mu_X$ y varianza $\sigma_X^2$ finitas $\forall i$.

Entonces,
\[
F_{Z_n}(z) \longrightarrow \Phi(z)
\quad \text{cuando } n \to \infty.
\]

Donde
\[
Z_n
= \frac{\bar X_n - \mathbb{E}[\bar X_n]}{\sqrt{\operatorname{Var}(\bar X_n)}}
= \frac{\bar X_n - \mu_X}{\sigma_X/\sqrt{n}}.
\]

Para $n$ fijo, el valor de la FDP de $Z_n$ ($n=1,2,\ldots$)
converge al valor $\phi(z)$.

---

\textbf{Corolario:}  
Si $X_1, X_2, \ldots, X_n$ son v.a. i.i.d. con media $\mu_X$
y varianza $\sigma_X^2$ finitas, entonces
\[
\mathbb{P}\!\left(
a < \frac{\bar X_n - \mu_X}{\sigma_X/\sqrt{n}} < b
\right)
\approx \Phi(b) - \Phi(a).
\]

---

\textbf{Nota:}  
Recordemos que para $Y = g(X)$,
\[
f_Y(y)
= \left| \frac{d}{dy} g^{-1}(y) \right|
f_X\!\big(g^{-1}(y)\big)\,\mathbf{1}_D(y).
\]


\textbf{Ej:} Sup. $X \sim \mathrm{Be}(\alpha,\beta)$, c\'omo se distribuye
\[
Y = -\log_e X \, ?
\]

Se tiene
\[
y = -\ln x \;\;\Rightarrow\;\; x = e^{-y},
\qquad
y = \ln x^{-1} \;\Rightarrow\; e^{y} = x^{-1}.
\]

Luego,
\[
x = g^{-1}(y) = e^{-y},
\qquad
\frac{dx}{dy} = -e^{-y}.
\]

Por la f\'ormula de transformaci\'on,
\[
f_Y(y)
= \left| \frac{d}{dy} g^{-1}(y) \right|
f_X\!\big(g^{-1}(y)\big).
\]

Como
\[
f_X(x)
= \frac{1}{\mathrm{Be}(\alpha,\beta)}
x^{\alpha-1}(1-x)^{\beta-1},
\]

entonces
\[
f_Y(y)
= e^{-y}\,
\frac{1}{\mathrm{Be}(\alpha,\beta)}
(e^{-y})^{\alpha-1}
(1-e^{-y})^{\beta-1}.
\]

Simplificando,
\[
f_Y(y)
= \frac{1}{\mathrm{Be}(\alpha,\beta)}
e^{-\alpha y}
(1-e^{-y})^{\beta-1}.
\]

Si $\beta = 1$, se tiene
\[
f_Y(y)
= \frac{1}{\mathrm{Be}(\alpha,1)} e^{-\alpha y}.
\]

Adem\'as,
\[
\mathrm{Be}(\alpha,1)
= \frac{\Gamma(\alpha)\Gamma(1)}{\Gamma(\alpha+1)}
= \frac{1}{\alpha}.
\]

Por tanto,
\[
f_Y(y) = \alpha e^{-\alpha y},
\qquad
Y \sim \mathrm{Exp}(\alpha).
\]

---

\textbf{Ej:} Sea $X$ v.a. con densidad Pareto, i.e.
\[
f_X(x)
= \theta x^{-\theta-1}\,\mathbf{1}_{(1,\infty)}(x),
\]
y
\[
Y = \log_e X = \ln X.
\]

Entonces
\[
y = \ln x,
\qquad
x = e^{y},
\qquad
dx = e^{y} dy.
\]

Luego,
\[
f_Y(y)
= e^{y}\,\theta (e^{y})^{-\theta-1}\,\mathbf{1}_{(0,\infty)}(y)
= \theta e^{-\theta y}\,\mathbf{1}_{(0,\infty)}(y).
\]

\textbf{Ej:} Sean $X_1, X_2$ v.a. independientes,
\[
X_1, X_2 \sim N(0,1).
\]

Definimos
\[
Y_1 = X_1 + X_2,
\qquad
Y_2 = \frac{X_1}{X_2}.
\]

Entonces,
\[
x_1 = g_1^{-1}(y_1,y_2) = \frac{y_1 y_2}{1+y_2},
\qquad
x_2 = g_2^{-1}(y_1,y_2) = \frac{y_1}{1+y_2}.
\]

Las transformaciones directas son
\[
g_1(x_1,x_2) = x_1 + x_2,
\qquad
g_2(x_1,x_2) = \frac{x_1}{x_2}.
\]

Derivadas parciales:
\[
\frac{\partial g_1}{\partial x_1} = 1,
\qquad
\frac{\partial g_1}{\partial x_2} = 1,
\]
\[
\frac{\partial g_2}{\partial x_1} = \frac{1}{x_2},
\qquad
\frac{\partial g_2}{\partial x_2} = -\frac{x_1}{x_2^2}.
\]

El jacobiano es
\[
|J|
=
\left|
\begin{matrix}
1 & 1 \\
\frac{1}{x_2} & -\frac{x_1}{x_2^2}
\end{matrix}
\right|
=
\left|
-\frac{x_1}{x_2^2} - \frac{1}{x_2}
\right|
=
\left|
-\frac{x_1+x_2}{x_2^2}
\right|.
\]

Por lo tanto,
\[
|J| = \frac{x_1+x_2}{x_2^2}.
\]

Luego,
\[
|J^{-1}| = \frac{x_2^2}{x_1+x_2}.
\]

Sustituyendo
\[
x_2 = \frac{y_1}{1+y_2},
\qquad
x_1+x_2 = y_1,
\]
se obtiene
\[
|J^{-1}|
=
\frac{\left(\frac{y_1}{1+y_2}\right)^2}{y_1}
=
\frac{y_1}{(1+y_2)^2}.
\]












